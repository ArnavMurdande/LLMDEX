[
  {
    "model_id": null,
    "model_slug": "gpt-oss-20b",
    "canonical_name": "gpt-oss-20B (low)",
    "model_name": "gpt-oss-20B (low)",
    "aliases": [
      "gpt-oss-20B (high)",
      "gpt-oss-20B (low)",
      "gpt-oss-20b"
    ],
    "provider": "OpenAI",
    "context_window": 131072,
    "open_source": true,
    "arena_votes": 10758,
    "license_type": "Apache 2.0",
    "creator": null,
    "intelligence_score": 22.63,
    "coding_score": 16.45,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 1317.02,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": 0.105,
    "latency_seconds": 0.525,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": 270.5,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": 644.4871,
    "terminalbench_hard": 0.0755,
    "tau2": 0.5525,
    "lcr": 30.85,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": 7.45,
    "gpqa": 64.95,
    "scicode": 34.2,
    "ifbench": 61.45,
    "aime25": 75.8,
    "critpt": 0.7,
    "mmmu_pro": null,
    "livecodebench": 71.45,
    "sources": [
      "Artificial Analysis",
      "LMSYS Chatbot Arena"
    ],
    "source_count": 2,
    "data_tier": 1,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.902967+00:00",
        "confidence": 1.0,
        "raw_name": "gpt-oss-20B (low)",
        "raw_scores": {
          "intelligence_score": 20.79,
          "coding_score": 14.37,
          "blended_cost_per_1m": 0.1,
          "latency_seconds": 0.54,
          "tokens_per_second": 248.0,
          "context_window": 131072,
          "gdpval": 597.3537707177181,
          "terminalbench_hard": 0.045,
          "tau2": 0.503,
          "lcr": 31.0,
          "hle": 5.1,
          "gpqa": 61.1,
          "scicode": 34.0,
          "ifbench": 57.8,
          "aime25": 62.3,
          "critpt": 0.0,
          "livecodebench": 65.2
        }
      },
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.903166+00:00",
        "confidence": 1.0,
        "raw_name": "gpt-oss-20B (high)",
        "raw_scores": {
          "intelligence_score": 24.47,
          "coding_score": 18.53,
          "blended_cost_per_1m": 0.11,
          "latency_seconds": 0.51,
          "tokens_per_second": 293.0,
          "context_window": 131072,
          "gdpval": 691.6204479808839,
          "terminalbench_hard": 0.106,
          "tau2": 0.602,
          "lcr": 30.7,
          "hle": 9.8,
          "gpqa": 68.8,
          "scicode": 34.4,
          "ifbench": 65.1,
          "aime25": 89.3,
          "critpt": 1.4,
          "livecodebench": 77.7
        }
      },
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.893671+00:00",
        "confidence": 1.0,
        "raw_name": "gpt-oss-20b",
        "raw_scores": {
          "arena_elo": 1317.02,
          "arena_votes": 10758
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 20.79,
        "coding_score": 14.37,
        "gdpval": 597.3537707177181,
        "terminalbench_hard": 0.045,
        "tau2": 0.503,
        "lcr": 31.0,
        "hle": 5.1,
        "gpqa": 61.1,
        "scicode": 34.0,
        "ifbench": 57.8,
        "aime25": 62.3,
        "critpt": 0.0,
        "livecodebench": 65.2
      },
      "LMSYS Chatbot Arena": {
        "arena_elo": 1317.02
      }
    },
    "confidence_score": 1.0,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "gpt-oss-120b",
    "canonical_name": "gpt-oss-120B (low)",
    "model_name": "gpt-oss-120B (low)",
    "aliases": [
      "gpt-oss-120B (high)",
      "gpt-oss-120B (low)",
      "gpt-oss-120b"
    ],
    "provider": "OpenAI",
    "context_window": 131072,
    "open_source": true,
    "arena_votes": 30756,
    "license_type": "Apache 2.0",
    "creator": null,
    "intelligence_score": 28.87,
    "coding_score": 22.075,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 1353.9,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": 0.26,
    "latency_seconds": 0.47,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": 298.5,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": 917.1764,
    "terminalbench_hard": 0.144,
    "tau2": 0.554,
    "lcr": 47.2,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": 11.85,
    "gpqa": 72.7,
    "scicode": 37.45,
    "ifbench": 63.65,
    "aime25": 80.05,
    "critpt": 0.55,
    "mmmu_pro": null,
    "livecodebench": 79.25,
    "sources": [
      "Artificial Analysis",
      "LMSYS Chatbot Arena"
    ],
    "source_count": 2,
    "data_tier": 1,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.903060+00:00",
        "confidence": 1.0,
        "raw_name": "gpt-oss-120B (low)",
        "raw_scores": {
          "intelligence_score": 24.47,
          "coding_score": 15.53,
          "blended_cost_per_1m": 0.26,
          "latency_seconds": 0.43,
          "tokens_per_second": 295.0,
          "context_window": 131072,
          "gdpval": 866.8353679047509,
          "terminalbench_hard": 0.053,
          "tau2": 0.45,
          "lcr": 43.7,
          "hle": 5.2,
          "gpqa": 67.2,
          "scicode": 36.0,
          "ifbench": 58.3,
          "aime25": 66.7,
          "critpt": 0.0,
          "livecodebench": 70.7
        }
      },
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.903115+00:00",
        "confidence": 1.0,
        "raw_name": "gpt-oss-120B (high)",
        "raw_scores": {
          "intelligence_score": 33.27,
          "coding_score": 28.62,
          "blended_cost_per_1m": 0.26,
          "latency_seconds": 0.51,
          "tokens_per_second": 302.0,
          "context_window": 131072,
          "gdpval": 967.5174007366429,
          "terminalbench_hard": 0.235,
          "tau2": 0.658,
          "lcr": 50.7,
          "hle": 18.5,
          "gpqa": 78.2,
          "scicode": 38.9,
          "ifbench": 69.0,
          "aime25": 93.4,
          "critpt": 1.1,
          "livecodebench": 87.8
        }
      },
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.892978+00:00",
        "confidence": 1.0,
        "raw_name": "gpt-oss-120b",
        "raw_scores": {
          "arena_elo": 1353.9,
          "arena_votes": 30756
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 24.47,
        "coding_score": 15.53,
        "gdpval": 866.8353679047509,
        "terminalbench_hard": 0.053,
        "tau2": 0.45,
        "lcr": 43.7,
        "hle": 5.2,
        "gpqa": 67.2,
        "scicode": 36.0,
        "ifbench": 58.3,
        "aime25": 66.7,
        "critpt": 0.0,
        "livecodebench": 70.7
      },
      "LMSYS Chatbot Arena": {
        "arena_elo": 1353.9
      }
    },
    "confidence_score": 1.0,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "gpt-51-codex-mini",
    "canonical_name": "GPT-5.1 Codex mini (high)",
    "model_name": "GPT-5.1 Codex mini (high)",
    "aliases": [
      "GPT-5.1 Codex mini (high)"
    ],
    "provider": "OpenAI",
    "context_window": 400000,
    "open_source": null,
    "arena_votes": null,
    "license_type": null,
    "creator": null,
    "intelligence_score": 38.63,
    "coding_score": 36.42,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": null,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": 0.69,
    "latency_seconds": 12.48,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": 131.0,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": 1034.4188,
    "terminalbench_hard": 0.333,
    "tau2": 0.629,
    "lcr": 62.7,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": 16.9,
    "gpqa": 81.3,
    "scicode": 42.6,
    "ifbench": 67.9,
    "aime25": 91.7,
    "critpt": 0.0,
    "mmmu_pro": 69.0,
    "livecodebench": 83.6,
    "sources": [
      "Artificial Analysis"
    ],
    "source_count": 1,
    "data_tier": 2,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.903234+00:00",
        "confidence": 1.0,
        "raw_name": "GPT-5.1 Codex mini (high)",
        "raw_scores": {
          "intelligence_score": 38.63,
          "coding_score": 36.42,
          "blended_cost_per_1m": 0.69,
          "latency_seconds": 12.48,
          "tokens_per_second": 131.0,
          "context_window": 400000,
          "gdpval": 1034.4188469963951,
          "terminalbench_hard": 0.333,
          "tau2": 0.629,
          "lcr": 62.7,
          "hle": 16.9,
          "gpqa": 81.3,
          "scicode": 42.6,
          "ifbench": 67.9,
          "aime25": 91.7,
          "critpt": 0.0,
          "mmmu_pro": 69.0,
          "livecodebench": 83.6
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 38.63,
        "coding_score": 36.42,
        "gdpval": 1034.4188469963951,
        "terminalbench_hard": 0.333,
        "tau2": 0.629,
        "lcr": 62.7,
        "hle": 16.9,
        "gpqa": 81.3,
        "scicode": 42.6,
        "ifbench": 67.9,
        "aime25": 91.7,
        "critpt": 0.0,
        "mmmu_pro": 69.0,
        "livecodebench": 83.6
      }
    },
    "confidence_score": 1.0,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "gpt-5-nano",
    "canonical_name": "GPT-5 nano (high)",
    "model_name": "GPT-5 nano (high)",
    "aliases": [
      "GPT-5 nano (high)",
      "GPT-5 nano (medium)",
      "GPT-5 nano (minimal)",
      "gpt-5-nano-high"
    ],
    "provider": "OpenAI",
    "context_window": 400000,
    "open_source": null,
    "arena_votes": 8352,
    "license_type": "Proprietary",
    "creator": null,
    "intelligence_score": 22.7669,
    "coding_score": 19.1267,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 1337.83,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": 0.14,
    "latency_seconds": 61.1067,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": 131.6667,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": 597.0942,
    "terminalbench_hard": 0.121,
    "tau2": 0.3087,
    "lcr": 33.9,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": 6.6333,
    "gpqa": 59.1333,
    "scicode": 33.1667,
    "ifbench": 55.3333,
    "aime25": 63.1,
    "critpt": 0.0,
    "mmmu_pro": 50.3333,
    "livecodebench": 67.4,
    "sources": [
      "Artificial Analysis",
      "LMSYS Chatbot Arena"
    ],
    "source_count": 2,
    "data_tier": 1,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.903287+00:00",
        "confidence": 1.0,
        "raw_name": "GPT-5 nano (high)",
        "raw_scores": {
          "intelligence_score": 26.83,
          "coding_score": 20.27,
          "blended_cost_per_1m": 0.14,
          "latency_seconds": 129.42,
          "tokens_per_second": 123.0,
          "context_window": 400000,
          "gdpval": 799.7829763070654,
          "terminalbench_hard": 0.121,
          "tau2": 0.365,
          "lcr": 41.7,
          "hle": 8.2,
          "gpqa": 67.6,
          "scicode": 36.6,
          "ifbench": 67.6,
          "aime25": 83.7,
          "critpt": 0.0,
          "mmmu_pro": 61.0,
          "livecodebench": 78.9
        }
      },
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.912330+00:00",
        "confidence": 1.0,
        "raw_name": "GPT-5 nano (minimal)",
        "raw_scores": {
          "intelligence_score": 15.590783726427158,
          "coding_score": 14.23,
          "blended_cost_per_1m": 0.14,
          "latency_seconds": 0.59,
          "tokens_per_second": 133.0,
          "context_window": 400000,
          "gdpval": 345.5816249934593,
          "terminalbench_hard": 0.068,
          "tau2": 0.257,
          "lcr": 20.0,
          "hle": 4.1,
          "gpqa": 42.8,
          "scicode": 29.1,
          "ifbench": 32.5,
          "aime25": 27.3,
          "critpt": 0.0,
          "mmmu_pro": 31.8,
          "livecodebench": 47.0
        }
      },
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.912532+00:00",
        "confidence": 1.0,
        "raw_name": "GPT-5 nano (medium)",
        "raw_scores": {
          "intelligence_score": 25.88,
          "coding_score": 22.88,
          "blended_cost_per_1m": 0.14,
          "latency_seconds": 53.31,
          "tokens_per_second": 139.0,
          "context_window": 400000,
          "gdpval": 645.9179240849795,
          "terminalbench_hard": 0.174,
          "tau2": 0.304,
          "lcr": 40.0,
          "hle": 7.6,
          "gpqa": 67.0,
          "scicode": 33.8,
          "ifbench": 65.9,
          "aime25": 78.3,
          "critpt": 0.0,
          "mmmu_pro": 58.2,
          "livecodebench": 76.3
        }
      },
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.893281+00:00",
        "confidence": 1.0,
        "raw_name": "gpt-5-nano-high",
        "raw_scores": {
          "arena_elo": 1337.83,
          "arena_votes": 8352
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 26.83,
        "coding_score": 20.27,
        "gdpval": 799.7829763070654,
        "terminalbench_hard": 0.121,
        "tau2": 0.365,
        "lcr": 41.7,
        "hle": 8.2,
        "gpqa": 67.6,
        "scicode": 36.6,
        "ifbench": 67.6,
        "aime25": 83.7,
        "critpt": 0.0,
        "mmmu_pro": 61.0,
        "livecodebench": 78.9
      },
      "LMSYS Chatbot Arena": {
        "arena_elo": 1337.83
      }
    },
    "confidence_score": 1.0,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "grok-1",
    "canonical_name": "Grok-1",
    "model_name": "Grok-1",
    "aliases": [
      "Grok-1"
    ],
    "provider": "xAI",
    "context_window": 8192,
    "open_source": true,
    "arena_votes": null,
    "license_type": "Apache 2.0",
    "creator": null,
    "intelligence_score": 11.6902,
    "coding_score": null,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": null,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": 0.0,
    "latency_seconds": 0.0,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": 0.0,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": null,
    "tau2": null,
    "lcr": null,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": null,
    "gpqa": null,
    "scicode": null,
    "ifbench": null,
    "aime25": null,
    "critpt": null,
    "mmmu_pro": null,
    "livecodebench": null,
    "sources": [
      "Artificial Analysis"
    ],
    "source_count": 1,
    "data_tier": 2,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.903314+00:00",
        "confidence": 0.65,
        "raw_name": "Grok-1",
        "raw_scores": {
          "intelligence_score": 11.690189040378796,
          "blended_cost_per_1m": 0.0,
          "latency_seconds": 0.0,
          "tokens_per_second": 0.0,
          "context_window": 8192
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 11.690189040378796
      }
    },
    "confidence_score": 0.65,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "gpt-52",
    "canonical_name": "GPT-5.2 (medium)",
    "model_name": "GPT-5.2 (medium)",
    "aliases": [
      "GPT-5.2 (Non-reasoning)",
      "GPT-5.2 (medium)",
      "GPT-5.2 (xhigh)",
      "gpt-5.2",
      "gpt-5.2-chat-latest-20260210",
      "gpt-5.2-high"
    ],
    "provider": "OpenAI",
    "context_window": 400000,
    "open_source": null,
    "arena_votes": 19253,
    "license_type": "Proprietary",
    "creator": null,
    "intelligence_score": 44.6023,
    "coding_score": 43.0994,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 1452.9067,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": 4.81,
    "latency_seconds": 13.575,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": 45.0,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": 1380.7651,
    "terminalbench_hard": 0.4133,
    "tau2": 0.7019,
    "lcr": 59.5054,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": 23.6799,
    "gpqa": 83.4939,
    "scicode": 46.6724,
    "ifbench": 63.8158,
    "aime25": 84.5842,
    "critpt": 7.1591,
    "mmmu_pro": 70.7162,
    "livecodebench": 82.8498,
    "sources": [
      "Artificial Analysis",
      "LMSYS Chatbot Arena"
    ],
    "source_count": 2,
    "data_tier": 1,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.903374+00:00",
        "confidence": 1.0,
        "raw_name": "GPT-5.2 (medium)",
        "raw_scores": {
          "intelligence_score": 46.64,
          "coding_score": 44.18,
          "blended_cost_per_1m": 4.81,
          "latency_seconds": 0.0,
          "tokens_per_second": 0.0,
          "context_window": 400000,
          "gdpval": 1418.6711616378857,
          "terminalbench_hard": 0.432,
          "tau2": 0.743,
          "lcr": 63.3,
          "hle": 24.9,
          "gpqa": 86.4,
          "scicode": 46.2,
          "ifbench": 65.2,
          "aime25": 96.7,
          "critpt": 7.9,
          "mmmu_pro": 74.6,
          "livecodebench": 89.4
        }
      },
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.903425+00:00",
        "confidence": 0.79,
        "raw_name": "GPT-5.2 (Non-reasoning)",
        "raw_scores": {
          "intelligence_score": 33.57,
          "coding_score": 34.68,
          "context_window": 400000,
          "gdpval": 1229.776167782739,
          "terminalbench_hard": 0.318,
          "tau2": 0.465,
          "lcr": 38.0,
          "hle": 7.3,
          "gpqa": 71.2,
          "scicode": 40.4,
          "ifbench": 47.4,
          "aime25": 51.0,
          "critpt": 0.6,
          "mmmu_pro": 65.8,
          "livecodebench": 66.9
        }
      },
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.903517+00:00",
        "confidence": 1.0,
        "raw_name": "GPT-5.2 (xhigh)",
        "raw_scores": {
          "intelligence_score": 51.28,
          "coding_score": 48.67,
          "blended_cost_per_1m": 4.81,
          "latency_seconds": 27.15,
          "tokens_per_second": 90.0,
          "context_window": 400000,
          "gdpval": 1462.1404149522382,
          "terminalbench_hard": 0.47,
          "tau2": 0.848,
          "lcr": 72.7,
          "hle": 35.4,
          "gpqa": 90.3,
          "scicode": 52.1,
          "ifbench": 75.4,
          "aime25": 99.0,
          "critpt": 11.6,
          "livecodebench": 88.9
        }
      },
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.891345+00:00",
        "confidence": 1.0,
        "raw_name": "gpt-5.2-chat-latest-20260210",
        "raw_scores": {
          "arena_elo": 1481.31,
          "arena_votes": 3605
        }
      },
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.891660+00:00",
        "confidence": 1.0,
        "raw_name": "gpt-5.2-high",
        "raw_scores": {
          "arena_elo": 1440.87,
          "arena_votes": 19253
        }
      },
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.891697+00:00",
        "confidence": 1.0,
        "raw_name": "gpt-5.2",
        "raw_scores": {
          "arena_elo": 1436.54,
          "arena_votes": 16113
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 46.64,
        "coding_score": 44.18,
        "gdpval": 1418.6711616378857,
        "terminalbench_hard": 0.432,
        "tau2": 0.743,
        "lcr": 63.3,
        "hle": 24.9,
        "gpqa": 86.4,
        "scicode": 46.2,
        "ifbench": 65.2,
        "aime25": 96.7,
        "critpt": 7.9,
        "mmmu_pro": 74.6,
        "livecodebench": 89.4
      },
      "LMSYS Chatbot Arena": {
        "arena_elo": 1481.31
      }
    },
    "confidence_score": 0.965,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "gpt-53-codex",
    "canonical_name": "GPT-5.3 Codex (xhigh)",
    "model_name": "GPT-5.3 Codex (xhigh)",
    "aliases": [
      "GPT-5.3 Codex (xhigh)"
    ],
    "provider": "OpenAI",
    "context_window": 400000,
    "open_source": null,
    "arena_votes": null,
    "license_type": null,
    "creator": null,
    "intelligence_score": 53.97,
    "coding_score": 53.1,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": null,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": 4.81,
    "latency_seconds": 61.42,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": 99.0,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": 1459.033,
    "terminalbench_hard": 0.53,
    "tau2": 0.909,
    "lcr": 74.0,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": 39.9,
    "gpqa": 91.5,
    "scicode": 53.2,
    "ifbench": 75.4,
    "aime25": null,
    "critpt": 16.9,
    "mmmu_pro": 78.5,
    "livecodebench": null,
    "sources": [
      "Artificial Analysis"
    ],
    "source_count": 1,
    "data_tier": 2,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.903470+00:00",
        "confidence": 0.8600000000000001,
        "raw_name": "GPT-5.3 Codex (xhigh)",
        "raw_scores": {
          "intelligence_score": 53.97,
          "coding_score": 53.1,
          "blended_cost_per_1m": 4.81,
          "latency_seconds": 61.42,
          "tokens_per_second": 99.0,
          "context_window": 400000,
          "gdpval": 1459.032967173148,
          "terminalbench_hard": 0.53,
          "tau2": 0.909,
          "lcr": 74.0,
          "hle": 39.9,
          "gpqa": 91.5,
          "scicode": 53.2,
          "ifbench": 75.4,
          "critpt": 16.9,
          "mmmu_pro": 78.5
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 53.97,
        "coding_score": 53.1,
        "gdpval": 1459.032967173148,
        "terminalbench_hard": 0.53,
        "tau2": 0.909,
        "lcr": 74.0,
        "hle": 39.9,
        "gpqa": 91.5,
        "scicode": 53.2,
        "ifbench": 75.4,
        "critpt": 16.9,
        "mmmu_pro": 78.5
      }
    },
    "confidence_score": 0.86,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "gpt-5-mini",
    "canonical_name": "GPT-5 mini (high)",
    "model_name": "GPT-5 mini (high)",
    "aliases": [
      "GPT-5 mini (high)",
      "GPT-5 mini (medium)",
      "GPT-5 mini (minimal)",
      "gpt-5-mini-high"
    ],
    "provider": "OpenAI",
    "context_window": 400000,
    "open_source": null,
    "arena_votes": 26941,
    "license_type": "Proprietary",
    "creator": null,
    "intelligence_score": 33.9469,
    "coding_score": 30.0167,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 1390.38,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": 0.69,
    "latency_seconds": 47.4233,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": 75.0,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": 911.0161,
    "terminalbench_hard": 0.255,
    "tau2": 0.5713,
    "lcr": 56.5667,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": 13.1,
    "gpqa": 77.2667,
    "scicode": 39.0333,
    "ifbench": 64.0667,
    "aime25": 74.1333,
    "critpt": 0.4667,
    "mmmu_pro": 65.7667,
    "livecodebench": 69.1667,
    "sources": [
      "Artificial Analysis",
      "LMSYS Chatbot Arena"
    ],
    "source_count": 2,
    "data_tier": 1,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.903564+00:00",
        "confidence": 1.0,
        "raw_name": "GPT-5 mini (high)",
        "raw_scores": {
          "intelligence_score": 41.17,
          "coding_score": 35.3,
          "blended_cost_per_1m": 0.69,
          "latency_seconds": 105.88,
          "tokens_per_second": 77.0,
          "context_window": 400000,
          "gdpval": 1194.9191144970962,
          "terminalbench_hard": 0.333,
          "tau2": 0.684,
          "lcr": 68.0,
          "hle": 19.7,
          "gpqa": 82.8,
          "scicode": 39.2,
          "ifbench": 75.4,
          "aime25": 90.7,
          "critpt": 0.0,
          "mmmu_pro": 70.1,
          "livecodebench": 83.8
        }
      },
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.912112+00:00",
        "confidence": 1.0,
        "raw_name": "GPT-5 mini (minimal)",
        "raw_scores": {
          "intelligence_score": 21.730677087717407,
          "coding_score": 21.9,
          "blended_cost_per_1m": 0.69,
          "latency_seconds": 0.82,
          "tokens_per_second": 77.0,
          "context_window": 400000,
          "gdpval": 516.1884875759439,
          "terminalbench_hard": 0.144,
          "tau2": 0.319,
          "lcr": 35.7,
          "hle": 5.0,
          "gpqa": 68.7,
          "scicode": 36.9,
          "ifbench": 45.6,
          "aime25": 46.7,
          "critpt": 0.0,
          "mmmu_pro": 58.4,
          "livecodebench": 54.5
        }
      },
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.912464+00:00",
        "confidence": 1.0,
        "raw_name": "GPT-5 mini (medium)",
        "raw_scores": {
          "intelligence_score": 38.94,
          "coding_score": 32.85,
          "blended_cost_per_1m": 0.69,
          "latency_seconds": 35.57,
          "tokens_per_second": 71.0,
          "context_window": 400000,
          "gdpval": 1021.9408052231893,
          "terminalbench_hard": 0.288,
          "tau2": 0.711,
          "lcr": 66.0,
          "hle": 14.6,
          "gpqa": 80.3,
          "scicode": 41.0,
          "ifbench": 71.2,
          "aime25": 85.0,
          "critpt": 1.4,
          "mmmu_pro": 68.8,
          "livecodebench": 69.2
        }
      },
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.892422+00:00",
        "confidence": 1.0,
        "raw_name": "gpt-5-mini-high",
        "raw_scores": {
          "arena_elo": 1390.38,
          "arena_votes": 26941
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 41.17,
        "coding_score": 35.3,
        "gdpval": 1194.9191144970962,
        "terminalbench_hard": 0.333,
        "tau2": 0.684,
        "lcr": 68.0,
        "hle": 19.7,
        "gpqa": 82.8,
        "scicode": 39.2,
        "ifbench": 75.4,
        "aime25": 90.7,
        "critpt": 0.0,
        "mmmu_pro": 70.1,
        "livecodebench": 83.8
      },
      "LMSYS Chatbot Arena": {
        "arena_elo": 1390.38
      }
    },
    "confidence_score": 1.0,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "lfm2-24b-a2b",
    "canonical_name": "LFM2 24B A2B",
    "model_name": "LFM2 24B A2B",
    "aliases": [
      "LFM2 24B A2B"
    ],
    "provider": null,
    "context_window": 32768,
    "open_source": true,
    "arena_votes": null,
    "license_type": "lfm 1.0",
    "creator": null,
    "intelligence_score": 10.49,
    "coding_score": 3.63,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": null,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": 0.05,
    "latency_seconds": 0.21,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": 109.0,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": 282.0456,
    "terminalbench_hard": 0.0,
    "tau2": 0.111,
    "lcr": 0.0,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": 4.4,
    "gpqa": 47.4,
    "scicode": 10.9,
    "ifbench": 45.9,
    "aime25": null,
    "critpt": 0.0,
    "mmmu_pro": null,
    "livecodebench": null,
    "sources": [
      "Artificial Analysis"
    ],
    "source_count": 1,
    "data_tier": 2,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.903603+00:00",
        "confidence": 0.8600000000000001,
        "raw_name": "LFM2 24B A2B",
        "raw_scores": {
          "intelligence_score": 10.49,
          "coding_score": 3.63,
          "blended_cost_per_1m": 0.05,
          "latency_seconds": 0.21,
          "tokens_per_second": 109.0,
          "context_window": 32768,
          "gdpval": 282.04561447201695,
          "terminalbench_hard": 0.0,
          "tau2": 0.111,
          "lcr": 0.0,
          "hle": 4.4,
          "gpqa": 47.4,
          "scicode": 10.9,
          "ifbench": 45.9,
          "critpt": 0.0
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 10.49,
        "coding_score": 3.63,
        "gdpval": 282.04561447201695,
        "terminalbench_hard": 0.0,
        "tau2": 0.111,
        "lcr": 0.0,
        "hle": 4.4,
        "gpqa": 47.4,
        "scicode": 10.9,
        "ifbench": 45.9,
        "critpt": 0.0
      }
    },
    "confidence_score": 0.86,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "o3",
    "canonical_name": "o3",
    "model_name": "o3",
    "aliases": [
      "o3",
      "o3-2025-04-16"
    ],
    "provider": "OpenAI",
    "context_window": 200000,
    "open_source": null,
    "arena_votes": 60957,
    "license_type": "Proprietary",
    "creator": null,
    "intelligence_score": 38.37,
    "coding_score": 38.4,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 1432.45,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": 3.5,
    "latency_seconds": 22.83,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": 108.0,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": 742.2834,
    "terminalbench_hard": 0.371,
    "tau2": 0.807,
    "lcr": 69.3,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": 20.0,
    "gpqa": 82.7,
    "scicode": 41.0,
    "ifbench": 71.4,
    "aime25": 88.3,
    "critpt": 1.1,
    "mmmu_pro": 70.1,
    "livecodebench": 80.8,
    "sources": [
      "Artificial Analysis",
      "LMSYS Chatbot Arena"
    ],
    "source_count": 2,
    "data_tier": 1,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.903651+00:00",
        "confidence": 1.0,
        "raw_name": "o3",
        "raw_scores": {
          "intelligence_score": 38.37,
          "coding_score": 38.4,
          "blended_cost_per_1m": 3.5,
          "latency_seconds": 22.83,
          "tokens_per_second": 108.0,
          "context_window": 200000,
          "gdpval": 742.2833785235354,
          "terminalbench_hard": 0.371,
          "tau2": 0.807,
          "lcr": 69.3,
          "hle": 20.0,
          "gpqa": 82.7,
          "scicode": 41.0,
          "ifbench": 71.4,
          "aime25": 88.3,
          "critpt": 1.1,
          "mmmu_pro": 70.1,
          "livecodebench": 80.8
        }
      },
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.891748+00:00",
        "confidence": 1.0,
        "raw_name": "o3-2025-04-16",
        "raw_scores": {
          "arena_elo": 1432.45,
          "arena_votes": 60957
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 38.37,
        "coding_score": 38.4,
        "gdpval": 742.2833785235354,
        "terminalbench_hard": 0.371,
        "tau2": 0.807,
        "lcr": 69.3,
        "hle": 20.0,
        "gpqa": 82.7,
        "scicode": 41.0,
        "ifbench": 71.4,
        "aime25": 88.3,
        "critpt": 1.1,
        "mmmu_pro": 70.1,
        "livecodebench": 80.8
      },
      "LMSYS Chatbot Arena": {
        "arena_elo": 1432.45
      }
    },
    "confidence_score": 1.0,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "gpt-52-codex",
    "canonical_name": "GPT-5.2 Codex (xhigh)",
    "model_name": "GPT-5.2 Codex (xhigh)",
    "aliases": [
      "GPT-5.2 Codex (xhigh)"
    ],
    "provider": "OpenAI",
    "context_window": 400000,
    "open_source": null,
    "arena_votes": null,
    "license_type": null,
    "creator": null,
    "intelligence_score": 49.03,
    "coding_score": 42.96,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": null,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": 4.81,
    "latency_seconds": 28.5,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": 87.0,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": 1286.8664,
    "terminalbench_hard": 0.371,
    "tau2": 0.921,
    "lcr": 75.7,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": 33.5,
    "gpqa": 89.9,
    "scicode": 54.6,
    "ifbench": 77.6,
    "aime25": null,
    "critpt": 8.7,
    "mmmu_pro": 76.3,
    "livecodebench": null,
    "sources": [
      "Artificial Analysis"
    ],
    "source_count": 1,
    "data_tier": 2,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.903691+00:00",
        "confidence": 0.8600000000000001,
        "raw_name": "GPT-5.2 Codex (xhigh)",
        "raw_scores": {
          "intelligence_score": 49.03,
          "coding_score": 42.96,
          "blended_cost_per_1m": 4.81,
          "latency_seconds": 28.5,
          "tokens_per_second": 87.0,
          "context_window": 400000,
          "gdpval": 1286.866404031031,
          "terminalbench_hard": 0.371,
          "tau2": 0.921,
          "lcr": 75.7,
          "hle": 33.5,
          "gpqa": 89.9,
          "scicode": 54.6,
          "ifbench": 77.6,
          "critpt": 8.7,
          "mmmu_pro": 76.3
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 49.03,
        "coding_score": 42.96,
        "gdpval": 1286.866404031031,
        "terminalbench_hard": 0.371,
        "tau2": 0.921,
        "lcr": 75.7,
        "hle": 33.5,
        "gpqa": 89.9,
        "scicode": 54.6,
        "ifbench": 77.6,
        "critpt": 8.7,
        "mmmu_pro": 76.3
      }
    },
    "confidence_score": 0.86,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "llama-33-instruct-70b",
    "canonical_name": "Llama 3.3 Instruct 70B",
    "model_name": "Llama 3.3 Instruct 70B",
    "aliases": [
      "Llama 3.3 Instruct 70B"
    ],
    "provider": "Meta",
    "context_window": 128000,
    "open_source": true,
    "arena_votes": null,
    "license_type": "LLAMA 3.3 COMMUNITY LICENSE AGREEMENT",
    "creator": null,
    "intelligence_score": 14.49,
    "coding_score": 10.7,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": null,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": 452.9232,
    "terminalbench_hard": 0.03,
    "tau2": 0.266,
    "lcr": 15.0,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": 4.0,
    "gpqa": 49.8,
    "scicode": 26.0,
    "ifbench": 47.1,
    "aime25": 7.7,
    "critpt": 0.0,
    "mmmu_pro": null,
    "livecodebench": 28.8,
    "sources": [
      "Artificial Analysis"
    ],
    "source_count": 1,
    "data_tier": 2,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.903732+00:00",
        "confidence": 0.79,
        "raw_name": "Llama 3.3 Instruct 70B",
        "raw_scores": {
          "intelligence_score": 14.49,
          "coding_score": 10.7,
          "context_window": 128000,
          "gdpval": 452.9231824786573,
          "terminalbench_hard": 0.03,
          "tau2": 0.266,
          "lcr": 15.0,
          "hle": 4.0,
          "gpqa": 49.8,
          "scicode": 26.0,
          "ifbench": 47.1,
          "aime25": 7.7,
          "critpt": 0.0,
          "livecodebench": 28.8
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 14.49,
        "coding_score": 10.7,
        "gdpval": 452.9231824786573,
        "terminalbench_hard": 0.03,
        "tau2": 0.266,
        "lcr": 15.0,
        "hle": 4.0,
        "gpqa": 49.8,
        "scicode": 26.0,
        "ifbench": 47.1,
        "aime25": 7.7,
        "critpt": 0.0,
        "livecodebench": 28.8
      }
    },
    "confidence_score": 0.79,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "llama-31-instruct-405b",
    "canonical_name": "Llama 3.1 Instruct 405B",
    "model_name": "Llama 3.1 Instruct 405B",
    "aliases": [
      "Llama 3.1 Instruct 405B"
    ],
    "provider": "Meta",
    "context_window": 128000,
    "open_source": true,
    "arena_votes": null,
    "license_type": "LLAMA 3.1 COMMUNITY LICENSE AGREEMENT",
    "creator": null,
    "intelligence_score": 15.2032,
    "coding_score": 14.5,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": null,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": 304.6001,
    "terminalbench_hard": 0.068,
    "tau2": 0.19,
    "lcr": 24.3,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": 4.2,
    "gpqa": 51.5,
    "scicode": 29.9,
    "ifbench": 39.0,
    "aime25": 3.0,
    "critpt": 0.0,
    "mmmu_pro": null,
    "livecodebench": 30.5,
    "sources": [
      "Artificial Analysis"
    ],
    "source_count": 1,
    "data_tier": 2,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.903773+00:00",
        "confidence": 0.79,
        "raw_name": "Llama 3.1 Instruct 405B",
        "raw_scores": {
          "intelligence_score": 15.203185114467049,
          "coding_score": 14.5,
          "context_window": 128000,
          "gdpval": 304.60007364418016,
          "terminalbench_hard": 0.068,
          "tau2": 0.19,
          "lcr": 24.3,
          "hle": 4.2,
          "gpqa": 51.5,
          "scicode": 29.9,
          "ifbench": 39.0,
          "aime25": 3.0,
          "critpt": 0.0,
          "livecodebench": 30.5
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 15.203185114467049,
        "coding_score": 14.5,
        "gdpval": 304.60007364418016,
        "terminalbench_hard": 0.068,
        "tau2": 0.19,
        "lcr": 24.3,
        "hle": 4.2,
        "gpqa": 51.5,
        "scicode": 29.9,
        "ifbench": 39.0,
        "aime25": 3.0,
        "critpt": 0.0,
        "livecodebench": 30.5
      }
    },
    "confidence_score": 0.79,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "llama-32-instruct-90b",
    "canonical_name": "Llama 3.2 Instruct 90B (Vision)",
    "model_name": "Llama 3.2 Instruct 90B (Vision)",
    "aliases": [
      "Llama 3.2 Instruct 90B (Vision)"
    ],
    "provider": "Meta",
    "context_window": 128000,
    "open_source": true,
    "arena_votes": null,
    "license_type": "LLAMA 3.2 COMMUNITY LICENSE AGREEMENT",
    "creator": null,
    "intelligence_score": 11.9013,
    "coding_score": null,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": null,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": null,
    "tau2": null,
    "lcr": null,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": 4.9,
    "gpqa": 43.2,
    "scicode": 24.0,
    "ifbench": null,
    "aime25": null,
    "critpt": null,
    "mmmu_pro": 39.5,
    "livecodebench": 21.4,
    "sources": [
      "Artificial Analysis"
    ],
    "source_count": 1,
    "data_tier": 2,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.903804+00:00",
        "confidence": 0.65,
        "raw_name": "Llama 3.2 Instruct 90B (Vision)",
        "raw_scores": {
          "intelligence_score": 11.90129896307905,
          "context_window": 128000,
          "hle": 4.9,
          "gpqa": 43.2,
          "scicode": 24.0,
          "mmmu_pro": 39.5,
          "livecodebench": 21.4
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 11.90129896307905,
        "hle": 4.9,
        "gpqa": 43.2,
        "scicode": 24.0,
        "mmmu_pro": 39.5,
        "livecodebench": 21.4
      }
    },
    "confidence_score": 0.65,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "llama-32-instruct-11b",
    "canonical_name": "Llama 3.2 Instruct 11B (Vision)",
    "model_name": "Llama 3.2 Instruct 11B (Vision)",
    "aliases": [
      "Llama 3.2 Instruct 11B (Vision)"
    ],
    "provider": "Meta",
    "context_window": 128000,
    "open_source": true,
    "arena_votes": null,
    "license_type": "LLAMA 3.2 COMMUNITY LICENSE AGREEMENT",
    "creator": null,
    "intelligence_score": 10.8874,
    "coding_score": 4.25,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": null,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": 0.008,
    "tau2": 0.146,
    "lcr": 11.7,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": 5.2,
    "gpqa": 22.1,
    "scicode": 11.2,
    "ifbench": 30.4,
    "aime25": 1.7,
    "critpt": 0.0,
    "mmmu_pro": 29.3,
    "livecodebench": 11.0,
    "sources": [
      "Artificial Analysis"
    ],
    "source_count": 1,
    "data_tier": 2,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.903846+00:00",
        "confidence": 0.79,
        "raw_name": "Llama 3.2 Instruct 11B (Vision)",
        "raw_scores": {
          "intelligence_score": 10.887386887368143,
          "coding_score": 4.25,
          "context_window": 128000,
          "terminalbench_hard": 0.008,
          "tau2": 0.146,
          "lcr": 11.7,
          "hle": 5.2,
          "gpqa": 22.1,
          "scicode": 11.2,
          "ifbench": 30.4,
          "aime25": 1.7,
          "critpt": 0.0,
          "mmmu_pro": 29.3,
          "livecodebench": 11.0
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 10.887386887368143,
        "coding_score": 4.25,
        "terminalbench_hard": 0.008,
        "tau2": 0.146,
        "lcr": 11.7,
        "hle": 5.2,
        "gpqa": 22.1,
        "scicode": 11.2,
        "ifbench": 30.4,
        "aime25": 1.7,
        "critpt": 0.0,
        "mmmu_pro": 29.3,
        "livecodebench": 11.0
      }
    },
    "confidence_score": 0.79,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "llama-4-scout",
    "canonical_name": "Llama 4 Scout",
    "model_name": "Llama 4 Scout",
    "aliases": [
      "Llama 4 Scout",
      "llama-4-scout-17b-16e-instruct"
    ],
    "provider": "Meta",
    "context_window": 10000000,
    "open_source": true,
    "arena_votes": 31053,
    "license_type": "LLAMA 4 COMMUNITY LICENSE AGREEMENT",
    "creator": null,
    "intelligence_score": 13.52,
    "coding_score": 6.68,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 1322.39,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": 0.29,
    "latency_seconds": 0.46,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": 136.0,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": 334.1687,
    "terminalbench_hard": 0.015,
    "tau2": 0.155,
    "lcr": 25.8,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": 4.3,
    "gpqa": 58.7,
    "scicode": 17.0,
    "ifbench": 39.5,
    "aime25": 14.0,
    "critpt": 0.0,
    "mmmu_pro": 52.9,
    "livecodebench": 29.9,
    "sources": [
      "Artificial Analysis",
      "LMSYS Chatbot Arena"
    ],
    "source_count": 2,
    "data_tier": 1,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.903894+00:00",
        "confidence": 1.0,
        "raw_name": "Llama 4 Scout",
        "raw_scores": {
          "intelligence_score": 13.52,
          "coding_score": 6.68,
          "blended_cost_per_1m": 0.29,
          "latency_seconds": 0.46,
          "tokens_per_second": 136.0,
          "context_window": 10000000,
          "gdpval": 334.168657434162,
          "terminalbench_hard": 0.015,
          "tau2": 0.155,
          "lcr": 25.8,
          "hle": 4.3,
          "gpqa": 58.7,
          "scicode": 17.0,
          "ifbench": 39.5,
          "aime25": 14.0,
          "critpt": 0.0,
          "mmmu_pro": 52.9,
          "livecodebench": 29.9
        }
      },
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.893547+00:00",
        "confidence": 1.0,
        "raw_name": "llama-4-scout-17b-16e-instruct",
        "raw_scores": {
          "arena_elo": 1322.39,
          "arena_votes": 31053
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 13.52,
        "coding_score": 6.68,
        "gdpval": 334.168657434162,
        "terminalbench_hard": 0.015,
        "tau2": 0.155,
        "lcr": 25.8,
        "hle": 4.3,
        "gpqa": 58.7,
        "scicode": 17.0,
        "ifbench": 39.5,
        "aime25": 14.0,
        "critpt": 0.0,
        "mmmu_pro": 52.9,
        "livecodebench": 29.9
      },
      "LMSYS Chatbot Arena": {
        "arena_elo": 1322.39
      }
    },
    "confidence_score": 1.0,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "llama-4-maverick",
    "canonical_name": "Llama 4 Maverick",
    "model_name": "Llama 4 Maverick",
    "aliases": [
      "Llama 4 Maverick",
      "llama-4-maverick-17b-128e-instruct"
    ],
    "provider": "Meta",
    "context_window": 1000000,
    "open_source": true,
    "arena_votes": 40932,
    "license_type": "LLAMA 4 COMMUNITY LICENSE AGREEMENT",
    "creator": null,
    "intelligence_score": 18.36,
    "coding_score": 15.58,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 1327.62,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": 0.46,
    "latency_seconds": 0.48,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": 114.0,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": 480.8312,
    "terminalbench_hard": 0.068,
    "tau2": 0.178,
    "lcr": 46.0,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": 4.8,
    "gpqa": 67.1,
    "scicode": 33.1,
    "ifbench": 43.0,
    "aime25": 19.3,
    "critpt": 0.0,
    "mmmu_pro": 62.1,
    "livecodebench": 39.7,
    "sources": [
      "Artificial Analysis",
      "LMSYS Chatbot Arena"
    ],
    "source_count": 2,
    "data_tier": 1,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.903939+00:00",
        "confidence": 1.0,
        "raw_name": "Llama 4 Maverick",
        "raw_scores": {
          "intelligence_score": 18.36,
          "coding_score": 15.58,
          "blended_cost_per_1m": 0.46,
          "latency_seconds": 0.48,
          "tokens_per_second": 114.0,
          "context_window": 1000000,
          "gdpval": 480.831153668395,
          "terminalbench_hard": 0.068,
          "tau2": 0.178,
          "lcr": 46.0,
          "hle": 4.8,
          "gpqa": 67.1,
          "scicode": 33.1,
          "ifbench": 43.0,
          "aime25": 19.3,
          "critpt": 0.0,
          "mmmu_pro": 62.1,
          "livecodebench": 39.7
        }
      },
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.893456+00:00",
        "confidence": 1.0,
        "raw_name": "llama-4-maverick-17b-128e-instruct",
        "raw_scores": {
          "arena_elo": 1327.62,
          "arena_votes": 40932
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 18.36,
        "coding_score": 15.58,
        "gdpval": 480.831153668395,
        "terminalbench_hard": 0.068,
        "tau2": 0.178,
        "lcr": 46.0,
        "hle": 4.8,
        "gpqa": 67.1,
        "scicode": 33.1,
        "ifbench": 43.0,
        "aime25": 19.3,
        "critpt": 0.0,
        "mmmu_pro": 62.1,
        "livecodebench": 39.7
      },
      "LMSYS Chatbot Arena": {
        "arena_elo": 1327.62
      }
    },
    "confidence_score": 1.0,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "gemma-3-27b-instruct",
    "canonical_name": "Gemma 3 27B Instruct",
    "model_name": "Gemma 3 27B Instruct",
    "aliases": [
      "Gemma 3 27B Instruct"
    ],
    "provider": "Google",
    "context_window": 128000,
    "open_source": true,
    "arena_votes": null,
    "license_type": "Gemma license",
    "creator": null,
    "intelligence_score": 10.31,
    "coding_score": 9.59,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": null,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": 353.4447,
    "terminalbench_hard": 0.038,
    "tau2": 0.105,
    "lcr": 5.7,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": 4.7,
    "gpqa": 42.8,
    "scicode": 21.2,
    "ifbench": 31.8,
    "aime25": 20.7,
    "critpt": 0.0,
    "mmmu_pro": 48.0,
    "livecodebench": 13.7,
    "sources": [
      "Artificial Analysis"
    ],
    "source_count": 1,
    "data_tier": 2,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.903981+00:00",
        "confidence": 0.79,
        "raw_name": "Gemma 3 27B Instruct",
        "raw_scores": {
          "intelligence_score": 10.31,
          "coding_score": 9.59,
          "context_window": 128000,
          "gdpval": 353.44474367806583,
          "terminalbench_hard": 0.038,
          "tau2": 0.105,
          "lcr": 5.7,
          "hle": 4.7,
          "gpqa": 42.8,
          "scicode": 21.2,
          "ifbench": 31.8,
          "aime25": 20.7,
          "critpt": 0.0,
          "mmmu_pro": 48.0,
          "livecodebench": 13.7
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 10.31,
        "coding_score": 9.59,
        "gdpval": 353.44474367806583,
        "terminalbench_hard": 0.038,
        "tau2": 0.105,
        "lcr": 5.7,
        "hle": 4.7,
        "gpqa": 42.8,
        "scicode": 21.2,
        "ifbench": 31.8,
        "aime25": 20.7,
        "critpt": 0.0,
        "mmmu_pro": 48.0,
        "livecodebench": 13.7
      }
    },
    "confidence_score": 0.79,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "gemini-3-flash-preview",
    "canonical_name": "Gemini 3 Flash Preview (Non-reasoning)",
    "model_name": "Gemini 3 Flash Preview (Non-reasoning)",
    "aliases": [
      "Gemini 3 Flash Preview (Non-reasoning)",
      "Gemini 3 Flash Preview (Reasoning)"
    ],
    "provider": "Google",
    "context_window": 1000000,
    "open_source": null,
    "arena_votes": null,
    "license_type": null,
    "creator": null,
    "intelligence_score": 40.74,
    "coding_score": 40.23,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": null,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": 1156.7402,
    "terminalbench_hard": 0.352,
    "tau2": 0.6185,
    "lcr": 57.15,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": 24.4,
    "gpqa": 85.5,
    "scicode": 50.25,
    "ifbench": 66.55,
    "aime25": 76.35,
    "critpt": 5.0,
    "mmmu_pro": 79.25,
    "livecodebench": 85.25,
    "sources": [
      "Artificial Analysis"
    ],
    "source_count": 1,
    "data_tier": 2,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.904023+00:00",
        "confidence": 0.79,
        "raw_name": "Gemini 3 Flash Preview (Non-reasoning)",
        "raw_scores": {
          "intelligence_score": 35.05,
          "coding_score": 37.84,
          "context_window": 1000000,
          "gdpval": 1122.2169313346094,
          "terminalbench_hard": 0.318,
          "tau2": 0.433,
          "lcr": 48.0,
          "hle": 14.1,
          "gpqa": 81.2,
          "scicode": 49.9,
          "ifbench": 55.1,
          "aime25": 55.7,
          "critpt": 1.4,
          "mmmu_pro": 78.6,
          "livecodebench": 79.7
        }
      },
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.904489+00:00",
        "confidence": 0.79,
        "raw_name": "Gemini 3 Flash Preview (Reasoning)",
        "raw_scores": {
          "intelligence_score": 46.43,
          "coding_score": 42.62,
          "context_window": 1000000,
          "gdpval": 1191.2633766321467,
          "terminalbench_hard": 0.386,
          "tau2": 0.804,
          "lcr": 66.3,
          "hle": 34.7,
          "gpqa": 89.8,
          "scicode": 50.6,
          "ifbench": 78.0,
          "aime25": 97.0,
          "critpt": 8.6,
          "mmmu_pro": 79.9,
          "livecodebench": 90.8
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 35.05,
        "coding_score": 37.84,
        "gdpval": 1122.2169313346094,
        "terminalbench_hard": 0.318,
        "tau2": 0.433,
        "lcr": 48.0,
        "hle": 14.1,
        "gpqa": 81.2,
        "scicode": 49.9,
        "ifbench": 55.1,
        "aime25": 55.7,
        "critpt": 1.4,
        "mmmu_pro": 78.6,
        "livecodebench": 79.7
      }
    },
    "confidence_score": 0.79,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "gemini-25-flash-lite-preview",
    "canonical_name": "Gemini 2.5 Flash-Lite Preview (Sep '25) (Non-reasoning)",
    "model_name": "Gemini 2.5 Flash-Lite Preview (Sep '25) (Non-reasoning)",
    "aliases": [
      "Gemini 2.5 Flash-Lite Preview (Sep '25) (Non-reasoning)",
      "Gemini 2.5 Flash-Lite Preview (Sep '25) (Reasoning)"
    ],
    "provider": "Google",
    "context_window": 1000000,
    "open_source": null,
    "arena_votes": null,
    "license_type": null,
    "creator": null,
    "intelligence_score": 23.7077,
    "coding_score": 16.345,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": null,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": 454.0255,
    "terminalbench_hard": 0.1025,
    "tau2": 0.3055,
    "lcr": 53.5,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": 5.6,
    "gpqa": 68.0,
    "scicode": 28.6,
    "ifbench": 47.2,
    "aime25": 57.7,
    "critpt": 0.0,
    "mmmu_pro": 64.2,
    "livecodebench": 66.45,
    "sources": [
      "Artificial Analysis"
    ],
    "source_count": 1,
    "data_tier": 2,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.904065+00:00",
        "confidence": 0.79,
        "raw_name": "Gemini 2.5 Flash-Lite Preview (Sep '25) (Non-reasoning)",
        "raw_scores": {
          "intelligence_score": 21.736436480231458,
          "coding_score": 14.54,
          "context_window": 1000000,
          "gdpval": 433.20551367501196,
          "terminalbench_hard": 0.076,
          "tau2": 0.304,
          "lcr": 48.0,
          "hle": 4.6,
          "gpqa": 65.1,
          "scicode": 28.5,
          "ifbench": 41.8,
          "aime25": 46.7,
          "critpt": 0.0,
          "mmmu_pro": 63.4,
          "livecodebench": 64.1
        }
      },
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.904532+00:00",
        "confidence": 0.79,
        "raw_name": "Gemini 2.5 Flash-Lite Preview (Sep '25) (Reasoning)",
        "raw_scores": {
          "intelligence_score": 25.679036352531774,
          "coding_score": 18.15,
          "context_window": 1000000,
          "gdpval": 474.8454094119413,
          "terminalbench_hard": 0.129,
          "tau2": 0.307,
          "lcr": 59.0,
          "hle": 6.6,
          "gpqa": 70.9,
          "scicode": 28.7,
          "ifbench": 52.6,
          "aime25": 68.7,
          "critpt": 0.0,
          "mmmu_pro": 65.0,
          "livecodebench": 68.8
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 21.736436480231458,
        "coding_score": 14.54,
        "gdpval": 433.20551367501196,
        "terminalbench_hard": 0.076,
        "tau2": 0.304,
        "lcr": 48.0,
        "hle": 4.6,
        "gpqa": 65.1,
        "scicode": 28.5,
        "ifbench": 41.8,
        "aime25": 46.7,
        "critpt": 0.0,
        "mmmu_pro": 63.4,
        "livecodebench": 64.1
      }
    },
    "confidence_score": 0.79,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "gemma-3-270m",
    "canonical_name": "Gemma 3 270M",
    "model_name": "Gemma 3 270M",
    "aliases": [
      "Gemma 3 270M"
    ],
    "provider": "Google",
    "context_window": 32000,
    "open_source": true,
    "arena_votes": null,
    "license_type": "Gemma",
    "creator": null,
    "intelligence_score": 8.3728,
    "coding_score": 0.0,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": null,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": 0.0,
    "latency_seconds": 0.0,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": 0.0,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": 0.0,
    "tau2": 0.091,
    "lcr": 0.0,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": 4.2,
    "gpqa": 22.4,
    "scicode": 0.0,
    "ifbench": 12.1,
    "aime25": 2.3,
    "critpt": 0.0,
    "mmmu_pro": null,
    "livecodebench": 0.3,
    "sources": [
      "Artificial Analysis"
    ],
    "source_count": 1,
    "data_tier": 2,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.904104+00:00",
        "confidence": 1.0,
        "raw_name": "Gemma 3 270M",
        "raw_scores": {
          "intelligence_score": 8.372813365251153,
          "coding_score": 0.0,
          "blended_cost_per_1m": 0.0,
          "latency_seconds": 0.0,
          "tokens_per_second": 0.0,
          "context_window": 32000,
          "terminalbench_hard": 0.0,
          "tau2": 0.091,
          "lcr": 0.0,
          "hle": 4.2,
          "gpqa": 22.4,
          "scicode": 0.0,
          "ifbench": 12.1,
          "aime25": 2.3,
          "critpt": 0.0,
          "livecodebench": 0.3
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 8.372813365251153,
        "coding_score": 0.0,
        "terminalbench_hard": 0.0,
        "tau2": 0.091,
        "lcr": 0.0,
        "hle": 4.2,
        "gpqa": 22.4,
        "scicode": 0.0,
        "ifbench": 12.1,
        "aime25": 2.3,
        "critpt": 0.0,
        "livecodebench": 0.3
      }
    },
    "confidence_score": 1.0,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "gemma-3n-e2b-instruct",
    "canonical_name": "Gemma 3n E2B Instruct",
    "model_name": "Gemma 3n E2B Instruct",
    "aliases": [
      "Gemma 3n E2B Instruct"
    ],
    "provider": "Google",
    "context_window": 32000,
    "open_source": true,
    "arena_votes": null,
    "license_type": "Gemma license",
    "creator": null,
    "intelligence_score": 9.7277,
    "coding_score": 2.24,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": null,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": 0.008,
    "tau2": 0.0,
    "lcr": 0.0,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": 4.0,
    "gpqa": 22.9,
    "scicode": 5.2,
    "ifbench": 22.0,
    "aime25": 10.3,
    "critpt": 0.0,
    "mmmu_pro": null,
    "livecodebench": 9.5,
    "sources": [
      "Artificial Analysis"
    ],
    "source_count": 1,
    "data_tier": 2,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.904141+00:00",
        "confidence": 0.79,
        "raw_name": "Gemma 3n E2B Instruct",
        "raw_scores": {
          "intelligence_score": 9.727723997995453,
          "coding_score": 2.24,
          "context_window": 32000,
          "terminalbench_hard": 0.008,
          "tau2": 0.0,
          "lcr": 0.0,
          "hle": 4.0,
          "gpqa": 22.9,
          "scicode": 5.2,
          "ifbench": 22.0,
          "aime25": 10.3,
          "critpt": 0.0,
          "livecodebench": 9.5
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 9.727723997995453,
        "coding_score": 2.24,
        "terminalbench_hard": 0.008,
        "tau2": 0.0,
        "lcr": 0.0,
        "hle": 4.0,
        "gpqa": 22.9,
        "scicode": 5.2,
        "ifbench": 22.0,
        "aime25": 10.3,
        "critpt": 0.0,
        "livecodebench": 9.5
      }
    },
    "confidence_score": 0.79,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "gemma-3-12b-instruct",
    "canonical_name": "Gemma 3 12B Instruct",
    "model_name": "Gemma 3 12B Instruct",
    "aliases": [
      "Gemma 3 12B Instruct"
    ],
    "provider": "Google",
    "context_window": 128000,
    "open_source": true,
    "arena_votes": null,
    "license_type": "Gemma license",
    "creator": null,
    "intelligence_score": 12.3806,
    "coding_score": 6.29,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": null,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": 351.9075,
    "terminalbench_hard": 0.008,
    "tau2": 0.108,
    "lcr": 6.7,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": 4.8,
    "gpqa": 34.9,
    "scicode": 17.4,
    "ifbench": 36.7,
    "aime25": 18.3,
    "critpt": 0.0,
    "mmmu_pro": 37.5,
    "livecodebench": 13.7,
    "sources": [
      "Artificial Analysis"
    ],
    "source_count": 1,
    "data_tier": 2,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.904182+00:00",
        "confidence": 0.79,
        "raw_name": "Gemma 3 12B Instruct",
        "raw_scores": {
          "intelligence_score": 12.380590362401561,
          "coding_score": 6.29,
          "context_window": 128000,
          "gdpval": 351.9075155349133,
          "terminalbench_hard": 0.008,
          "tau2": 0.108,
          "lcr": 6.7,
          "hle": 4.8,
          "gpqa": 34.9,
          "scicode": 17.4,
          "ifbench": 36.7,
          "aime25": 18.3,
          "critpt": 0.0,
          "mmmu_pro": 37.5,
          "livecodebench": 13.7
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 12.380590362401561,
        "coding_score": 6.29,
        "gdpval": 351.9075155349133,
        "terminalbench_hard": 0.008,
        "tau2": 0.108,
        "lcr": 6.7,
        "hle": 4.8,
        "gpqa": 34.9,
        "scicode": 17.4,
        "ifbench": 36.7,
        "aime25": 18.3,
        "critpt": 0.0,
        "mmmu_pro": 37.5,
        "livecodebench": 13.7
      }
    },
    "confidence_score": 0.79,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "gemini-25-pro",
    "canonical_name": "Gemini 2.5 Pro",
    "model_name": "Gemini 2.5 Pro",
    "aliases": [
      "Gemini 2.5 Pro",
      "gemini-2.5-pro"
    ],
    "provider": "Google",
    "context_window": 1000000,
    "open_source": null,
    "arena_votes": 97296,
    "license_type": "Proprietary",
    "creator": null,
    "intelligence_score": 34.63,
    "coding_score": 31.95,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 1449.24,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": 3.44,
    "latency_seconds": 35.93,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": 152.0,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": 926.9539,
    "terminalbench_hard": 0.265,
    "tau2": 0.541,
    "lcr": 66.0,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": 21.1,
    "gpqa": 84.4,
    "scicode": 42.8,
    "ifbench": 48.7,
    "aime25": 87.7,
    "critpt": 2.6,
    "mmmu_pro": 74.9,
    "livecodebench": 80.1,
    "sources": [
      "Artificial Analysis",
      "LMSYS Chatbot Arena"
    ],
    "source_count": 2,
    "data_tier": 1,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.904243+00:00",
        "confidence": 1.0,
        "raw_name": "Gemini 2.5 Pro",
        "raw_scores": {
          "intelligence_score": 34.63,
          "coding_score": 31.95,
          "blended_cost_per_1m": 3.44,
          "latency_seconds": 35.93,
          "tokens_per_second": 152.0,
          "context_window": 1000000,
          "gdpval": 926.953872421449,
          "terminalbench_hard": 0.265,
          "tau2": 0.541,
          "lcr": 66.0,
          "hle": 21.1,
          "gpqa": 84.4,
          "scicode": 42.8,
          "ifbench": 48.7,
          "aime25": 87.7,
          "critpt": 2.6,
          "mmmu_pro": 74.9,
          "livecodebench": 80.1
        }
      },
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.891579+00:00",
        "confidence": 1.0,
        "raw_name": "gemini-2.5-pro",
        "raw_scores": {
          "arena_elo": 1449.24,
          "arena_votes": 97296
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 34.63,
        "coding_score": 31.95,
        "gdpval": 926.953872421449,
        "terminalbench_hard": 0.265,
        "tau2": 0.541,
        "lcr": 66.0,
        "hle": 21.1,
        "gpqa": 84.4,
        "scicode": 42.8,
        "ifbench": 48.7,
        "aime25": 87.7,
        "critpt": 2.6,
        "mmmu_pro": 74.9,
        "livecodebench": 80.1
      },
      "LMSYS Chatbot Arena": {
        "arena_elo": 1449.24
      }
    },
    "confidence_score": 1.0,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "gemma-3-4b-instruct",
    "canonical_name": "Gemma 3 4B Instruct",
    "model_name": "Gemma 3 4B Instruct",
    "aliases": [
      "Gemma 3 4B Instruct"
    ],
    "provider": "Google",
    "context_window": 128000,
    "open_source": true,
    "arena_votes": null,
    "license_type": "Gemma license",
    "creator": null,
    "intelligence_score": 10.6534,
    "coding_score": 2.94,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": null,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": 315.5857,
    "terminalbench_hard": 0.008,
    "tau2": 0.05,
    "lcr": 5.7,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": 5.2,
    "gpqa": 29.1,
    "scicode": 7.3,
    "ifbench": 28.3,
    "aime25": 12.7,
    "critpt": 0.0,
    "mmmu_pro": 29.9,
    "livecodebench": 11.2,
    "sources": [
      "Artificial Analysis"
    ],
    "source_count": 1,
    "data_tier": 2,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.904286+00:00",
        "confidence": 0.79,
        "raw_name": "Gemma 3 4B Instruct",
        "raw_scores": {
          "intelligence_score": 10.653355925612512,
          "coding_score": 2.94,
          "context_window": 128000,
          "gdpval": 315.58568454415104,
          "terminalbench_hard": 0.008,
          "tau2": 0.05,
          "lcr": 5.7,
          "hle": 5.2,
          "gpqa": 29.1,
          "scicode": 7.3,
          "ifbench": 28.3,
          "aime25": 12.7,
          "critpt": 0.0,
          "mmmu_pro": 29.9,
          "livecodebench": 11.2
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 10.653355925612512,
        "coding_score": 2.94,
        "gdpval": 315.58568454415104,
        "terminalbench_hard": 0.008,
        "tau2": 0.05,
        "lcr": 5.7,
        "hle": 5.2,
        "gpqa": 29.1,
        "scicode": 7.3,
        "ifbench": 28.3,
        "aime25": 12.7,
        "critpt": 0.0,
        "mmmu_pro": 29.9,
        "livecodebench": 11.2
      }
    },
    "confidence_score": 0.79,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "gemini-3-pro",
    "canonical_name": "Gemini 3 Pro Preview (low)",
    "model_name": "Gemini 3 Pro Preview (low)",
    "aliases": [
      "Gemini 3 Pro Preview (high)",
      "Gemini 3 Pro Preview (low)",
      "gemini-3-pro"
    ],
    "provider": "Google",
    "context_window": 1000000,
    "open_source": null,
    "arena_votes": 38248,
    "license_type": "Proprietary",
    "creator": null,
    "intelligence_score": 44.845,
    "coding_score": 42.925,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 1486.23,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": 4.5,
    "latency_seconds": 16.86,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": 133.5,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": 1188.0363,
    "terminalbench_hard": 0.379,
    "tau2": 0.776,
    "lcr": 69.0,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": 32.4,
    "gpqa": 89.75,
    "scicode": 53.0,
    "ifbench": 60.05,
    "aime25": 91.2,
    "critpt": 4.55,
    "mmmu_pro": 80.2,
    "livecodebench": 88.7,
    "sources": [
      "Artificial Analysis",
      "LMSYS Chatbot Arena"
    ],
    "source_count": 2,
    "data_tier": 1,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.904330+00:00",
        "confidence": 1.0,
        "raw_name": "Gemini 3 Pro Preview (low)",
        "raw_scores": {
          "intelligence_score": 41.3,
          "coding_score": 39.36,
          "blended_cost_per_1m": 4.5,
          "latency_seconds": 4.13,
          "tokens_per_second": 129.0,
          "context_window": 1000000,
          "gdpval": 1174.7491059910863,
          "terminalbench_hard": 0.341,
          "tau2": 0.681,
          "lcr": 67.3,
          "hle": 27.6,
          "gpqa": 88.7,
          "scicode": 49.9,
          "ifbench": 49.7,
          "aime25": 86.7,
          "critpt": 0.0,
          "livecodebench": 85.7
        }
      },
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.913747+00:00",
        "confidence": 1.0,
        "raw_name": "Gemini 3 Pro Preview (high)",
        "raw_scores": {
          "intelligence_score": 48.39,
          "coding_score": 46.49,
          "blended_cost_per_1m": 4.5,
          "latency_seconds": 29.59,
          "tokens_per_second": 138.0,
          "context_window": 1000000,
          "gdpval": 1201.3235057371494,
          "terminalbench_hard": 0.417,
          "tau2": 0.871,
          "lcr": 70.7,
          "hle": 37.2,
          "gpqa": 90.8,
          "scicode": 56.1,
          "ifbench": 70.4,
          "aime25": 95.7,
          "critpt": 9.1,
          "mmmu_pro": 80.2,
          "livecodebench": 91.7
        }
      },
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.891331+00:00",
        "confidence": 1.0,
        "raw_name": "gemini-3-pro",
        "raw_scores": {
          "arena_elo": 1486.23,
          "arena_votes": 38248
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 41.3,
        "coding_score": 39.36,
        "gdpval": 1174.7491059910863,
        "terminalbench_hard": 0.341,
        "tau2": 0.681,
        "lcr": 67.3,
        "hle": 27.6,
        "gpqa": 88.7,
        "scicode": 49.9,
        "ifbench": 49.7,
        "aime25": 86.7,
        "critpt": 0.0,
        "livecodebench": 85.7,
        "mmmu_pro": 80.2
      },
      "LMSYS Chatbot Arena": {
        "arena_elo": 1486.23
      }
    },
    "confidence_score": 1.0,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "gemini-31-pro-preview",
    "canonical_name": "Gemini 3.1 Pro Preview",
    "model_name": "Gemini 3.1 Pro Preview",
    "aliases": [
      "Gemini 3.1 Pro Preview",
      "gemini-3.1-pro-preview"
    ],
    "provider": "Google",
    "context_window": 1000000,
    "open_source": null,
    "arena_votes": 4052,
    "license_type": "Proprietary",
    "creator": null,
    "intelligence_score": 57.18,
    "coding_score": 55.5,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 1500.36,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": 4.5,
    "latency_seconds": 35.19,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": 91.0,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": 1308.2282,
    "terminalbench_hard": 0.538,
    "tau2": 0.956,
    "lcr": 72.7,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": 44.7,
    "gpqa": 94.1,
    "scicode": 58.9,
    "ifbench": 77.1,
    "aime25": null,
    "critpt": 17.7,
    "mmmu_pro": 82.4,
    "livecodebench": null,
    "sources": [
      "Artificial Analysis",
      "LMSYS Chatbot Arena"
    ],
    "source_count": 2,
    "data_tier": 1,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.904370+00:00",
        "confidence": 0.8600000000000001,
        "raw_name": "Gemini 3.1 Pro Preview",
        "raw_scores": {
          "intelligence_score": 57.18,
          "coding_score": 55.5,
          "blended_cost_per_1m": 4.5,
          "latency_seconds": 35.19,
          "tokens_per_second": 91.0,
          "context_window": 1000000,
          "gdpval": 1308.2281789018216,
          "terminalbench_hard": 0.538,
          "tau2": 0.956,
          "lcr": 72.7,
          "hle": 44.7,
          "gpqa": 94.1,
          "scicode": 58.9,
          "ifbench": 77.1,
          "critpt": 17.7,
          "mmmu_pro": 82.4
        }
      },
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.891302+00:00",
        "confidence": 1.0,
        "raw_name": "gemini-3.1-pro-preview",
        "raw_scores": {
          "arena_elo": 1500.36,
          "arena_votes": 4052
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 57.18,
        "coding_score": 55.5,
        "gdpval": 1308.2281789018216,
        "terminalbench_hard": 0.538,
        "tau2": 0.956,
        "lcr": 72.7,
        "hle": 44.7,
        "gpqa": 94.1,
        "scicode": 58.9,
        "ifbench": 77.1,
        "critpt": 17.7,
        "mmmu_pro": 82.4
      },
      "LMSYS Chatbot Arena": {
        "arena_elo": 1500.36
      }
    },
    "confidence_score": 0.93,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "gemma-3-1b-instruct",
    "canonical_name": "Gemma 3 1B Instruct",
    "model_name": "Gemma 3 1B Instruct",
    "aliases": [
      "Gemma 3 1B Instruct"
    ],
    "provider": "Google",
    "context_window": 32000,
    "open_source": true,
    "arena_votes": null,
    "license_type": "Gemma license",
    "creator": null,
    "intelligence_score": 8.6479,
    "coding_score": 0.23,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": null,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": 0.0,
    "tau2": 0.105,
    "lcr": 0.0,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": 5.2,
    "gpqa": 23.7,
    "scicode": 0.7,
    "ifbench": 19.9,
    "aime25": 3.3,
    "critpt": 0.0,
    "mmmu_pro": null,
    "livecodebench": 1.7,
    "sources": [
      "Artificial Analysis"
    ],
    "source_count": 1,
    "data_tier": 2,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.904407+00:00",
        "confidence": 0.79,
        "raw_name": "Gemma 3 1B Instruct",
        "raw_scores": {
          "intelligence_score": 8.647929714252196,
          "coding_score": 0.23,
          "context_window": 32000,
          "terminalbench_hard": 0.0,
          "tau2": 0.105,
          "lcr": 0.0,
          "hle": 5.2,
          "gpqa": 23.7,
          "scicode": 0.7,
          "ifbench": 19.9,
          "aime25": 3.3,
          "critpt": 0.0,
          "livecodebench": 1.7
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 8.647929714252196,
        "coding_score": 0.23,
        "terminalbench_hard": 0.0,
        "tau2": 0.105,
        "lcr": 0.0,
        "hle": 5.2,
        "gpqa": 23.7,
        "scicode": 0.7,
        "ifbench": 19.9,
        "aime25": 3.3,
        "critpt": 0.0,
        "livecodebench": 1.7
      }
    },
    "confidence_score": 0.79,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "gemma-3n-e4b-instruct",
    "canonical_name": "Gemma 3n E4B Instruct",
    "model_name": "Gemma 3n E4B Instruct",
    "aliases": [
      "Gemma 3n E4B Instruct"
    ],
    "provider": "Google",
    "context_window": 32000,
    "open_source": true,
    "arena_votes": null,
    "license_type": "Gemma license",
    "creator": null,
    "intelligence_score": 10.8845,
    "coding_score": 4.22,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": null,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": 296.6924,
    "terminalbench_hard": 0.023,
    "tau2": 0.05,
    "lcr": 0.0,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": 4.4,
    "gpqa": 29.6,
    "scicode": 8.1,
    "ifbench": 27.9,
    "aime25": 14.3,
    "critpt": 0.0,
    "mmmu_pro": 26.2,
    "livecodebench": 14.6,
    "sources": [
      "Artificial Analysis"
    ],
    "source_count": 1,
    "data_tier": 2,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.904447+00:00",
        "confidence": 0.79,
        "raw_name": "Gemma 3n E4B Instruct",
        "raw_scores": {
          "intelligence_score": 10.884502112092596,
          "coding_score": 4.22,
          "context_window": 32000,
          "gdpval": 296.69237987693623,
          "terminalbench_hard": 0.023,
          "tau2": 0.05,
          "lcr": 0.0,
          "hle": 4.4,
          "gpqa": 29.6,
          "scicode": 8.1,
          "ifbench": 27.9,
          "aime25": 14.3,
          "critpt": 0.0,
          "mmmu_pro": 26.2,
          "livecodebench": 14.6
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 10.884502112092596,
        "coding_score": 4.22,
        "gdpval": 296.69237987693623,
        "terminalbench_hard": 0.023,
        "tau2": 0.05,
        "lcr": 0.0,
        "hle": 4.4,
        "gpqa": 29.6,
        "scicode": 8.1,
        "ifbench": 27.9,
        "aime25": 14.3,
        "critpt": 0.0,
        "mmmu_pro": 26.2,
        "livecodebench": 14.6
      }
    },
    "confidence_score": 0.79,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "claude-sonnet-46",
    "canonical_name": "Claude Sonnet 4.6 (Non-reasoning, High Effort)",
    "model_name": "Claude Sonnet 4.6 (Non-reasoning, High Effort)",
    "aliases": [
      "Claude Sonnet 4.6 (Adaptive Reasoning, Max Effort)",
      "Claude Sonnet 4.6 (Non-reasoning, High Effort)",
      "Claude Sonnet 4.6 (Non-reasoning, Low Effort)",
      "claude-sonnet-4.6"
    ],
    "provider": "Anthropic",
    "context_window": 200000,
    "open_source": null,
    "arena_votes": 3470,
    "license_type": "Proprietary",
    "creator": null,
    "intelligence_score": 45.8801,
    "coding_score": 46.4136,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 1458.05,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": 6.0,
    "latency_seconds": 0.95,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": 61.0,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": 1530.6536,
    "terminalbench_hard": 0.4673,
    "tau2": 0.7812,
    "lcr": 62.0102,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": 17.3,
    "gpqa": 82.1074,
    "scicode": 45.7551,
    "ifbench": 46.312,
    "aime25": null,
    "critpt": 1.562,
    "mmmu_pro": 70.8551,
    "livecodebench": null,
    "sources": [
      "Artificial Analysis",
      "LMSYS Chatbot Arena"
    ],
    "source_count": 2,
    "data_tier": 1,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.904567+00:00",
        "confidence": 0.65,
        "raw_name": "Claude Sonnet 4.6 (Non-reasoning, High Effort)",
        "raw_scores": {
          "intelligence_score": 44.38,
          "coding_score": 46.43,
          "context_window": 200000,
          "gdpval": 1553.3150150350505,
          "terminalbench_hard": 0.462,
          "tau2": 0.795,
          "lcr": 57.7,
          "hle": 13.2,
          "gpqa": 79.9,
          "scicode": 46.9,
          "ifbench": 41.2,
          "critpt": 0.9,
          "mmmu_pro": 70.6
        }
      },
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.904754+00:00",
        "confidence": 0.65,
        "raw_name": "Claude Sonnet 4.6 (Adaptive Reasoning, Max Effort)",
        "raw_scores": {
          "intelligence_score": 51.72,
          "coding_score": 50.94,
          "context_window": 200000,
          "gdpval": 1632.6648495109562,
          "terminalbench_hard": 0.53,
          "tau2": 0.757,
          "lcr": 70.7,
          "hle": 30.0,
          "gpqa": 87.5,
          "scicode": 46.8,
          "ifbench": 56.6,
          "critpt": 3.1,
          "mmmu_pro": 73.3
        }
      },
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.904793+00:00",
        "confidence": 0.8600000000000001,
        "raw_name": "Claude Sonnet 4.6 (Non-reasoning, Low Effort)",
        "raw_scores": {
          "intelligence_score": 42.6,
          "coding_score": 42.98,
          "blended_cost_per_1m": 6.0,
          "latency_seconds": 0.95,
          "tokens_per_second": 61.0,
          "context_window": 200000,
          "gdpval": 1436.4243515088237,
          "terminalbench_hard": 0.424,
          "tau2": 0.789,
          "lcr": 58.7,
          "hle": 10.8,
          "gpqa": 79.7,
          "scicode": 44.1,
          "ifbench": 42.4,
          "critpt": 0.9,
          "mmmu_pro": 69.2
        }
      },
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.891469+00:00",
        "confidence": 1.0,
        "raw_name": "claude-sonnet-4.6",
        "raw_scores": {
          "arena_elo": 1458.05,
          "arena_votes": 3470
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 44.38,
        "coding_score": 46.43,
        "gdpval": 1553.3150150350505,
        "terminalbench_hard": 0.462,
        "tau2": 0.795,
        "lcr": 57.7,
        "hle": 13.2,
        "gpqa": 79.9,
        "scicode": 46.9,
        "ifbench": 41.2,
        "critpt": 0.9,
        "mmmu_pro": 70.6
      },
      "LMSYS Chatbot Arena": {
        "arena_elo": 1458.05
      }
    },
    "confidence_score": 0.79,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "claude-opus-46",
    "canonical_name": "Claude Opus 4.6 (Adaptive Reasoning, Max Effort)",
    "model_name": "Claude Opus 4.6 (Adaptive Reasoning, Max Effort)",
    "aliases": [
      "Claude Opus 4.6 (Adaptive Reasoning, Max Effort)",
      "Claude Opus 4.6 (Non-reasoning, High Effort)",
      "claude-opus-4.6",
      "claude-opus-4.6-thinking"
    ],
    "provider": "Anthropic",
    "context_window": 200000,
    "open_source": null,
    "arena_votes": 7454,
    "license_type": "Proprietary",
    "creator": null,
    "intelligence_score": 49.705,
    "coding_score": 47.825,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 1503.18,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": 1592.5148,
    "terminalbench_hard": 0.4735,
    "tau2": 0.8845,
    "lcr": 64.5,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": 27.65,
    "gpqa": 86.8,
    "scicode": 48.8,
    "ifbench": 48.85,
    "aime25": null,
    "critpt": 7.7,
    "mmmu_pro": 73.95,
    "livecodebench": null,
    "sources": [
      "Artificial Analysis",
      "LMSYS Chatbot Arena"
    ],
    "source_count": 2,
    "data_tier": 1,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.904602+00:00",
        "confidence": 0.65,
        "raw_name": "Claude Opus 4.6 (Adaptive Reasoning, Max Effort)",
        "raw_scores": {
          "intelligence_score": 52.95,
          "coding_score": 48.09,
          "context_window": 200000,
          "gdpval": 1606.4175944685169,
          "terminalbench_hard": 0.462,
          "tau2": 0.921,
          "lcr": 70.7,
          "hle": 36.7,
          "gpqa": 89.6,
          "scicode": 51.9,
          "ifbench": 53.1,
          "critpt": 12.6,
          "mmmu_pro": 75.4
        }
      },
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.904637+00:00",
        "confidence": 0.65,
        "raw_name": "Claude Opus 4.6 (Non-reasoning, High Effort)",
        "raw_scores": {
          "intelligence_score": 46.46,
          "coding_score": 47.56,
          "context_window": 200000,
          "gdpval": 1578.6121034600746,
          "terminalbench_hard": 0.485,
          "tau2": 0.848,
          "lcr": 58.3,
          "hle": 18.6,
          "gpqa": 84.0,
          "scicode": 45.7,
          "ifbench": 44.6,
          "critpt": 2.8,
          "mmmu_pro": 72.5
        }
      },
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.891230+00:00",
        "confidence": 1.0,
        "raw_name": "claude-opus-4.6-thinking",
        "raw_scores": {
          "arena_elo": 1503.45,
          "arena_votes": 6583
        }
      },
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.891279+00:00",
        "confidence": 1.0,
        "raw_name": "claude-opus-4.6",
        "raw_scores": {
          "arena_elo": 1502.91,
          "arena_votes": 7454
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 52.95,
        "coding_score": 48.09,
        "gdpval": 1606.4175944685169,
        "terminalbench_hard": 0.462,
        "tau2": 0.921,
        "lcr": 70.7,
        "hle": 36.7,
        "gpqa": 89.6,
        "scicode": 51.9,
        "ifbench": 53.1,
        "critpt": 12.6,
        "mmmu_pro": 75.4
      },
      "LMSYS Chatbot Arena": {
        "arena_elo": 1503.45
      }
    },
    "confidence_score": 0.825,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "claude-45-haiku",
    "canonical_name": "Claude 4.5 Haiku (Reasoning)",
    "model_name": "Claude 4.5 Haiku (Reasoning)",
    "aliases": [
      "Claude 4.5 Haiku (Non-reasoning)",
      "Claude 4.5 Haiku (Reasoning)",
      "claude-haiku-4.5-20251001"
    ],
    "provider": "Anthropic",
    "context_window": 200000,
    "open_source": null,
    "arena_votes": 47369,
    "license_type": "Proprietary",
    "creator": null,
    "intelligence_score": 34.07,
    "coding_score": 31.125,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 1405.56,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": 1165.7531,
    "terminalbench_hard": 0.273,
    "tau2": 0.436,
    "lcr": 57.0,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": 7.0,
    "gpqa": 65.9,
    "scicode": 38.85,
    "ifbench": 48.15,
    "aime25": 61.35,
    "critpt": 0.0,
    "mmmu_pro": 56.7,
    "livecodebench": 56.3,
    "sources": [
      "Artificial Analysis",
      "LMSYS Chatbot Arena"
    ],
    "source_count": 2,
    "data_tier": 1,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.904678+00:00",
        "confidence": 0.79,
        "raw_name": "Claude 4.5 Haiku (Reasoning)",
        "raw_scores": {
          "intelligence_score": 37.09,
          "coding_score": 32.61,
          "context_window": 200000,
          "gdpval": 1167.9329114320517,
          "terminalbench_hard": 0.273,
          "tau2": 0.547,
          "lcr": 70.3,
          "hle": 9.7,
          "gpqa": 67.2,
          "scicode": 43.3,
          "ifbench": 54.3,
          "aime25": 83.7,
          "critpt": 0.0,
          "mmmu_pro": 58.3,
          "livecodebench": 61.5
        }
      },
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.904719+00:00",
        "confidence": 0.79,
        "raw_name": "Claude 4.5 Haiku (Non-reasoning)",
        "raw_scores": {
          "intelligence_score": 31.05,
          "coding_score": 29.64,
          "context_window": 200000,
          "gdpval": 1163.5732859582918,
          "terminalbench_hard": 0.273,
          "tau2": 0.325,
          "lcr": 43.7,
          "hle": 4.3,
          "gpqa": 64.6,
          "scicode": 34.4,
          "ifbench": 42.0,
          "aime25": 39.0,
          "critpt": 0.0,
          "mmmu_pro": 55.1,
          "livecodebench": 51.1
        }
      },
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.892167+00:00",
        "confidence": 1.0,
        "raw_name": "claude-haiku-4.5-20251001",
        "raw_scores": {
          "arena_elo": 1405.56,
          "arena_votes": 47369
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 37.09,
        "coding_score": 32.61,
        "gdpval": 1167.9329114320517,
        "terminalbench_hard": 0.273,
        "tau2": 0.547,
        "lcr": 70.3,
        "hle": 9.7,
        "gpqa": 67.2,
        "scicode": 43.3,
        "ifbench": 54.3,
        "aime25": 83.7,
        "critpt": 0.0,
        "mmmu_pro": 58.3,
        "livecodebench": 61.5
      },
      "LMSYS Chatbot Arena": {
        "arena_elo": 1405.56
      }
    },
    "confidence_score": 0.86,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "ministral-3-14b",
    "canonical_name": "Ministral 3 14B",
    "model_name": "Ministral 3 14B",
    "aliases": [
      "Ministral 3 14B"
    ],
    "provider": null,
    "context_window": 256000,
    "open_source": true,
    "arena_votes": null,
    "license_type": "Apache 2.0",
    "creator": null,
    "intelligence_score": 16.2187,
    "coding_score": 10.9,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": null,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": 0.2,
    "latency_seconds": 0.28,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": 140.0,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": 697.7619,
    "terminalbench_hard": 0.045,
    "tau2": 0.272,
    "lcr": 22.0,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": 4.6,
    "gpqa": 57.2,
    "scicode": 23.6,
    "ifbench": 32.0,
    "aime25": 30.0,
    "critpt": 0.0,
    "mmmu_pro": 49.8,
    "livecodebench": 35.1,
    "sources": [
      "Artificial Analysis"
    ],
    "source_count": 1,
    "data_tier": 2,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.904838+00:00",
        "confidence": 1.0,
        "raw_name": "Ministral 3 14B",
        "raw_scores": {
          "intelligence_score": 16.218700898434303,
          "coding_score": 10.9,
          "blended_cost_per_1m": 0.2,
          "latency_seconds": 0.28,
          "tokens_per_second": 140.0,
          "context_window": 256000,
          "gdpval": 697.7619317277707,
          "terminalbench_hard": 0.045,
          "tau2": 0.272,
          "lcr": 22.0,
          "hle": 4.6,
          "gpqa": 57.2,
          "scicode": 23.6,
          "ifbench": 32.0,
          "aime25": 30.0,
          "critpt": 0.0,
          "mmmu_pro": 49.8,
          "livecodebench": 35.1
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 16.218700898434303,
        "coding_score": 10.9,
        "gdpval": 697.7619317277707,
        "terminalbench_hard": 0.045,
        "tau2": 0.272,
        "lcr": 22.0,
        "hle": 4.6,
        "gpqa": 57.2,
        "scicode": 23.6,
        "ifbench": 32.0,
        "aime25": 30.0,
        "critpt": 0.0,
        "mmmu_pro": 49.8,
        "livecodebench": 35.1
      }
    },
    "confidence_score": 1.0,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "ministral-3-8b",
    "canonical_name": "Ministral 3 8B",
    "model_name": "Ministral 3 8B",
    "aliases": [
      "Ministral 3 8B"
    ],
    "provider": null,
    "context_window": 256000,
    "open_source": true,
    "arena_votes": null,
    "license_type": "Apache 2.0",
    "creator": null,
    "intelligence_score": 15.2516,
    "coding_score": 9.97,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": null,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": 0.15,
    "latency_seconds": 0.28,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": 184.0,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": 688.0685,
    "terminalbench_hard": 0.045,
    "tau2": 0.266,
    "lcr": 24.0,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": 4.3,
    "gpqa": 47.1,
    "scicode": 20.8,
    "ifbench": 29.1,
    "aime25": 31.7,
    "critpt": 0.0,
    "mmmu_pro": 46.0,
    "livecodebench": 30.3,
    "sources": [
      "Artificial Analysis"
    ],
    "source_count": 1,
    "data_tier": 2,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.904884+00:00",
        "confidence": 1.0,
        "raw_name": "Ministral 3 8B",
        "raw_scores": {
          "intelligence_score": 15.251608195023016,
          "coding_score": 9.97,
          "blended_cost_per_1m": 0.15,
          "latency_seconds": 0.28,
          "tokens_per_second": 184.0,
          "context_window": 256000,
          "gdpval": 688.0685459342236,
          "terminalbench_hard": 0.045,
          "tau2": 0.266,
          "lcr": 24.0,
          "hle": 4.3,
          "gpqa": 47.1,
          "scicode": 20.8,
          "ifbench": 29.1,
          "aime25": 31.7,
          "critpt": 0.0,
          "mmmu_pro": 46.0,
          "livecodebench": 30.3
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 15.251608195023016,
        "coding_score": 9.97,
        "gdpval": 688.0685459342236,
        "terminalbench_hard": 0.045,
        "tau2": 0.266,
        "lcr": 24.0,
        "hle": 4.3,
        "gpqa": 47.1,
        "scicode": 20.8,
        "ifbench": 29.1,
        "aime25": 31.7,
        "critpt": 0.0,
        "mmmu_pro": 46.0,
        "livecodebench": 30.3
      }
    },
    "confidence_score": 1.0,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "mistral-medium-31",
    "canonical_name": "Mistral Medium 3.1",
    "model_name": "Mistral Medium 3.1",
    "aliases": [
      "Mistral Medium 3.1"
    ],
    "provider": "Mistral AI",
    "context_window": 128000,
    "open_source": null,
    "arena_votes": null,
    "license_type": null,
    "creator": null,
    "intelligence_score": 18.4675,
    "coding_score": 18.34,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": null,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": 0.8,
    "latency_seconds": 0.39,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": 85.0,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": 820.5769,
    "terminalbench_hard": 0.106,
    "tau2": 0.406,
    "lcr": 19.7,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": 4.4,
    "gpqa": 58.8,
    "scicode": 33.8,
    "ifbench": 39.8,
    "aime25": 38.3,
    "critpt": 0.0,
    "mmmu_pro": 54.2,
    "livecodebench": 40.6,
    "sources": [
      "Artificial Analysis"
    ],
    "source_count": 1,
    "data_tier": 2,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.904928+00:00",
        "confidence": 1.0,
        "raw_name": "Mistral Medium 3.1",
        "raw_scores": {
          "intelligence_score": 18.467546584031638,
          "coding_score": 18.34,
          "blended_cost_per_1m": 0.8,
          "latency_seconds": 0.39,
          "tokens_per_second": 85.0,
          "context_window": 128000,
          "gdpval": 820.576938433752,
          "terminalbench_hard": 0.106,
          "tau2": 0.406,
          "lcr": 19.7,
          "hle": 4.4,
          "gpqa": 58.8,
          "scicode": 33.8,
          "ifbench": 39.8,
          "aime25": 38.3,
          "critpt": 0.0,
          "mmmu_pro": 54.2,
          "livecodebench": 40.6
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 18.467546584031638,
        "coding_score": 18.34,
        "gdpval": 820.576938433752,
        "terminalbench_hard": 0.106,
        "tau2": 0.406,
        "lcr": 19.7,
        "hle": 4.4,
        "gpqa": 58.8,
        "scicode": 33.8,
        "ifbench": 39.8,
        "aime25": 38.3,
        "critpt": 0.0,
        "mmmu_pro": 54.2,
        "livecodebench": 40.6
      }
    },
    "confidence_score": 1.0,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "devstral-2",
    "canonical_name": "Devstral 2",
    "model_name": "Devstral 2",
    "aliases": [
      "Devstral 2"
    ],
    "provider": null,
    "context_window": 256000,
    "open_source": false,
    "arena_votes": null,
    "license_type": "Modified MIT License",
    "creator": null,
    "intelligence_score": 18.9585,
    "coding_score": 23.66,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": null,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": 0.0,
    "latency_seconds": 0.39,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": 75.0,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": 892.5294,
    "terminalbench_hard": 0.189,
    "tau2": 0.249,
    "lcr": 30.0,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": 3.6,
    "gpqa": 59.4,
    "scicode": 33.1,
    "ifbench": 38.1,
    "aime25": 36.7,
    "critpt": 0.0,
    "mmmu_pro": null,
    "livecodebench": 44.8,
    "sources": [
      "Artificial Analysis"
    ],
    "source_count": 1,
    "data_tier": 2,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.904972+00:00",
        "confidence": 1.0,
        "raw_name": "Devstral 2",
        "raw_scores": {
          "intelligence_score": 18.958453990270915,
          "coding_score": 23.66,
          "blended_cost_per_1m": 0.0,
          "latency_seconds": 0.39,
          "tokens_per_second": 75.0,
          "context_window": 256000,
          "gdpval": 892.5293807356395,
          "terminalbench_hard": 0.189,
          "tau2": 0.249,
          "lcr": 30.0,
          "hle": 3.6,
          "gpqa": 59.4,
          "scicode": 33.1,
          "ifbench": 38.1,
          "aime25": 36.7,
          "critpt": 0.0,
          "livecodebench": 44.8
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 18.958453990270915,
        "coding_score": 23.66,
        "gdpval": 892.5293807356395,
        "terminalbench_hard": 0.189,
        "tau2": 0.249,
        "lcr": 30.0,
        "hle": 3.6,
        "gpqa": 59.4,
        "scicode": 33.1,
        "ifbench": 38.1,
        "aime25": 36.7,
        "critpt": 0.0,
        "livecodebench": 44.8
      }
    },
    "confidence_score": 1.0,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "mistral-small-32",
    "canonical_name": "Mistral Small 3.2",
    "model_name": "Mistral Small 3.2",
    "aliases": [
      "Mistral Small 3.2"
    ],
    "provider": "Mistral AI",
    "context_window": 128000,
    "open_source": true,
    "arena_votes": null,
    "license_type": "Apache 2.0",
    "creator": null,
    "intelligence_score": 15.07,
    "coding_score": 13.34,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": null,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": 0.15,
    "latency_seconds": 0.3,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": 147.0,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": 349.6604,
    "terminalbench_hard": 0.068,
    "tau2": 0.295,
    "lcr": 17.3,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": 4.3,
    "gpqa": 50.5,
    "scicode": 26.4,
    "ifbench": 33.5,
    "aime25": 27.0,
    "critpt": 0.0,
    "mmmu_pro": 48.0,
    "livecodebench": 27.5,
    "sources": [
      "Artificial Analysis"
    ],
    "source_count": 1,
    "data_tier": 2,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.905023+00:00",
        "confidence": 1.0,
        "raw_name": "Mistral Small 3.2",
        "raw_scores": {
          "intelligence_score": 15.07,
          "coding_score": 13.34,
          "blended_cost_per_1m": 0.15,
          "latency_seconds": 0.3,
          "tokens_per_second": 147.0,
          "context_window": 128000,
          "gdpval": 349.6603809282807,
          "terminalbench_hard": 0.068,
          "tau2": 0.295,
          "lcr": 17.3,
          "hle": 4.3,
          "gpqa": 50.5,
          "scicode": 26.4,
          "ifbench": 33.5,
          "aime25": 27.0,
          "critpt": 0.0,
          "mmmu_pro": 48.0,
          "livecodebench": 27.5
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 15.07,
        "coding_score": 13.34,
        "gdpval": 349.6603809282807,
        "terminalbench_hard": 0.068,
        "tau2": 0.295,
        "lcr": 17.3,
        "hle": 4.3,
        "gpqa": 50.5,
        "scicode": 26.4,
        "ifbench": 33.5,
        "aime25": 27.0,
        "critpt": 0.0,
        "mmmu_pro": 48.0,
        "livecodebench": 27.5
      }
    },
    "confidence_score": 1.0,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "magistral-small-12",
    "canonical_name": "Magistral Small 1.2",
    "model_name": "Magistral Small 1.2",
    "aliases": [
      "Magistral Small 1.2"
    ],
    "provider": null,
    "context_window": 128000,
    "open_source": true,
    "arena_votes": null,
    "license_type": "Apache 2.0",
    "creator": null,
    "intelligence_score": 22.5461,
    "coding_score": 14.76,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": null,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": 0.75,
    "latency_seconds": 0.36,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": 210.0,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": 715.9518,
    "terminalbench_hard": 0.045,
    "tau2": 0.278,
    "lcr": 16.3,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": 6.1,
    "gpqa": 66.3,
    "scicode": 35.2,
    "ifbench": 44.4,
    "aime25": 80.3,
    "critpt": 0.3,
    "mmmu_pro": 55.5,
    "livecodebench": 72.3,
    "sources": [
      "Artificial Analysis"
    ],
    "source_count": 1,
    "data_tier": 2,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.905068+00:00",
        "confidence": 1.0,
        "raw_name": "Magistral Small 1.2",
        "raw_scores": {
          "intelligence_score": 22.54605154658181,
          "coding_score": 14.76,
          "blended_cost_per_1m": 0.75,
          "latency_seconds": 0.36,
          "tokens_per_second": 210.0,
          "context_window": 128000,
          "gdpval": 715.9517769017065,
          "terminalbench_hard": 0.045,
          "tau2": 0.278,
          "lcr": 16.3,
          "hle": 6.1,
          "gpqa": 66.3,
          "scicode": 35.2,
          "ifbench": 44.4,
          "aime25": 80.3,
          "critpt": 0.3,
          "mmmu_pro": 55.5,
          "livecodebench": 72.3
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 22.54605154658181,
        "coding_score": 14.76,
        "gdpval": 715.9517769017065,
        "terminalbench_hard": 0.045,
        "tau2": 0.278,
        "lcr": 16.3,
        "hle": 6.1,
        "gpqa": 66.3,
        "scicode": 35.2,
        "ifbench": 44.4,
        "aime25": 80.3,
        "critpt": 0.3,
        "mmmu_pro": 55.5,
        "livecodebench": 72.3
      }
    },
    "confidence_score": 1.0,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "magistral-medium-12",
    "canonical_name": "Magistral Medium 1.2",
    "model_name": "Magistral Medium 1.2",
    "aliases": [
      "Magistral Medium 1.2"
    ],
    "provider": null,
    "context_window": 128000,
    "open_source": null,
    "arena_votes": null,
    "license_type": null,
    "creator": null,
    "intelligence_score": 27.1,
    "coding_score": 21.66,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": null,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": 2.75,
    "latency_seconds": 0.45,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": 83.0,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": 663.3066,
    "terminalbench_hard": 0.129,
    "tau2": 0.52,
    "lcr": 51.3,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": 9.6,
    "gpqa": 73.9,
    "scicode": 39.2,
    "ifbench": 43.0,
    "aime25": 82.0,
    "critpt": 0.3,
    "mmmu_pro": 59.7,
    "livecodebench": 75.0,
    "sources": [
      "Artificial Analysis"
    ],
    "source_count": 1,
    "data_tier": 2,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.905113+00:00",
        "confidence": 1.0,
        "raw_name": "Magistral Medium 1.2",
        "raw_scores": {
          "intelligence_score": 27.1,
          "coding_score": 21.66,
          "blended_cost_per_1m": 2.75,
          "latency_seconds": 0.45,
          "tokens_per_second": 83.0,
          "context_window": 128000,
          "gdpval": 663.306641439205,
          "terminalbench_hard": 0.129,
          "tau2": 0.52,
          "lcr": 51.3,
          "hle": 9.6,
          "gpqa": 73.9,
          "scicode": 39.2,
          "ifbench": 43.0,
          "aime25": 82.0,
          "critpt": 0.3,
          "mmmu_pro": 59.7,
          "livecodebench": 75.0
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 27.1,
        "coding_score": 21.66,
        "gdpval": 663.306641439205,
        "terminalbench_hard": 0.129,
        "tau2": 0.52,
        "lcr": 51.3,
        "hle": 9.6,
        "gpqa": 73.9,
        "scicode": 39.2,
        "ifbench": 43.0,
        "aime25": 82.0,
        "critpt": 0.3,
        "mmmu_pro": 59.7,
        "livecodebench": 75.0
      }
    },
    "confidence_score": 1.0,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "mistral-large-3",
    "canonical_name": "Mistral Large 3",
    "model_name": "Mistral Large 3",
    "aliases": [
      "Mistral Large 3",
      "mistral-large-3"
    ],
    "provider": "Mistral",
    "context_window": 256000,
    "open_source": true,
    "arena_votes": 27128,
    "license_type": "Apache 2.0",
    "creator": null,
    "intelligence_score": 22.8,
    "coding_score": 22.68,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 1414.85,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": 0.75,
    "latency_seconds": 0.88,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": 50.0,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": 890.9981,
    "terminalbench_hard": 0.159,
    "tau2": 0.246,
    "lcr": 34.7,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": 4.1,
    "gpqa": 68.0,
    "scicode": 36.2,
    "ifbench": 36.2,
    "aime25": 38.0,
    "critpt": 0.0,
    "mmmu_pro": 55.7,
    "livecodebench": 46.5,
    "sources": [
      "Artificial Analysis",
      "LMSYS Chatbot Arena"
    ],
    "source_count": 2,
    "data_tier": 1,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.905158+00:00",
        "confidence": 1.0,
        "raw_name": "Mistral Large 3",
        "raw_scores": {
          "intelligence_score": 22.8,
          "coding_score": 22.68,
          "blended_cost_per_1m": 0.75,
          "latency_seconds": 0.88,
          "tokens_per_second": 50.0,
          "context_window": 256000,
          "gdpval": 890.9981268069312,
          "terminalbench_hard": 0.159,
          "tau2": 0.246,
          "lcr": 34.7,
          "hle": 4.1,
          "gpqa": 68.0,
          "scicode": 36.2,
          "ifbench": 36.2,
          "aime25": 38.0,
          "critpt": 0.0,
          "mmmu_pro": 55.7,
          "livecodebench": 46.5
        }
      },
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.892049+00:00",
        "confidence": 1.0,
        "raw_name": "mistral-large-3",
        "raw_scores": {
          "arena_elo": 1414.85,
          "arena_votes": 27128
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 22.8,
        "coding_score": 22.68,
        "gdpval": 890.9981268069312,
        "terminalbench_hard": 0.159,
        "tau2": 0.246,
        "lcr": 34.7,
        "hle": 4.1,
        "gpqa": 68.0,
        "scicode": 36.2,
        "ifbench": 36.2,
        "aime25": 38.0,
        "critpt": 0.0,
        "mmmu_pro": 55.7,
        "livecodebench": 46.5
      },
      "LMSYS Chatbot Arena": {
        "arena_elo": 1414.85
      }
    },
    "confidence_score": 1.0,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "ministral-3-3b",
    "canonical_name": "Ministral 3 3B",
    "model_name": "Ministral 3 3B",
    "aliases": [
      "Ministral 3 3B"
    ],
    "provider": null,
    "context_window": 256000,
    "open_source": true,
    "arena_votes": null,
    "license_type": "Apache 2.0",
    "creator": null,
    "intelligence_score": 12.8588,
    "coding_score": 4.78,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": null,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": 0.1,
    "latency_seconds": 0.27,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": 292.0,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": 538.0455,
    "terminalbench_hard": 0.0,
    "tau2": 0.249,
    "lcr": 11.7,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": 5.3,
    "gpqa": 35.8,
    "scicode": 14.4,
    "ifbench": 26.8,
    "aime25": 22.0,
    "critpt": 0.0,
    "mmmu_pro": 38.1,
    "livecodebench": 24.7,
    "sources": [
      "Artificial Analysis"
    ],
    "source_count": 1,
    "data_tier": 2,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.905218+00:00",
        "confidence": 1.0,
        "raw_name": "Ministral 3 3B",
        "raw_scores": {
          "intelligence_score": 12.858755650671329,
          "coding_score": 4.78,
          "blended_cost_per_1m": 0.1,
          "latency_seconds": 0.27,
          "tokens_per_second": 292.0,
          "context_window": 256000,
          "gdpval": 538.0454738244687,
          "terminalbench_hard": 0.0,
          "tau2": 0.249,
          "lcr": 11.7,
          "hle": 5.3,
          "gpqa": 35.8,
          "scicode": 14.4,
          "ifbench": 26.8,
          "aime25": 22.0,
          "critpt": 0.0,
          "mmmu_pro": 38.1,
          "livecodebench": 24.7
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 12.858755650671329,
        "coding_score": 4.78,
        "gdpval": 538.0454738244687,
        "terminalbench_hard": 0.0,
        "tau2": 0.249,
        "lcr": 11.7,
        "hle": 5.3,
        "gpqa": 35.8,
        "scicode": 14.4,
        "ifbench": 26.8,
        "aime25": 22.0,
        "critpt": 0.0,
        "mmmu_pro": 38.1,
        "livecodebench": 24.7
      }
    },
    "confidence_score": 1.0,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "devstral-small-2",
    "canonical_name": "Devstral Small 2",
    "model_name": "Devstral Small 2",
    "aliases": [
      "Devstral Small 2"
    ],
    "provider": null,
    "context_window": 256000,
    "open_source": true,
    "arena_votes": null,
    "license_type": "Apache 2.0",
    "creator": null,
    "intelligence_score": 16.7073,
    "coding_score": 20.72,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": null,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": 0.0,
    "latency_seconds": 0.36,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": 200.0,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": 862.7543,
    "terminalbench_hard": 0.167,
    "tau2": 0.234,
    "lcr": 24.0,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": 3.4,
    "gpqa": 53.2,
    "scicode": 28.8,
    "ifbench": 31.2,
    "aime25": 34.3,
    "critpt": 0.0,
    "mmmu_pro": 44.6,
    "livecodebench": 34.8,
    "sources": [
      "Artificial Analysis"
    ],
    "source_count": 1,
    "data_tier": 2,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.905265+00:00",
        "confidence": 1.0,
        "raw_name": "Devstral Small 2",
        "raw_scores": {
          "intelligence_score": 16.707286688382993,
          "coding_score": 20.72,
          "blended_cost_per_1m": 0.0,
          "latency_seconds": 0.36,
          "tokens_per_second": 200.0,
          "context_window": 256000,
          "gdpval": 862.754331510318,
          "terminalbench_hard": 0.167,
          "tau2": 0.234,
          "lcr": 24.0,
          "hle": 3.4,
          "gpqa": 53.2,
          "scicode": 28.8,
          "ifbench": 31.2,
          "aime25": 34.3,
          "critpt": 0.0,
          "mmmu_pro": 44.6,
          "livecodebench": 34.8
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 16.707286688382993,
        "coding_score": 20.72,
        "gdpval": 862.754331510318,
        "terminalbench_hard": 0.167,
        "tau2": 0.234,
        "lcr": 24.0,
        "hle": 3.4,
        "gpqa": 53.2,
        "scicode": 28.8,
        "ifbench": 31.2,
        "aime25": 34.3,
        "critpt": 0.0,
        "mmmu_pro": 44.6,
        "livecodebench": 34.8
      }
    },
    "confidence_score": 1.0,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "deepseek-r1-distill-llama-70b",
    "canonical_name": "DeepSeek R1 Distill Llama 70B",
    "model_name": "DeepSeek R1 Distill Llama 70B",
    "aliases": [
      "DeepSeek R1 Distill Llama 70B"
    ],
    "provider": "Meta",
    "context_window": 128000,
    "open_source": true,
    "arena_votes": null,
    "license_type": "LLAMA 3.3 COMMUNITY LICENSE AGREEMENT",
    "creator": null,
    "intelligence_score": 15.9502,
    "coding_score": 11.43,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": null,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": 0.88,
    "latency_seconds": 0.8,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": 58.0,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": 0.015,
    "tau2": 0.219,
    "lcr": 11.0,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": 6.1,
    "gpqa": 40.2,
    "scicode": 31.2,
    "ifbench": 27.6,
    "aime25": 53.7,
    "critpt": 0.0,
    "mmmu_pro": null,
    "livecodebench": 26.6,
    "sources": [
      "Artificial Analysis"
    ],
    "source_count": 1,
    "data_tier": 2,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.905307+00:00",
        "confidence": 1.0,
        "raw_name": "DeepSeek R1 Distill Llama 70B",
        "raw_scores": {
          "intelligence_score": 15.950177423585206,
          "coding_score": 11.43,
          "blended_cost_per_1m": 0.88,
          "latency_seconds": 0.8,
          "tokens_per_second": 58.0,
          "context_window": 128000,
          "terminalbench_hard": 0.015,
          "tau2": 0.219,
          "lcr": 11.0,
          "hle": 6.1,
          "gpqa": 40.2,
          "scicode": 31.2,
          "ifbench": 27.6,
          "aime25": 53.7,
          "critpt": 0.0,
          "livecodebench": 26.6
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 15.950177423585206,
        "coding_score": 11.43,
        "terminalbench_hard": 0.015,
        "tau2": 0.219,
        "lcr": 11.0,
        "hle": 6.1,
        "gpqa": 40.2,
        "scicode": 31.2,
        "ifbench": 27.6,
        "aime25": 53.7,
        "critpt": 0.0,
        "livecodebench": 26.6
      }
    },
    "confidence_score": 1.0,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "deepseek-r1",
    "canonical_name": "DeepSeek R1 0528 (May '25)",
    "model_name": "DeepSeek R1 0528 (May '25)",
    "aliases": [
      "DeepSeek R1 (Jan '25)",
      "DeepSeek R1 0528 (May '25)",
      "deepseek-r1",
      "deepseek-r1-0528"
    ],
    "provider": "DeepSeek",
    "context_window": 128000,
    "open_source": true,
    "arena_votes": 19177,
    "license_type": "MIT",
    "creator": null,
    "intelligence_score": 25.0679,
    "coding_score": 19.99,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 1408.4,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": 507.3651,
    "terminalbench_hard": 0.11,
    "tau2": 0.2395,
    "lcr": 53.5,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": 12.1,
    "gpqa": 76.05,
    "scicode": 38.0,
    "ifbench": 39.3,
    "aime25": 72.0,
    "critpt": 1.0,
    "mmmu_pro": null,
    "livecodebench": 69.35,
    "sources": [
      "Artificial Analysis",
      "LMSYS Chatbot Arena"
    ],
    "source_count": 2,
    "data_tier": 1,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.905350+00:00",
        "confidence": 0.79,
        "raw_name": "DeepSeek R1 0528 (May '25)",
        "raw_scores": {
          "intelligence_score": 27.07,
          "coding_score": 24.03,
          "context_window": 128000,
          "gdpval": 708.7516196167932,
          "terminalbench_hard": 0.159,
          "tau2": 0.365,
          "lcr": 54.7,
          "hle": 14.9,
          "gpqa": 81.3,
          "scicode": 40.3,
          "ifbench": 39.6,
          "aime25": 76.0,
          "critpt": 1.4,
          "livecodebench": 77.0
        }
      },
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.916104+00:00",
        "confidence": 0.79,
        "raw_name": "DeepSeek R1 (Jan '25)",
        "raw_scores": {
          "intelligence_score": 23.065775824964756,
          "coding_score": 15.95,
          "context_window": 128000,
          "gdpval": 305.97849499074437,
          "terminalbench_hard": 0.061,
          "tau2": 0.114,
          "lcr": 52.3,
          "hle": 9.3,
          "gpqa": 70.8,
          "scicode": 35.7,
          "ifbench": 39.0,
          "aime25": 68.0,
          "critpt": 0.6,
          "livecodebench": 61.7
        }
      },
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.891936+00:00",
        "confidence": 1.0,
        "raw_name": "deepseek-r1-0528",
        "raw_scores": {
          "arena_elo": 1419.15,
          "arena_votes": 19177
        }
      },
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.892321+00:00",
        "confidence": 1.0,
        "raw_name": "deepseek-r1",
        "raw_scores": {
          "arena_elo": 1397.65,
          "arena_votes": 18537
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 27.07,
        "coding_score": 24.03,
        "gdpval": 708.7516196167932,
        "terminalbench_hard": 0.159,
        "tau2": 0.365,
        "lcr": 54.7,
        "hle": 14.9,
        "gpqa": 81.3,
        "scicode": 40.3,
        "ifbench": 39.6,
        "aime25": 76.0,
        "critpt": 1.4,
        "livecodebench": 77.0
      },
      "LMSYS Chatbot Arena": {
        "arena_elo": 1419.15
      }
    },
    "confidence_score": 0.895,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "deepseek-v32",
    "canonical_name": "DeepSeek V3.2 (Non-reasoning)",
    "model_name": "DeepSeek V3.2 (Non-reasoning)",
    "aliases": [
      "DeepSeek V3.2 (Non-reasoning)",
      "DeepSeek V3.2 (Reasoning)",
      "deepseek-v3.2",
      "deepseek-v3.2-thinking"
    ],
    "provider": "DeepSeek",
    "context_window": 128000,
    "open_source": true,
    "arena_votes": 30709,
    "license_type": "MIT",
    "creator": null,
    "intelligence_score": 36.9,
    "coding_score": 35.65,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 1419.975,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": 1038.2821,
    "terminalbench_hard": 0.341,
    "tau2": 0.8475,
    "lcr": 52.0,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": 16.35,
    "gpqa": 79.55,
    "scicode": 38.8,
    "ifbench": 54.85,
    "aime25": 75.5,
    "critpt": 1.9,
    "mmmu_pro": null,
    "livecodebench": 72.75,
    "sources": [
      "Artificial Analysis",
      "LMSYS Chatbot Arena"
    ],
    "source_count": 2,
    "data_tier": 1,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.905390+00:00",
        "confidence": 0.79,
        "raw_name": "DeepSeek V3.2 (Non-reasoning)",
        "raw_scores": {
          "intelligence_score": 32.09,
          "coding_score": 34.6,
          "context_window": 128000,
          "gdpval": 882.7896277058733,
          "terminalbench_hard": 0.326,
          "tau2": 0.789,
          "lcr": 39.0,
          "hle": 10.5,
          "gpqa": 75.1,
          "scicode": 38.7,
          "ifbench": 49.0,
          "aime25": 59.0,
          "critpt": 0.9,
          "livecodebench": 59.3
        }
      },
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.905431+00:00",
        "confidence": 0.79,
        "raw_name": "DeepSeek V3.2 (Reasoning)",
        "raw_scores": {
          "intelligence_score": 41.71,
          "coding_score": 36.7,
          "context_window": 128000,
          "gdpval": 1193.774503238369,
          "terminalbench_hard": 0.356,
          "tau2": 0.906,
          "lcr": 65.0,
          "hle": 22.2,
          "gpqa": 84.0,
          "scicode": 38.9,
          "ifbench": 60.7,
          "aime25": 92.0,
          "critpt": 2.9,
          "livecodebench": 86.2
        }
      },
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.891911+00:00",
        "confidence": 1.0,
        "raw_name": "deepseek-v3.2-thinking",
        "raw_scores": {
          "arena_elo": 1420.12,
          "arena_votes": 25692
        }
      },
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.891923+00:00",
        "confidence": 1.0,
        "raw_name": "deepseek-v3.2",
        "raw_scores": {
          "arena_elo": 1419.83,
          "arena_votes": 30709
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 32.09,
        "coding_score": 34.6,
        "gdpval": 882.7896277058733,
        "terminalbench_hard": 0.326,
        "tau2": 0.789,
        "lcr": 39.0,
        "hle": 10.5,
        "gpqa": 75.1,
        "scicode": 38.7,
        "ifbench": 49.0,
        "aime25": 59.0,
        "critpt": 0.9,
        "livecodebench": 59.3
      },
      "LMSYS Chatbot Arena": {
        "arena_elo": 1420.12
      }
    },
    "confidence_score": 0.895,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "deepseek-v32-speciale",
    "canonical_name": "DeepSeek V3.2 Speciale",
    "model_name": "DeepSeek V3.2 Speciale",
    "aliases": [
      "DeepSeek V3.2 Exp (Non-reasoning)",
      "DeepSeek V3.2 Exp (Reasoning)",
      "DeepSeek V3.2 Speciale",
      "deepseek-v3.2-exp",
      "deepseek-v3.2-exp-thinking"
    ],
    "provider": "DeepSeek",
    "context_window": 128000,
    "open_source": true,
    "arena_votes": 11680,
    "license_type": "MIT",
    "creator": null,
    "intelligence_score": 32.0037,
    "coding_score": 34.0564,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 1423.49,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": 0.0,
    "latency_seconds": 0.0,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": 0.0,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": 845.6949,
    "terminalbench_hard": 0.3067,
    "tau2": 0.2076,
    "lcr": 57.2791,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": 16.9752,
    "gpqa": 80.7616,
    "scicode": 40.8155,
    "ifbench": 54.5302,
    "aime25": 82.0023,
    "critpt": 3.7256,
    "mmmu_pro": null,
    "livecodebench": 75.8516,
    "sources": [
      "Artificial Analysis",
      "LMSYS Chatbot Arena"
    ],
    "source_count": 2,
    "data_tier": 1,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.905474+00:00",
        "confidence": 1.0,
        "raw_name": "DeepSeek V3.2 Speciale",
        "raw_scores": {
          "intelligence_score": 34.07946327638502,
          "coding_score": 37.89,
          "blended_cost_per_1m": 0.0,
          "latency_seconds": 0.0,
          "tokens_per_second": 0.0,
          "context_window": 128000,
          "gdpval": 500.0,
          "terminalbench_hard": 0.348,
          "tau2": 0.0,
          "lcr": 59.3,
          "hle": 26.1,
          "gpqa": 87.1,
          "scicode": 44.0,
          "ifbench": 63.9,
          "aime25": 96.7,
          "critpt": 7.4,
          "livecodebench": 89.6
        }
      },
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.916148+00:00",
        "confidence": 0.79,
        "raw_name": "DeepSeek V3.2 Exp (Non-reasoning)",
        "raw_scores": {
          "intelligence_score": 28.44,
          "coding_score": 29.98,
          "context_window": 128000,
          "gdpval": 1104.1675915685516,
          "terminalbench_hard": 0.25,
          "tau2": 0.339,
          "lcr": 43.0,
          "hle": 8.6,
          "gpqa": 73.8,
          "scicode": 39.9,
          "ifbench": 43.1,
          "aime25": 57.7,
          "critpt": 1.4,
          "livecodebench": 55.4
        }
      },
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.916291+00:00",
        "confidence": 0.79,
        "raw_name": "DeepSeek V3.2 Exp (Reasoning)",
        "raw_scores": {
          "intelligence_score": 32.94,
          "coding_score": 33.28,
          "context_window": 128000,
          "gdpval": 1024.8108251754109,
          "terminalbench_hard": 0.311,
          "tau2": 0.339,
          "lcr": 69.0,
          "hle": 13.8,
          "gpqa": 79.7,
          "scicode": 37.7,
          "ifbench": 54.1,
          "aime25": 87.7,
          "critpt": 1.4,
          "livecodebench": 78.9
        }
      },
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.891859+00:00",
        "confidence": 1.0,
        "raw_name": "deepseek-v3.2-exp",
        "raw_scores": {
          "arena_elo": 1423.66,
          "arena_votes": 11680
        }
      },
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.891872+00:00",
        "confidence": 1.0,
        "raw_name": "deepseek-v3.2-exp-thinking",
        "raw_scores": {
          "arena_elo": 1423.32,
          "arena_votes": 8944
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 34.07946327638502,
        "coding_score": 37.89,
        "gdpval": 500.0,
        "terminalbench_hard": 0.348,
        "tau2": 0.0,
        "lcr": 59.3,
        "hle": 26.1,
        "gpqa": 87.1,
        "scicode": 44.0,
        "ifbench": 63.9,
        "aime25": 96.7,
        "critpt": 7.4,
        "livecodebench": 89.6
      },
      "LMSYS Chatbot Arena": {
        "arena_elo": 1423.66
      }
    },
    "confidence_score": 0.916,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "deepseek-r1-0528-qwen3-8b",
    "canonical_name": "DeepSeek R1 0528 Qwen3 8B",
    "model_name": "DeepSeek R1 0528 Qwen3 8B",
    "aliases": [
      "DeepSeek R1 0528 Qwen3 8B"
    ],
    "provider": "Alibaba",
    "context_window": 32768,
    "open_source": true,
    "arena_votes": null,
    "license_type": "MIT",
    "creator": null,
    "intelligence_score": 16.4307,
    "coding_score": 7.8,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": null,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": 0.0,
    "latency_seconds": 0.0,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": 0.0,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": 0.015,
    "tau2": 0.0,
    "lcr": 13.0,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": 5.6,
    "gpqa": 61.2,
    "scicode": 20.4,
    "ifbench": 19.9,
    "aime25": 63.7,
    "critpt": 0.0,
    "mmmu_pro": null,
    "livecodebench": 51.3,
    "sources": [
      "Artificial Analysis"
    ],
    "source_count": 1,
    "data_tier": 2,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.905514+00:00",
        "confidence": 1.0,
        "raw_name": "DeepSeek R1 0528 Qwen3 8B",
        "raw_scores": {
          "intelligence_score": 16.43067398648048,
          "coding_score": 7.8,
          "blended_cost_per_1m": 0.0,
          "latency_seconds": 0.0,
          "tokens_per_second": 0.0,
          "context_window": 32768,
          "terminalbench_hard": 0.015,
          "tau2": 0.0,
          "lcr": 13.0,
          "hle": 5.6,
          "gpqa": 61.2,
          "scicode": 20.4,
          "ifbench": 19.9,
          "aime25": 63.7,
          "critpt": 0.0,
          "livecodebench": 51.3
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 16.43067398648048,
        "coding_score": 7.8,
        "terminalbench_hard": 0.015,
        "tau2": 0.0,
        "lcr": 13.0,
        "hle": 5.6,
        "gpqa": 61.2,
        "scicode": 20.4,
        "ifbench": 19.9,
        "aime25": 63.7,
        "critpt": 0.0,
        "livecodebench": 51.3
      }
    },
    "confidence_score": 1.0,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "r1-1776",
    "canonical_name": "R1 1776",
    "model_name": "R1 1776",
    "aliases": [
      "R1 1776"
    ],
    "provider": "DeepSeek",
    "context_window": 128000,
    "open_source": true,
    "arena_votes": null,
    "license_type": "MIT",
    "creator": null,
    "intelligence_score": 11.9893,
    "coding_score": null,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": null,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": 0.0,
    "latency_seconds": 0.0,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": 0.0,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": null,
    "tau2": null,
    "lcr": null,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": null,
    "gpqa": null,
    "scicode": null,
    "ifbench": null,
    "aime25": null,
    "critpt": null,
    "mmmu_pro": null,
    "livecodebench": null,
    "sources": [
      "Artificial Analysis"
    ],
    "source_count": 1,
    "data_tier": 2,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.905537+00:00",
        "confidence": 0.65,
        "raw_name": "R1 1776",
        "raw_scores": {
          "intelligence_score": 11.989331081091219,
          "blended_cost_per_1m": 0.0,
          "latency_seconds": 0.0,
          "tokens_per_second": 0.0,
          "context_window": 128000
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 11.989331081091219
      }
    },
    "confidence_score": 0.65,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "falcon-h1r-7b",
    "canonical_name": "Falcon-H1R-7B",
    "model_name": "Falcon-H1R-7B",
    "aliases": [
      "Falcon-H1R-7B"
    ],
    "provider": null,
    "context_window": 256000,
    "open_source": null,
    "arena_votes": null,
    "license_type": null,
    "creator": null,
    "intelligence_score": null,
    "coding_score": 9.81,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": null,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": 0.0,
    "latency_seconds": 0.0,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": 0.0,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": 417.7445,
    "terminalbench_hard": 0.023,
    "tau2": 0.278,
    "lcr": 8.7,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": 10.8,
    "gpqa": 66.1,
    "scicode": 24.9,
    "ifbench": 54.4,
    "aime25": 80.0,
    "critpt": 0.3,
    "mmmu_pro": null,
    "livecodebench": 72.4,
    "sources": [
      "Artificial Analysis"
    ],
    "source_count": 1,
    "data_tier": 2,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.905581+00:00",
        "confidence": 0.9300000000000002,
        "raw_name": "Falcon-H1R-7B",
        "raw_scores": {
          "coding_score": 9.81,
          "blended_cost_per_1m": 0.0,
          "latency_seconds": 0.0,
          "tokens_per_second": 0.0,
          "context_window": 256000,
          "gdpval": 417.74445114214234,
          "terminalbench_hard": 0.023,
          "tau2": 0.278,
          "lcr": 8.7,
          "hle": 10.8,
          "gpqa": 66.1,
          "scicode": 24.9,
          "ifbench": 54.4,
          "aime25": 80.0,
          "critpt": 0.3,
          "livecodebench": 72.4
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "coding_score": 9.81,
        "gdpval": 417.74445114214234,
        "terminalbench_hard": 0.023,
        "tau2": 0.278,
        "lcr": 8.7,
        "hle": 10.8,
        "gpqa": 66.1,
        "scicode": 24.9,
        "ifbench": 54.4,
        "aime25": 80.0,
        "critpt": 0.3,
        "livecodebench": 72.4
      }
    },
    "confidence_score": 0.93,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "grok-41-fast",
    "canonical_name": "Grok 4.1 Fast (Non-reasoning)",
    "model_name": "Grok 4.1 Fast (Non-reasoning)",
    "aliases": [
      "Grok 4.1 Fast (Non-reasoning)",
      "Grok 4.1 Fast (Reasoning)",
      "grok-4.1-fast-reasoning"
    ],
    "provider": "xAI",
    "context_window": 2000000,
    "open_source": null,
    "arena_votes": 31128,
    "license_type": "Proprietary",
    "creator": null,
    "intelligence_score": 29.2553,
    "coding_score": 25.185,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 1430.79,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": 933.528,
    "terminalbench_hard": 0.193,
    "tau2": 0.785,
    "lcr": 45.0,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": 11.3,
    "gpqa": 74.5,
    "scicode": 36.9,
    "ifbench": 44.6,
    "aime25": 61.8,
    "critpt": 1.45,
    "mmmu_pro": 55.85,
    "livecodebench": 61.05,
    "sources": [
      "Artificial Analysis",
      "LMSYS Chatbot Arena"
    ],
    "source_count": 2,
    "data_tier": 1,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.905624+00:00",
        "confidence": 0.79,
        "raw_name": "Grok 4.1 Fast (Non-reasoning)",
        "raw_scores": {
          "intelligence_score": 19.90050190769766,
          "coding_score": 19.47,
          "context_window": 2000000,
          "gdpval": 820.3934643664443,
          "terminalbench_hard": 0.144,
          "tau2": 0.637,
          "lcr": 22.0,
          "hle": 5.0,
          "gpqa": 63.7,
          "scicode": 29.6,
          "ifbench": 36.5,
          "aime25": 34.3,
          "critpt": 0.0,
          "mmmu_pro": 48.4,
          "livecodebench": 39.9
        }
      },
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.905775+00:00",
        "confidence": 0.79,
        "raw_name": "Grok 4.1 Fast (Reasoning)",
        "raw_scores": {
          "intelligence_score": 38.61,
          "coding_score": 30.9,
          "context_window": 2000000,
          "gdpval": 1046.662459943617,
          "terminalbench_hard": 0.242,
          "tau2": 0.933,
          "lcr": 68.0,
          "hle": 17.6,
          "gpqa": 85.3,
          "scicode": 44.2,
          "ifbench": 52.7,
          "aime25": 89.3,
          "critpt": 2.9,
          "mmmu_pro": 63.3,
          "livecodebench": 82.2
        }
      },
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.891780+00:00",
        "confidence": 1.0,
        "raw_name": "grok-4.1-fast-reasoning",
        "raw_scores": {
          "arena_elo": 1430.79,
          "arena_votes": 31128
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 19.90050190769766,
        "coding_score": 19.47,
        "gdpval": 820.3934643664443,
        "terminalbench_hard": 0.144,
        "tau2": 0.637,
        "lcr": 22.0,
        "hle": 5.0,
        "gpqa": 63.7,
        "scicode": 29.6,
        "ifbench": 36.5,
        "aime25": 34.3,
        "critpt": 0.0,
        "mmmu_pro": 48.4,
        "livecodebench": 39.9
      },
      "LMSYS Chatbot Arena": {
        "arena_elo": 1430.79
      }
    },
    "confidence_score": 0.86,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "grok-3-mini",
    "canonical_name": "Grok 3 mini Reasoning (high)",
    "model_name": "Grok 3 mini Reasoning (high)",
    "aliases": [
      "Grok 3 mini Reasoning (high)",
      "grok-3-mini-high"
    ],
    "provider": "xAI",
    "context_window": 1000000,
    "open_source": null,
    "arena_votes": 17413,
    "license_type": "Proprietary",
    "creator": null,
    "intelligence_score": 32.08,
    "coding_score": 25.16,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 1362.9,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": 0.35,
    "latency_seconds": 0.73,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": 177.0,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": 468.6406,
    "terminalbench_hard": 0.174,
    "tau2": 0.904,
    "lcr": 50.3,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": 11.1,
    "gpqa": 79.1,
    "scicode": 40.6,
    "ifbench": 45.9,
    "aime25": 84.7,
    "critpt": 0.6,
    "mmmu_pro": null,
    "livecodebench": 69.6,
    "sources": [
      "Artificial Analysis",
      "LMSYS Chatbot Arena"
    ],
    "source_count": 2,
    "data_tier": 1,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.905668+00:00",
        "confidence": 1.0,
        "raw_name": "Grok 3 mini Reasoning (high)",
        "raw_scores": {
          "intelligence_score": 32.08,
          "coding_score": 25.16,
          "blended_cost_per_1m": 0.35,
          "latency_seconds": 0.73,
          "tokens_per_second": 177.0,
          "context_window": 1000000,
          "gdpval": 468.6405815993188,
          "terminalbench_hard": 0.174,
          "tau2": 0.904,
          "lcr": 50.3,
          "hle": 11.1,
          "gpqa": 79.1,
          "scicode": 40.6,
          "ifbench": 45.9,
          "aime25": 84.7,
          "critpt": 0.6,
          "livecodebench": 69.6
        }
      },
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.892903+00:00",
        "confidence": 1.0,
        "raw_name": "grok-3-mini-high",
        "raw_scores": {
          "arena_elo": 1362.9,
          "arena_votes": 17413
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 32.08,
        "coding_score": 25.16,
        "gdpval": 468.6405815993188,
        "terminalbench_hard": 0.174,
        "tau2": 0.904,
        "lcr": 50.3,
        "hle": 11.1,
        "gpqa": 79.1,
        "scicode": 40.6,
        "ifbench": 45.9,
        "aime25": 84.7,
        "critpt": 0.6,
        "livecodebench": 69.6
      },
      "LMSYS Chatbot Arena": {
        "arena_elo": 1362.9
      }
    },
    "confidence_score": 1.0,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "grok-voice-agent",
    "canonical_name": "Grok Voice Agent",
    "model_name": "Grok Voice Agent",
    "aliases": [
      "Grok Voice Agent"
    ],
    "provider": "xAI",
    "context_window": 32000,
    "open_source": null,
    "arena_votes": null,
    "license_type": null,
    "creator": null,
    "intelligence_score": null,
    "coding_score": null,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": null,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": 0.0,
    "latency_seconds": 0.0,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": 0.0,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": null,
    "tau2": null,
    "lcr": null,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": null,
    "gpqa": null,
    "scicode": null,
    "ifbench": null,
    "aime25": null,
    "critpt": null,
    "mmmu_pro": null,
    "livecodebench": null,
    "sources": [
      "Artificial Analysis"
    ],
    "source_count": 1,
    "data_tier": 2,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.905688+00:00",
        "confidence": 0.5800000000000001,
        "raw_name": "Grok Voice Agent",
        "raw_scores": {
          "blended_cost_per_1m": 0.0,
          "latency_seconds": 0.0,
          "tokens_per_second": 0.0,
          "context_window": 32000
        }
      }
    ],
    "benchmark_breakdown": {},
    "confidence_score": 0.58,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "grok-4",
    "canonical_name": "Grok 4",
    "model_name": "Grok 4",
    "aliases": [
      "Grok 4",
      "grok-4-0709"
    ],
    "provider": "xAI",
    "context_window": 256000,
    "open_source": null,
    "arena_votes": 41753,
    "license_type": "Proprietary",
    "creator": null,
    "intelligence_score": 41.52,
    "coding_score": 40.49,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 1409.41,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": 6.0,
    "latency_seconds": 9.77,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": 42.0,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": 990.8278,
    "terminalbench_hard": 0.379,
    "tau2": 0.749,
    "lcr": 68.0,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": 23.9,
    "gpqa": 87.7,
    "scicode": 45.7,
    "ifbench": 53.7,
    "aime25": 92.7,
    "critpt": 2.0,
    "mmmu_pro": 68.8,
    "livecodebench": 81.9,
    "sources": [
      "Artificial Analysis",
      "LMSYS Chatbot Arena"
    ],
    "source_count": 2,
    "data_tier": 1,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.905734+00:00",
        "confidence": 1.0,
        "raw_name": "Grok 4",
        "raw_scores": {
          "intelligence_score": 41.52,
          "coding_score": 40.49,
          "blended_cost_per_1m": 6.0,
          "latency_seconds": 9.77,
          "tokens_per_second": 42.0,
          "context_window": 256000,
          "gdpval": 990.8278427922464,
          "terminalbench_hard": 0.379,
          "tau2": 0.749,
          "lcr": 68.0,
          "hle": 23.9,
          "gpqa": 87.7,
          "scicode": 45.7,
          "ifbench": 53.7,
          "aime25": 92.7,
          "critpt": 2.0,
          "mmmu_pro": 68.8,
          "livecodebench": 81.9
        }
      },
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.892150+00:00",
        "confidence": 1.0,
        "raw_name": "grok-4-0709",
        "raw_scores": {
          "arena_elo": 1409.41,
          "arena_votes": 41753
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 41.52,
        "coding_score": 40.49,
        "gdpval": 990.8278427922464,
        "terminalbench_hard": 0.379,
        "tau2": 0.749,
        "lcr": 68.0,
        "hle": 23.9,
        "gpqa": 87.7,
        "scicode": 45.7,
        "ifbench": 53.7,
        "aime25": 92.7,
        "critpt": 2.0,
        "mmmu_pro": 68.8,
        "livecodebench": 81.9
      },
      "LMSYS Chatbot Arena": {
        "arena_elo": 1409.41
      }
    },
    "confidence_score": 1.0,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "grok-code-fast-1",
    "canonical_name": "Grok Code Fast 1",
    "model_name": "Grok Code Fast 1",
    "aliases": [
      "Grok Code Fast 1"
    ],
    "provider": "xAI",
    "context_window": 256000,
    "open_source": null,
    "arena_votes": null,
    "license_type": null,
    "creator": null,
    "intelligence_score": 28.74,
    "coding_score": 23.69,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": null,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": 0.53,
    "latency_seconds": 6.72,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": 238.0,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": 775.9233,
    "terminalbench_hard": 0.174,
    "tau2": 0.757,
    "lcr": 48.3,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": 7.5,
    "gpqa": 72.7,
    "scicode": 36.2,
    "ifbench": 41.4,
    "aime25": 43.3,
    "critpt": 0.0,
    "mmmu_pro": null,
    "livecodebench": 65.7,
    "sources": [
      "Artificial Analysis"
    ],
    "source_count": 1,
    "data_tier": 2,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.905820+00:00",
        "confidence": 1.0,
        "raw_name": "Grok Code Fast 1",
        "raw_scores": {
          "intelligence_score": 28.74,
          "coding_score": 23.69,
          "blended_cost_per_1m": 0.53,
          "latency_seconds": 6.72,
          "tokens_per_second": 238.0,
          "context_window": 256000,
          "gdpval": 775.9232729441947,
          "terminalbench_hard": 0.174,
          "tau2": 0.757,
          "lcr": 48.3,
          "hle": 7.5,
          "gpqa": 72.7,
          "scicode": 36.2,
          "ifbench": 41.4,
          "aime25": 43.3,
          "critpt": 0.0,
          "livecodebench": 65.7
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 28.74,
        "coding_score": 23.69,
        "gdpval": 775.9232729441947,
        "terminalbench_hard": 0.174,
        "tau2": 0.757,
        "lcr": 48.3,
        "hle": 7.5,
        "gpqa": 72.7,
        "scicode": 36.2,
        "ifbench": 41.4,
        "aime25": 43.3,
        "critpt": 0.0,
        "livecodebench": 65.7
      }
    },
    "confidence_score": 1.0,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "nova-micro",
    "canonical_name": "Nova Micro",
    "model_name": "Nova Micro",
    "aliases": [
      "Nova Micro",
      "amazon-nova-micro-v1.0"
    ],
    "provider": "Amazon",
    "context_window": 130000,
    "open_source": null,
    "arena_votes": 19355,
    "license_type": "Proprietary",
    "creator": null,
    "intelligence_score": 11.5502,
    "coding_score": 4.14,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 1240.88,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": 0.06,
    "latency_seconds": 0.36,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": 387.0,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": 392.5414,
    "terminalbench_hard": 0.015,
    "tau2": 0.14,
    "lcr": 9.7,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": 4.7,
    "gpqa": 35.8,
    "scicode": 9.4,
    "ifbench": 29.4,
    "aime25": 6.0,
    "critpt": 0.0,
    "mmmu_pro": null,
    "livecodebench": 14.0,
    "sources": [
      "Artificial Analysis",
      "LMSYS Chatbot Arena"
    ],
    "source_count": 2,
    "data_tier": 1,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.905865+00:00",
        "confidence": 1.0,
        "raw_name": "Nova Micro",
        "raw_scores": {
          "intelligence_score": 11.550167660288439,
          "coding_score": 4.14,
          "blended_cost_per_1m": 0.06,
          "latency_seconds": 0.36,
          "tokens_per_second": 387.0,
          "context_window": 130000,
          "gdpval": 392.5413853363584,
          "terminalbench_hard": 0.015,
          "tau2": 0.14,
          "lcr": 9.7,
          "hle": 4.7,
          "gpqa": 35.8,
          "scicode": 9.4,
          "ifbench": 29.4,
          "aime25": 6.0,
          "critpt": 0.0,
          "livecodebench": 14.0
        }
      },
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.894378+00:00",
        "confidence": 1.0,
        "raw_name": "amazon-nova-micro-v1.0",
        "raw_scores": {
          "arena_elo": 1240.88,
          "arena_votes": 19355
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 11.550167660288439,
        "coding_score": 4.14,
        "gdpval": 392.5413853363584,
        "terminalbench_hard": 0.015,
        "tau2": 0.14,
        "lcr": 9.7,
        "hle": 4.7,
        "gpqa": 35.8,
        "scicode": 9.4,
        "ifbench": 29.4,
        "aime25": 6.0,
        "critpt": 0.0,
        "livecodebench": 14.0
      },
      "LMSYS Chatbot Arena": {
        "arena_elo": 1240.88
      }
    },
    "confidence_score": 1.0,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "nova-20-pro-preview",
    "canonical_name": "Nova 2.0 Pro Preview (medium)",
    "model_name": "Nova 2.0 Pro Preview (medium)",
    "aliases": [
      "Nova 2.0 Pro Preview (Non-reasoning)",
      "Nova 2.0 Pro Preview (low)",
      "Nova 2.0 Pro Preview (medium)"
    ],
    "provider": "Amazon",
    "context_window": 256000,
    "open_source": null,
    "arena_votes": null,
    "license_type": null,
    "creator": null,
    "intelligence_score": 30.4303,
    "coding_score": 25.4792,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": null,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": 3.44,
    "latency_seconds": 16.255,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": 133.5,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": 705.7513,
    "terminalbench_hard": 0.1964,
    "tau2": 0.8597,
    "lcr": 49.5903,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": 6.1864,
    "gpqa": 73.0624,
    "scicode": 37.1323,
    "ifbench": 71.5699,
    "aime25": 63.2806,
    "critpt": 0.0,
    "mmmu_pro": 63.6,
    "livecodebench": 62.4254,
    "sources": [
      "Artificial Analysis"
    ],
    "source_count": 1,
    "data_tier": 2,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.905913+00:00",
        "confidence": 1.0,
        "raw_name": "Nova 2.0 Pro Preview (medium)",
        "raw_scores": {
          "intelligence_score": 35.71,
          "coding_score": 30.4,
          "blended_cost_per_1m": 3.44,
          "latency_seconds": 20.55,
          "tokens_per_second": 132.0,
          "context_window": 256000,
          "gdpval": 980.686886347652,
          "terminalbench_hard": 0.242,
          "tau2": 0.927,
          "lcr": 54.3,
          "hle": 8.9,
          "gpqa": 78.5,
          "scicode": 42.7,
          "ifbench": 79.0,
          "aime25": 89.0,
          "critpt": 0.0,
          "mmmu_pro": 64.5,
          "livecodebench": 73.0
        }
      },
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.906082+00:00",
        "confidence": 0.79,
        "raw_name": "Nova 2.0 Pro Preview (Non-reasoning)",
        "raw_scores": {
          "intelligence_score": 21.886717651702874,
          "coding_score": 20.49,
          "context_window": 256000,
          "gdpval": 378.5129180477645,
          "terminalbench_hard": 0.167,
          "tau2": 0.716,
          "lcr": 28.3,
          "hle": 4.0,
          "gpqa": 63.6,
          "scicode": 28.1,
          "ifbench": 52.0,
          "aime25": 30.7,
          "critpt": 0.0,
          "livecodebench": 47.3
        }
      },
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.906277+00:00",
        "confidence": 1.0,
        "raw_name": "Nova 2.0 Pro Preview (low)",
        "raw_scores": {
          "intelligence_score": 31.9,
          "coding_score": 24.5,
          "blended_cost_per_1m": 3.44,
          "latency_seconds": 11.96,
          "tokens_per_second": 135.0,
          "context_window": 256000,
          "gdpval": 689.3340059706902,
          "terminalbench_hard": 0.174,
          "tau2": 0.906,
          "lcr": 61.7,
          "hle": 5.2,
          "gpqa": 75.1,
          "scicode": 38.7,
          "ifbench": 79.6,
          "aime25": 63.3,
          "critpt": 0.0,
          "mmmu_pro": 62.7,
          "livecodebench": 63.8
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 35.71,
        "coding_score": 30.4,
        "gdpval": 980.686886347652,
        "terminalbench_hard": 0.242,
        "tau2": 0.927,
        "lcr": 54.3,
        "hle": 8.9,
        "gpqa": 78.5,
        "scicode": 42.7,
        "ifbench": 79.0,
        "aime25": 89.0,
        "critpt": 0.0,
        "mmmu_pro": 64.5,
        "livecodebench": 73.0
      }
    },
    "confidence_score": 0.93,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "nova-20-lite",
    "canonical_name": "Nova 2.0 Lite (low)",
    "model_name": "Nova 2.0 Lite (low)",
    "aliases": [
      "Nova 2.0 Lite (Non-reasoning)",
      "Nova 2.0 Lite (low)",
      "Nova 2.0 Lite (medium)"
    ],
    "provider": "Amazon",
    "context_window": 1000000,
    "open_source": null,
    "arena_votes": null,
    "license_type": null,
    "creator": null,
    "intelligence_score": 24.7279,
    "coding_score": 16.9959,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": null,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": 0.85,
    "latency_seconds": 10.94,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": 233.5,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": 560.7228,
    "terminalbench_hard": 0.0952,
    "tau2": 0.7046,
    "lcr": 44.5459,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": 5.4373,
    "gpqa": 69.619,
    "scicode": 31.9211,
    "ifbench": 57.9552,
    "aime25": 58.0728,
    "critpt": 0.0,
    "mmmu_pro": 57.0645,
    "livecodebench": 50.3706,
    "sources": [
      "Artificial Analysis"
    ],
    "source_count": 1,
    "data_tier": 2,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.905959+00:00",
        "confidence": 1.0,
        "raw_name": "Nova 2.0 Lite (low)",
        "raw_scores": {
          "intelligence_score": 24.59,
          "coding_score": 13.64,
          "blended_cost_per_1m": 0.85,
          "latency_seconds": 8.17,
          "tokens_per_second": 230.0,
          "context_window": 1000000,
          "gdpval": 544.5056321916265,
          "terminalbench_hard": 0.038,
          "tau2": 0.719,
          "lcr": 52.0,
          "hle": 4.2,
          "gpqa": 69.8,
          "scicode": 33.3,
          "ifbench": 61.2,
          "aime25": 46.7,
          "critpt": 0.0,
          "mmmu_pro": 58.0,
          "livecodebench": 46.9
        }
      },
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.906002+00:00",
        "confidence": 0.79,
        "raw_name": "Nova 2.0 Lite (Non-reasoning)",
        "raw_scores": {
          "intelligence_score": 18.570604974774277,
          "coding_score": 12.53,
          "context_window": 1000000,
          "gdpval": 429.18305437116396,
          "terminalbench_hard": 0.068,
          "tau2": 0.62,
          "lcr": 17.7,
          "hle": 3.0,
          "gpqa": 60.3,
          "scicode": 24.0,
          "ifbench": 40.5,
          "aime25": 33.7,
          "critpt": 0.0,
          "mmmu_pro": 49.0,
          "livecodebench": 34.6
        }
      },
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.906322+00:00",
        "confidence": 1.0,
        "raw_name": "Nova 2.0 Lite (medium)",
        "raw_scores": {
          "intelligence_score": 29.73,
          "coding_score": 23.88,
          "blended_cost_per_1m": 0.85,
          "latency_seconds": 13.71,
          "tokens_per_second": 237.0,
          "context_window": 1000000,
          "gdpval": 680.8564894328853,
          "terminalbench_hard": 0.174,
          "tau2": 0.757,
          "lcr": 58.3,
          "hle": 8.6,
          "gpqa": 76.8,
          "scicode": 36.8,
          "ifbench": 68.5,
          "aime25": 88.7,
          "critpt": 0.0,
          "mmmu_pro": 62.5,
          "livecodebench": 66.3
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 24.59,
        "coding_score": 13.64,
        "gdpval": 544.5056321916265,
        "terminalbench_hard": 0.038,
        "tau2": 0.719,
        "lcr": 52.0,
        "hle": 4.2,
        "gpqa": 69.8,
        "scicode": 33.3,
        "ifbench": 61.2,
        "aime25": 46.7,
        "critpt": 0.0,
        "mmmu_pro": 58.0,
        "livecodebench": 46.9
      }
    },
    "confidence_score": 0.93,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "nova-20-omni",
    "canonical_name": "Nova 2.0 Omni (Non-reasoning)",
    "model_name": "Nova 2.0 Omni (Non-reasoning)",
    "aliases": [
      "Nova 2.0 Omni (Non-reasoning)",
      "Nova 2.0 Omni (low)",
      "Nova 2.0 Omni (medium)"
    ],
    "provider": "Amazon",
    "context_window": 1000000,
    "open_source": null,
    "arena_votes": null,
    "license_type": null,
    "creator": null,
    "intelligence_score": 24.506,
    "coding_score": 14.3346,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": null,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": 0.85,
    "latency_seconds": 0.0,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": 0.0,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": 539.4905,
    "terminalbench_hard": 0.049,
    "tau2": 0.6578,
    "lcr": 43.8412,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": 4.9753,
    "gpqa": 68.009,
    "scicode": 33.1688,
    "ifbench": 57.5158,
    "aime25": 62.6989,
    "critpt": 0.0,
    "mmmu_pro": 57.7495,
    "livecodebench": 53.5108,
    "sources": [
      "Artificial Analysis"
    ],
    "source_count": 1,
    "data_tier": 2,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.906043+00:00",
        "confidence": 0.79,
        "raw_name": "Nova 2.0 Omni (Non-reasoning)",
        "raw_scores": {
          "intelligence_score": 17.832719657356495,
          "coding_score": 13.84,
          "context_window": 1000000,
          "gdpval": 359.89114035052285,
          "terminalbench_hard": 0.068,
          "tau2": 0.447,
          "lcr": 22.3,
          "hle": 3.9,
          "gpqa": 55.5,
          "scicode": 27.9,
          "ifbench": 41.1,
          "aime25": 37.0,
          "critpt": 0.0,
          "mmmu_pro": 49.9,
          "livecodebench": 30.5
        }
      },
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.906126+00:00",
        "confidence": 1.0,
        "raw_name": "Nova 2.0 Omni (medium)",
        "raw_scores": {
          "intelligence_score": 28.02,
          "coding_score": 15.11,
          "blended_cost_per_1m": 0.85,
          "latency_seconds": 0.0,
          "tokens_per_second": 0.0,
          "context_window": 1000000,
          "gdpval": 812.0685376009737,
          "terminalbench_hard": 0.045,
          "tau2": 0.804,
          "lcr": 53.7,
          "hle": 6.8,
          "gpqa": 76.0,
          "scicode": 36.2,
          "ifbench": 66.2,
          "aime25": 89.7,
          "critpt": 0.0,
          "mmmu_pro": 61.9,
          "livecodebench": 66.0
        }
      },
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.906171+00:00",
        "confidence": 1.0,
        "raw_name": "Nova 2.0 Omni (low)",
        "raw_scores": {
          "intelligence_score": 26.264020294343602,
          "coding_score": 13.95,
          "blended_cost_per_1m": 0.85,
          "latency_seconds": 0.0,
          "tokens_per_second": 0.0,
          "context_window": 1000000,
          "gdpval": 408.7959957232224,
          "terminalbench_hard": 0.038,
          "tau2": 0.678,
          "lcr": 51.0,
          "hle": 4.0,
          "gpqa": 69.9,
          "scicode": 34.3,
          "ifbench": 61.8,
          "aime25": 56.0,
          "critpt": 0.0,
          "mmmu_pro": 59.8,
          "livecodebench": 59.2
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 17.832719657356495,
        "coding_score": 13.84,
        "gdpval": 359.89114035052285,
        "terminalbench_hard": 0.068,
        "tau2": 0.447,
        "lcr": 22.3,
        "hle": 3.9,
        "gpqa": 55.5,
        "scicode": 27.9,
        "ifbench": 41.1,
        "aime25": 37.0,
        "critpt": 0.0,
        "mmmu_pro": 49.9,
        "livecodebench": 30.5
      }
    },
    "confidence_score": 0.93,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "nova-premier",
    "canonical_name": "Nova Premier",
    "model_name": "Nova Premier",
    "aliases": [
      "Nova Premier"
    ],
    "provider": "Amazon",
    "context_window": 1000000,
    "open_source": null,
    "arena_votes": null,
    "license_type": null,
    "creator": null,
    "intelligence_score": 16.9841,
    "coding_score": 13.84,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": null,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": 5.0,
    "latency_seconds": 0.85,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": 77.0,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": 553.2925,
    "terminalbench_hard": 0.068,
    "tau2": 0.383,
    "lcr": 30.0,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": 4.7,
    "gpqa": 56.9,
    "scicode": 27.9,
    "ifbench": 36.2,
    "aime25": 17.3,
    "critpt": 0.0,
    "mmmu_pro": null,
    "livecodebench": 31.7,
    "sources": [
      "Artificial Analysis"
    ],
    "source_count": 1,
    "data_tier": 2,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.906227+00:00",
        "confidence": 1.0,
        "raw_name": "Nova Premier",
        "raw_scores": {
          "intelligence_score": 16.984054832996712,
          "coding_score": 13.84,
          "blended_cost_per_1m": 5.0,
          "latency_seconds": 0.85,
          "tokens_per_second": 77.0,
          "context_window": 1000000,
          "gdpval": 553.2924925434746,
          "terminalbench_hard": 0.068,
          "tau2": 0.383,
          "lcr": 30.0,
          "hle": 4.7,
          "gpqa": 56.9,
          "scicode": 27.9,
          "ifbench": 36.2,
          "aime25": 17.3,
          "critpt": 0.0,
          "livecodebench": 31.7
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 16.984054832996712,
        "coding_score": 13.84,
        "gdpval": 553.2924925434746,
        "terminalbench_hard": 0.068,
        "tau2": 0.383,
        "lcr": 30.0,
        "hle": 4.7,
        "gpqa": 56.9,
        "scicode": 27.9,
        "ifbench": 36.2,
        "aime25": 17.3,
        "critpt": 0.0,
        "livecodebench": 31.7
      }
    },
    "confidence_score": 1.0,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "phi-4",
    "canonical_name": "Phi-4",
    "model_name": "Phi-4",
    "aliases": [
      "Phi-4",
      "phi-4"
    ],
    "provider": "Microsoft",
    "context_window": 16000,
    "open_source": true,
    "arena_votes": 24126,
    "license_type": "MIT",
    "creator": null,
    "intelligence_score": 13.1831,
    "coding_score": 11.21,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 1256.07,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": 0.22,
    "latency_seconds": 0.85,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": 7.0,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": 0.038,
    "tau2": 0.0,
    "lcr": 0.0,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": 4.1,
    "gpqa": 57.5,
    "scicode": 26.0,
    "ifbench": 23.5,
    "aime25": 18.0,
    "critpt": 0.0,
    "mmmu_pro": null,
    "livecodebench": 23.1,
    "sources": [
      "Artificial Analysis",
      "LMSYS Chatbot Arena"
    ],
    "source_count": 2,
    "data_tier": 1,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.906364+00:00",
        "confidence": 1.0,
        "raw_name": "Phi-4",
        "raw_scores": {
          "intelligence_score": 13.183090068262993,
          "coding_score": 11.21,
          "blended_cost_per_1m": 0.22,
          "latency_seconds": 0.85,
          "tokens_per_second": 7.0,
          "context_window": 16000,
          "terminalbench_hard": 0.038,
          "tau2": 0.0,
          "lcr": 0.0,
          "hle": 4.1,
          "gpqa": 57.5,
          "scicode": 26.0,
          "ifbench": 23.5,
          "aime25": 18.0,
          "critpt": 0.0,
          "livecodebench": 23.1
        }
      },
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.894328+00:00",
        "confidence": 1.0,
        "raw_name": "phi-4",
        "raw_scores": {
          "arena_elo": 1256.07,
          "arena_votes": 24126
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 13.183090068262993,
        "coding_score": 11.21,
        "terminalbench_hard": 0.038,
        "tau2": 0.0,
        "lcr": 0.0,
        "hle": 4.1,
        "gpqa": 57.5,
        "scicode": 26.0,
        "ifbench": 23.5,
        "aime25": 18.0,
        "critpt": 0.0,
        "livecodebench": 23.1
      },
      "LMSYS Chatbot Arena": {
        "arena_elo": 1256.07
      }
    },
    "confidence_score": 1.0,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "phi-4-mini-instruct",
    "canonical_name": "Phi-4 Mini Instruct",
    "model_name": "Phi-4 Mini Instruct",
    "aliases": [
      "Phi-4 Mini Instruct"
    ],
    "provider": "Microsoft",
    "context_window": 128000,
    "open_source": true,
    "arena_votes": null,
    "license_type": "MIT",
    "creator": null,
    "intelligence_score": 10.9394,
    "coding_score": 3.59,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": null,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": 281.7795,
    "terminalbench_hard": 0.0,
    "tau2": 0.082,
    "lcr": 13.7,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": 4.2,
    "gpqa": 33.1,
    "scicode": 10.8,
    "ifbench": 21.1,
    "aime25": 6.7,
    "critpt": 0.0,
    "mmmu_pro": null,
    "livecodebench": 12.6,
    "sources": [
      "Artificial Analysis"
    ],
    "source_count": 1,
    "data_tier": 2,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.906404+00:00",
        "confidence": 0.79,
        "raw_name": "Phi-4 Mini Instruct",
        "raw_scores": {
          "intelligence_score": 10.939443779805618,
          "coding_score": 3.59,
          "context_window": 128000,
          "gdpval": 281.7795198686267,
          "terminalbench_hard": 0.0,
          "tau2": 0.082,
          "lcr": 13.7,
          "hle": 4.2,
          "gpqa": 33.1,
          "scicode": 10.8,
          "ifbench": 21.1,
          "aime25": 6.7,
          "critpt": 0.0,
          "livecodebench": 12.6
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 10.939443779805618,
        "coding_score": 3.59,
        "gdpval": 281.7795198686267,
        "terminalbench_hard": 0.0,
        "tau2": 0.082,
        "lcr": 13.7,
        "hle": 4.2,
        "gpqa": 33.1,
        "scicode": 10.8,
        "ifbench": 21.1,
        "aime25": 6.7,
        "critpt": 0.0,
        "livecodebench": 12.6
      }
    },
    "confidence_score": 0.79,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "phi-4-multimodal-instruct",
    "canonical_name": "Phi-4 Multimodal Instruct",
    "model_name": "Phi-4 Multimodal Instruct",
    "aliases": [
      "Phi-4 Multimodal Instruct"
    ],
    "provider": "Microsoft",
    "context_window": 128000,
    "open_source": true,
    "arena_votes": null,
    "license_type": "MIT",
    "creator": null,
    "intelligence_score": 10.0404,
    "coding_score": null,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": null,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": null,
    "tau2": null,
    "lcr": null,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": 4.4,
    "gpqa": 31.5,
    "scicode": 11.0,
    "ifbench": null,
    "aime25": null,
    "critpt": null,
    "mmmu_pro": 14.5,
    "livecodebench": 13.1,
    "sources": [
      "Artificial Analysis"
    ],
    "source_count": 1,
    "data_tier": 2,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.906435+00:00",
        "confidence": 0.65,
        "raw_name": "Phi-4 Multimodal Instruct",
        "raw_scores": {
          "intelligence_score": 10.04043714573622,
          "context_window": 128000,
          "hle": 4.4,
          "gpqa": 31.5,
          "scicode": 11.0,
          "mmmu_pro": 14.5,
          "livecodebench": 13.1
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 10.04043714573622,
        "hle": 4.4,
        "gpqa": 31.5,
        "scicode": 11.0,
        "mmmu_pro": 14.5,
        "livecodebench": 13.1
      }
    },
    "confidence_score": 0.65,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "lfm25-vl-16b",
    "canonical_name": "LFM2.5-VL-1.6B",
    "model_name": "LFM2.5-VL-1.6B",
    "aliases": [
      "LFM2.5-VL-1.6B"
    ],
    "provider": null,
    "context_window": 32000,
    "open_source": true,
    "arena_votes": null,
    "license_type": "lfm 1.0",
    "creator": null,
    "intelligence_score": null,
    "coding_score": 1.0,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": null,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": 0.0,
    "latency_seconds": 0.0,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": 0.0,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": 286.526,
    "terminalbench_hard": 0.0,
    "tau2": 0.085,
    "lcr": 0.0,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": 5.1,
    "gpqa": 28.9,
    "scicode": 3.0,
    "ifbench": 33.1,
    "aime25": null,
    "critpt": 0.0,
    "mmmu_pro": 26.5,
    "livecodebench": null,
    "sources": [
      "Artificial Analysis"
    ],
    "source_count": 1,
    "data_tier": 2,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.906472+00:00",
        "confidence": 0.79,
        "raw_name": "LFM2.5-VL-1.6B",
        "raw_scores": {
          "coding_score": 1.0,
          "blended_cost_per_1m": 0.0,
          "latency_seconds": 0.0,
          "tokens_per_second": 0.0,
          "context_window": 32000,
          "gdpval": 286.5259623070484,
          "terminalbench_hard": 0.0,
          "tau2": 0.085,
          "lcr": 0.0,
          "hle": 5.1,
          "gpqa": 28.9,
          "scicode": 3.0,
          "ifbench": 33.1,
          "critpt": 0.0,
          "mmmu_pro": 26.5
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "coding_score": 1.0,
        "gdpval": 286.5259623070484,
        "terminalbench_hard": 0.0,
        "tau2": 0.085,
        "lcr": 0.0,
        "hle": 5.1,
        "gpqa": 28.9,
        "scicode": 3.0,
        "ifbench": 33.1,
        "critpt": 0.0,
        "mmmu_pro": 26.5
      }
    },
    "confidence_score": 0.79,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "lfm25-12b-thinking",
    "canonical_name": "LFM2.5-1.2B-Thinking",
    "model_name": "LFM2.5-1.2B-Thinking",
    "aliases": [
      "LFM2.5-1.2B-Thinking"
    ],
    "provider": null,
    "context_window": 32000,
    "open_source": true,
    "arena_votes": null,
    "license_type": "lfm 1.0",
    "creator": null,
    "intelligence_score": null,
    "coding_score": 1.39,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": null,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": 0.0,
    "latency_seconds": 0.0,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": 0.0,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": 312.6738,
    "terminalbench_hard": 0.0,
    "tau2": 0.196,
    "lcr": 0.0,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": 6.1,
    "gpqa": 33.9,
    "scicode": 4.2,
    "ifbench": 41.8,
    "aime25": null,
    "critpt": 0.0,
    "mmmu_pro": null,
    "livecodebench": null,
    "sources": [
      "Artificial Analysis"
    ],
    "source_count": 1,
    "data_tier": 2,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.906506+00:00",
        "confidence": 0.79,
        "raw_name": "LFM2.5-1.2B-Thinking",
        "raw_scores": {
          "coding_score": 1.39,
          "blended_cost_per_1m": 0.0,
          "latency_seconds": 0.0,
          "tokens_per_second": 0.0,
          "context_window": 32000,
          "gdpval": 312.6737726085147,
          "terminalbench_hard": 0.0,
          "tau2": 0.196,
          "lcr": 0.0,
          "hle": 6.1,
          "gpqa": 33.9,
          "scicode": 4.2,
          "ifbench": 41.8,
          "critpt": 0.0
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "coding_score": 1.39,
        "gdpval": 312.6737726085147,
        "terminalbench_hard": 0.0,
        "tau2": 0.196,
        "lcr": 0.0,
        "hle": 6.1,
        "gpqa": 33.9,
        "scicode": 4.2,
        "ifbench": 41.8,
        "critpt": 0.0
      }
    },
    "confidence_score": 0.79,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "lfm2-8b-a1b",
    "canonical_name": "LFM2 8B A1B",
    "model_name": "LFM2 8B A1B",
    "aliases": [
      "LFM2 8B A1B"
    ],
    "provider": null,
    "context_window": 32768,
    "open_source": true,
    "arena_votes": null,
    "license_type": "lfm 1.0",
    "creator": null,
    "intelligence_score": 11.4466,
    "coding_score": 2.28,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": null,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": 0.0,
    "latency_seconds": 0.0,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": 0.0,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": 322.0224,
    "terminalbench_hard": 0.0,
    "tau2": 0.105,
    "lcr": 0.0,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": 4.9,
    "gpqa": 34.4,
    "scicode": 6.8,
    "ifbench": 26.3,
    "aime25": 25.3,
    "critpt": 0.0,
    "mmmu_pro": null,
    "livecodebench": 15.1,
    "sources": [
      "Artificial Analysis"
    ],
    "source_count": 1,
    "data_tier": 2,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.906548+00:00",
        "confidence": 1.0,
        "raw_name": "LFM2 8B A1B",
        "raw_scores": {
          "intelligence_score": 11.446568066689569,
          "coding_score": 2.28,
          "blended_cost_per_1m": 0.0,
          "latency_seconds": 0.0,
          "tokens_per_second": 0.0,
          "context_window": 32768,
          "gdpval": 322.02243465105767,
          "terminalbench_hard": 0.0,
          "tau2": 0.105,
          "lcr": 0.0,
          "hle": 4.9,
          "gpqa": 34.4,
          "scicode": 6.8,
          "ifbench": 26.3,
          "aime25": 25.3,
          "critpt": 0.0,
          "livecodebench": 15.1
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 11.446568066689569,
        "coding_score": 2.28,
        "gdpval": 322.02243465105767,
        "terminalbench_hard": 0.0,
        "tau2": 0.105,
        "lcr": 0.0,
        "hle": 4.9,
        "gpqa": 34.4,
        "scicode": 6.8,
        "ifbench": 26.3,
        "aime25": 25.3,
        "critpt": 0.0,
        "livecodebench": 15.1
      }
    },
    "confidence_score": 1.0,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "lfm2-26b",
    "canonical_name": "LFM2 2.6B",
    "model_name": "LFM2 2.6B",
    "aliases": [
      "LFM2 2.6B"
    ],
    "provider": null,
    "context_window": 32768,
    "open_source": true,
    "arena_votes": null,
    "license_type": "lfm 1.0",
    "creator": null,
    "intelligence_score": 9.8784,
    "coding_score": 1.35,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": null,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": 0.0,
    "latency_seconds": 0.0,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": 0.0,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": 297.1659,
    "terminalbench_hard": 0.008,
    "tau2": 0.135,
    "lcr": 0.0,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": 5.2,
    "gpqa": 30.6,
    "scicode": 2.5,
    "ifbench": 19.5,
    "aime25": 8.3,
    "critpt": 0.0,
    "mmmu_pro": null,
    "livecodebench": 8.1,
    "sources": [
      "Artificial Analysis"
    ],
    "source_count": 1,
    "data_tier": 2,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.906591+00:00",
        "confidence": 1.0,
        "raw_name": "LFM2 2.6B",
        "raw_scores": {
          "intelligence_score": 9.8783940490465,
          "coding_score": 1.35,
          "blended_cost_per_1m": 0.0,
          "latency_seconds": 0.0,
          "tokens_per_second": 0.0,
          "context_window": 32768,
          "gdpval": 297.16589943873964,
          "terminalbench_hard": 0.008,
          "tau2": 0.135,
          "lcr": 0.0,
          "hle": 5.2,
          "gpqa": 30.6,
          "scicode": 2.5,
          "ifbench": 19.5,
          "aime25": 8.3,
          "critpt": 0.0,
          "livecodebench": 8.1
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 9.8783940490465,
        "coding_score": 1.35,
        "gdpval": 297.16589943873964,
        "terminalbench_hard": 0.008,
        "tau2": 0.135,
        "lcr": 0.0,
        "hle": 5.2,
        "gpqa": 30.6,
        "scicode": 2.5,
        "ifbench": 19.5,
        "aime25": 8.3,
        "critpt": 0.0,
        "livecodebench": 8.1
      }
    },
    "confidence_score": 1.0,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "lfm25-12b-instruct",
    "canonical_name": "LFM2.5-1.2B-Instruct",
    "model_name": "LFM2.5-1.2B-Instruct",
    "aliases": [
      "LFM2.5-1.2B-Instruct"
    ],
    "provider": null,
    "context_window": 32000,
    "open_source": true,
    "arena_votes": null,
    "license_type": "lfm 1.0",
    "creator": null,
    "intelligence_score": null,
    "coding_score": 0.77,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": null,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": 0.0,
    "latency_seconds": 0.0,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": 0.0,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": 319.0213,
    "terminalbench_hard": 0.0,
    "tau2": 0.108,
    "lcr": 0.0,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": 6.8,
    "gpqa": 32.6,
    "scicode": 2.3,
    "ifbench": 43.8,
    "aime25": null,
    "critpt": 0.0,
    "mmmu_pro": null,
    "livecodebench": null,
    "sources": [
      "Artificial Analysis"
    ],
    "source_count": 1,
    "data_tier": 2,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.906626+00:00",
        "confidence": 0.79,
        "raw_name": "LFM2.5-1.2B-Instruct",
        "raw_scores": {
          "coding_score": 0.77,
          "blended_cost_per_1m": 0.0,
          "latency_seconds": 0.0,
          "tokens_per_second": 0.0,
          "context_window": 32000,
          "gdpval": 319.0212905983557,
          "terminalbench_hard": 0.0,
          "tau2": 0.108,
          "lcr": 0.0,
          "hle": 6.8,
          "gpqa": 32.6,
          "scicode": 2.3,
          "ifbench": 43.8,
          "critpt": 0.0
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "coding_score": 0.77,
        "gdpval": 319.0212905983557,
        "terminalbench_hard": 0.0,
        "tau2": 0.108,
        "lcr": 0.0,
        "hle": 6.8,
        "gpqa": 32.6,
        "scicode": 2.3,
        "ifbench": 43.8,
        "critpt": 0.0
      }
    },
    "confidence_score": 0.79,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "solar-pro-2",
    "canonical_name": "Solar Pro 2 (Non-reasoning)",
    "model_name": "Solar Pro 2 (Non-reasoning)",
    "aliases": [
      "Solar Pro 2 (Non-reasoning)",
      "Solar Pro 2 (Preview) (Non-reasoning)",
      "Solar Pro 2 (Preview) (Reasoning)",
      "Solar Pro 2 (Reasoning)"
    ],
    "provider": null,
    "context_window": 65536,
    "open_source": null,
    "arena_votes": null,
    "license_type": null,
    "creator": null,
    "intelligence_score": 17.6466,
    "coding_score": 11.69,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": null,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": 496.8732,
    "terminalbench_hard": 0.0375,
    "tau2": 0.3,
    "lcr": 0.0,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": 5.1066,
    "gpqa": 59.5562,
    "scicode": 24.9271,
    "ifbench": 35.4,
    "aime25": 45.65,
    "critpt": 0.0,
    "mmmu_pro": null,
    "livecodebench": 47.6441,
    "sources": [
      "Artificial Analysis"
    ],
    "source_count": 1,
    "data_tier": 2,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.906666+00:00",
        "confidence": 0.79,
        "raw_name": "Solar Pro 2 (Non-reasoning)",
        "raw_scores": {
          "intelligence_score": 16.073226748517293,
          "coding_score": 11.29,
          "context_window": 65536,
          "gdpval": 494.40267904496704,
          "terminalbench_hard": 0.045,
          "tau2": 0.319,
          "lcr": 0.0,
          "hle": 3.8,
          "gpqa": 56.1,
          "scicode": 24.8,
          "ifbench": 33.7,
          "aime25": 30.0,
          "critpt": 0.0,
          "livecodebench": 42.4
        }
      },
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.906706+00:00",
        "confidence": 0.79,
        "raw_name": "Solar Pro 2 (Reasoning)",
        "raw_scores": {
          "intelligence_score": 19.62295300802516,
          "coding_score": 12.09,
          "context_window": 65536,
          "gdpval": 499.3436726994478,
          "terminalbench_hard": 0.03,
          "tau2": 0.281,
          "lcr": 0.0,
          "hle": 7.0,
          "gpqa": 68.7,
          "scicode": 30.2,
          "ifbench": 37.1,
          "aime25": 61.3,
          "critpt": 0.0,
          "livecodebench": 61.6
        }
      },
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.916977+00:00",
        "confidence": 0.65,
        "raw_name": "Solar Pro 2 (Preview) (Reasoning)",
        "raw_scores": {
          "intelligence_score": 18.80930200641581,
          "context_window": 64000,
          "hle": 5.7,
          "gpqa": 57.8,
          "scicode": 16.4,
          "livecodebench": 46.2
        }
      },
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.917004+00:00",
        "confidence": 0.65,
        "raw_name": "Solar Pro 2 (Preview) (Non-reasoning)",
        "raw_scores": {
          "intelligence_score": 15.99411151166208,
          "context_window": 64000,
          "hle": 3.8,
          "gpqa": 54.4,
          "scicode": 27.2,
          "livecodebench": 38.5
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 16.073226748517293,
        "coding_score": 11.29,
        "gdpval": 494.40267904496704,
        "terminalbench_hard": 0.045,
        "tau2": 0.319,
        "lcr": 0.0,
        "hle": 3.8,
        "gpqa": 56.1,
        "scicode": 24.8,
        "ifbench": 33.7,
        "aime25": 30.0,
        "critpt": 0.0,
        "livecodebench": 42.4
      }
    },
    "confidence_score": 0.72,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "solar-open-100b",
    "canonical_name": "Solar Open 100B (Reasoning)",
    "model_name": "Solar Open 100B (Reasoning)",
    "aliases": [
      "Solar Open 100B (Reasoning)"
    ],
    "provider": null,
    "context_window": 128000,
    "open_source": null,
    "arena_votes": null,
    "license_type": null,
    "creator": null,
    "intelligence_score": null,
    "coding_score": 10.47,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": null,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": 708.8756,
    "terminalbench_hard": 0.023,
    "tau2": 0.482,
    "lcr": 36.0,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": 9.2,
    "gpqa": 65.7,
    "scicode": 26.9,
    "ifbench": 57.7,
    "aime25": null,
    "critpt": 0.0,
    "mmmu_pro": null,
    "livecodebench": null,
    "sources": [
      "Artificial Analysis"
    ],
    "source_count": 1,
    "data_tier": 2,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.906737+00:00",
        "confidence": 0.5800000000000001,
        "raw_name": "Solar Open 100B (Reasoning)",
        "raw_scores": {
          "coding_score": 10.47,
          "context_window": 128000,
          "gdpval": 708.8756252298529,
          "terminalbench_hard": 0.023,
          "tau2": 0.482,
          "lcr": 36.0,
          "hle": 9.2,
          "gpqa": 65.7,
          "scicode": 26.9,
          "ifbench": 57.7,
          "critpt": 0.0
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "coding_score": 10.47,
        "gdpval": 708.8756252298529,
        "terminalbench_hard": 0.023,
        "tau2": 0.482,
        "lcr": 36.0,
        "hle": 9.2,
        "gpqa": 65.7,
        "scicode": 26.9,
        "ifbench": 57.7,
        "critpt": 0.0
      }
    },
    "confidence_score": 0.58,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "minimax-m25",
    "canonical_name": "MiniMax-M2.5",
    "model_name": "MiniMax-M2.5",
    "aliases": [
      "MiniMax-M2.5",
      "minimax-m2.5"
    ],
    "provider": "MiniMax",
    "context_window": 204800,
    "open_source": true,
    "arena_votes": 6065,
    "license_type": "MIT",
    "creator": null,
    "intelligence_score": 41.93,
    "coding_score": 37.43,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 1401.47,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": 0.53,
    "latency_seconds": 2.87,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": 57.0,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": 1205.6391,
    "terminalbench_hard": 0.348,
    "tau2": 0.953,
    "lcr": 66.0,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": 19.1,
    "gpqa": 84.8,
    "scicode": 42.6,
    "ifbench": 71.6,
    "aime25": null,
    "critpt": 1.1,
    "mmmu_pro": null,
    "livecodebench": null,
    "sources": [
      "Artificial Analysis",
      "LMSYS Chatbot Arena"
    ],
    "source_count": 2,
    "data_tier": 1,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.906774+00:00",
        "confidence": 0.8600000000000001,
        "raw_name": "MiniMax-M2.5",
        "raw_scores": {
          "intelligence_score": 41.93,
          "coding_score": 37.43,
          "blended_cost_per_1m": 0.53,
          "latency_seconds": 2.87,
          "tokens_per_second": 57.0,
          "context_window": 204800,
          "gdpval": 1205.6391113494267,
          "terminalbench_hard": 0.348,
          "tau2": 0.953,
          "lcr": 66.0,
          "hle": 19.1,
          "gpqa": 84.8,
          "scicode": 42.6,
          "ifbench": 71.6,
          "critpt": 1.1
        }
      },
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.892270+00:00",
        "confidence": 1.0,
        "raw_name": "minimax-m2.5",
        "raw_scores": {
          "arena_elo": 1401.47,
          "arena_votes": 6065
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 41.93,
        "coding_score": 37.43,
        "gdpval": 1205.6391113494267,
        "terminalbench_hard": 0.348,
        "tau2": 0.953,
        "lcr": 66.0,
        "hle": 19.1,
        "gpqa": 84.8,
        "scicode": 42.6,
        "ifbench": 71.6,
        "critpt": 1.1
      },
      "LMSYS Chatbot Arena": {
        "arena_elo": 1401.47
      }
    },
    "confidence_score": 0.93,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "llama-31-nemotron-instruct-70b",
    "canonical_name": "Llama 3.1 Nemotron Instruct 70B",
    "model_name": "Llama 3.1 Nemotron Instruct 70B",
    "aliases": [
      "Llama 3.1 Nemotron Instruct 70B"
    ],
    "provider": "Meta",
    "context_window": 128000,
    "open_source": true,
    "arena_votes": null,
    "license_type": "LLAMA 3.1 COMMUNITY LICENSE AGREEMENT",
    "creator": null,
    "intelligence_score": 13.5084,
    "coding_score": 10.78,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": null,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": 395.2511,
    "terminalbench_hard": 0.045,
    "tau2": 0.231,
    "lcr": 7.0,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": 4.6,
    "gpqa": 46.5,
    "scicode": 23.3,
    "ifbench": 30.7,
    "aime25": 11.0,
    "critpt": 0.0,
    "mmmu_pro": null,
    "livecodebench": 16.9,
    "sources": [
      "Artificial Analysis"
    ],
    "source_count": 1,
    "data_tier": 2,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.906818+00:00",
        "confidence": 0.79,
        "raw_name": "Llama 3.1 Nemotron Instruct 70B",
        "raw_scores": {
          "intelligence_score": 13.508443750240778,
          "coding_score": 10.78,
          "context_window": 128000,
          "gdpval": 395.2510563919391,
          "terminalbench_hard": 0.045,
          "tau2": 0.231,
          "lcr": 7.0,
          "hle": 4.6,
          "gpqa": 46.5,
          "scicode": 23.3,
          "ifbench": 30.7,
          "aime25": 11.0,
          "critpt": 0.0,
          "livecodebench": 16.9
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 13.508443750240778,
        "coding_score": 10.78,
        "gdpval": 395.2510563919391,
        "terminalbench_hard": 0.045,
        "tau2": 0.231,
        "lcr": 7.0,
        "hle": 4.6,
        "gpqa": 46.5,
        "scicode": 23.3,
        "ifbench": 30.7,
        "aime25": 11.0,
        "critpt": 0.0,
        "livecodebench": 16.9
      }
    },
    "confidence_score": 0.79,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "nvidia-nemotron-nano-12b-v2-vl",
    "canonical_name": "NVIDIA Nemotron Nano 12B v2 VL (Non-reasoning)",
    "model_name": "NVIDIA Nemotron Nano 12B v2 VL (Non-reasoning)",
    "aliases": [
      "NVIDIA Nemotron Nano 12B v2 VL (Non-reasoning)",
      "NVIDIA Nemotron Nano 12B v2 VL (Reasoning)"
    ],
    "provider": null,
    "context_window": 128000,
    "open_source": true,
    "arena_votes": null,
    "license_type": "Nvidia Open Model License",
    "creator": null,
    "intelligence_score": 17.6832,
    "coding_score": 8.805,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": null,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": 362.7694,
    "terminalbench_hard": 0.0225,
    "tau2": 0.203,
    "lcr": 28.5,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": 4.9,
    "gpqa": 50.55,
    "scicode": 21.9,
    "ifbench": 28.9,
    "aime25": 50.85,
    "critpt": 0.0,
    "mmmu_pro": 48.7,
    "livecodebench": 51.95,
    "sources": [
      "Artificial Analysis"
    ],
    "source_count": 1,
    "data_tier": 2,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.906860+00:00",
        "confidence": 0.79,
        "raw_name": "NVIDIA Nemotron Nano 12B v2 VL (Non-reasoning)",
        "raw_scores": {
          "intelligence_score": 14.16465748909028,
          "coding_score": 5.86,
          "context_window": 128000,
          "gdpval": 342.60546868838924,
          "terminalbench_hard": 0.0,
          "tau2": 0.193,
          "lcr": 17.0,
          "hle": 4.5,
          "gpqa": 43.9,
          "scicode": 17.6,
          "ifbench": 25.9,
          "aime25": 26.7,
          "critpt": 0.0,
          "mmmu_pro": 44.5,
          "livecodebench": 34.5
        }
      },
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.907182+00:00",
        "confidence": 0.79,
        "raw_name": "NVIDIA Nemotron Nano 12B v2 VL (Reasoning)",
        "raw_scores": {
          "intelligence_score": 21.201670015781215,
          "coding_score": 11.75,
          "context_window": 128000,
          "gdpval": 382.9334019735222,
          "terminalbench_hard": 0.045,
          "tau2": 0.213,
          "lcr": 40.0,
          "hle": 5.3,
          "gpqa": 57.2,
          "scicode": 26.2,
          "ifbench": 31.9,
          "aime25": 75.0,
          "critpt": 0.0,
          "mmmu_pro": 52.9,
          "livecodebench": 69.4
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 14.16465748909028,
        "coding_score": 5.86,
        "gdpval": 342.60546868838924,
        "terminalbench_hard": 0.0,
        "tau2": 0.193,
        "lcr": 17.0,
        "hle": 4.5,
        "gpqa": 43.9,
        "scicode": 17.6,
        "ifbench": 25.9,
        "aime25": 26.7,
        "critpt": 0.0,
        "mmmu_pro": 44.5,
        "livecodebench": 34.5
      }
    },
    "confidence_score": 0.79,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "nvidia-nemotron-3-nano-30b-a3b",
    "canonical_name": "NVIDIA Nemotron 3 Nano 30B A3B (Reasoning)",
    "model_name": "NVIDIA Nemotron 3 Nano 30B A3B (Reasoning)",
    "aliases": [
      "NVIDIA Nemotron 3 Nano 30B A3B (Non-reasoning)",
      "NVIDIA Nemotron 3 Nano 30B A3B (Reasoning)"
    ],
    "provider": null,
    "context_window": 1000000,
    "open_source": null,
    "arena_votes": null,
    "license_type": null,
    "creator": null,
    "intelligence_score": 19.2437,
    "coding_score": 17.365,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": null,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": 501.3176,
    "terminalbench_hard": 0.1285,
    "tau2": 0.3315,
    "lcr": 20.2,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": 7.4,
    "gpqa": 57.8,
    "scicode": 26.3,
    "ifbench": 54.3,
    "aime25": 52.15,
    "critpt": 0.45,
    "mmmu_pro": null,
    "livecodebench": 55.05,
    "sources": [
      "Artificial Analysis"
    ],
    "source_count": 1,
    "data_tier": 2,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.906900+00:00",
        "confidence": 0.79,
        "raw_name": "NVIDIA Nemotron 3 Nano 30B A3B (Reasoning)",
        "raw_scores": {
          "intelligence_score": 24.27,
          "coding_score": 18.97,
          "context_window": 1000000,
          "gdpval": 608.2078350257842,
          "terminalbench_hard": 0.136,
          "tau2": 0.409,
          "lcr": 33.7,
          "hle": 10.2,
          "gpqa": 75.7,
          "scicode": 29.6,
          "ifbench": 71.1,
          "aime25": 91.0,
          "critpt": 0.9,
          "livecodebench": 74.1
        }
      },
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.906942+00:00",
        "confidence": 0.79,
        "raw_name": "NVIDIA Nemotron 3 Nano 30B A3B (Non-reasoning)",
        "raw_scores": {
          "intelligence_score": 14.217305970920366,
          "coding_score": 15.76,
          "context_window": 1000000,
          "gdpval": 394.42731810971486,
          "terminalbench_hard": 0.121,
          "tau2": 0.254,
          "lcr": 6.7,
          "hle": 4.6,
          "gpqa": 39.9,
          "scicode": 23.0,
          "ifbench": 37.5,
          "aime25": 13.3,
          "critpt": 0.0,
          "livecodebench": 36.0
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 24.27,
        "coding_score": 18.97,
        "gdpval": 608.2078350257842,
        "terminalbench_hard": 0.136,
        "tau2": 0.409,
        "lcr": 33.7,
        "hle": 10.2,
        "gpqa": 75.7,
        "scicode": 29.6,
        "ifbench": 71.1,
        "aime25": 91.0,
        "critpt": 0.9,
        "livecodebench": 74.1
      }
    },
    "confidence_score": 0.79,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "nvidia-nemotron-nano-9b-v2",
    "canonical_name": "NVIDIA Nemotron Nano 9B V2 (Non-reasoning)",
    "model_name": "NVIDIA Nemotron Nano 9B V2 (Non-reasoning)",
    "aliases": [
      "NVIDIA Nemotron Nano 9B V2 (Non-reasoning)",
      "NVIDIA Nemotron Nano 9B V2 (Reasoning)"
    ],
    "provider": null,
    "context_window": 131072,
    "open_source": true,
    "arena_votes": null,
    "license_type": "NVIDIA Open Model License Agreement",
    "creator": null,
    "intelligence_score": 16.7742,
    "coding_score": 7.915,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": null,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": 425.9722,
    "terminalbench_hard": 0.0115,
    "tau2": 0.2265,
    "lcr": 21.85,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": 4.3,
    "gpqa": 56.35,
    "scicode": 21.45,
    "ifbench": 27.35,
    "aime25": 66.0,
    "critpt": 0.0,
    "mmmu_pro": null,
    "livecodebench": 71.25,
    "sources": [
      "Artificial Analysis"
    ],
    "source_count": 1,
    "data_tier": 2,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.906982+00:00",
        "confidence": 0.79,
        "raw_name": "NVIDIA Nemotron Nano 9B V2 (Non-reasoning)",
        "raw_scores": {
          "intelligence_score": 18.788405538471565,
          "coding_score": 7.49,
          "context_window": 131072,
          "gdpval": 366.3303045644327,
          "terminalbench_hard": 0.008,
          "tau2": 0.234,
          "lcr": 22.7,
          "hle": 4.0,
          "gpqa": 55.7,
          "scicode": 20.9,
          "ifbench": 27.1,
          "aime25": 62.3,
          "critpt": 0.0,
          "livecodebench": 70.1
        }
      },
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.907102+00:00",
        "confidence": 0.79,
        "raw_name": "NVIDIA Nemotron Nano 9B V2 (Reasoning)",
        "raw_scores": {
          "intelligence_score": 14.76,
          "coding_score": 8.34,
          "context_window": 131072,
          "gdpval": 485.6141187751441,
          "terminalbench_hard": 0.015,
          "tau2": 0.219,
          "lcr": 21.0,
          "hle": 4.6,
          "gpqa": 57.0,
          "scicode": 22.0,
          "ifbench": 27.6,
          "aime25": 69.7,
          "critpt": 0.0,
          "livecodebench": 72.4
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 18.788405538471565,
        "coding_score": 7.49,
        "gdpval": 366.3303045644327,
        "terminalbench_hard": 0.008,
        "tau2": 0.234,
        "lcr": 22.7,
        "hle": 4.0,
        "gpqa": 55.7,
        "scicode": 20.9,
        "ifbench": 27.1,
        "aime25": 62.3,
        "critpt": 0.0,
        "livecodebench": 70.1
      }
    },
    "confidence_score": 0.79,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "llama-nemotron-super-49b-v15",
    "canonical_name": "Llama Nemotron Super 49B v1.5 (Non-reasoning)",
    "model_name": "Llama Nemotron Super 49B v1.5 (Non-reasoning)",
    "aliases": [
      "Llama Nemotron Super 49B v1.5 (Non-reasoning)",
      "Llama Nemotron Super 49B v1.5 (Reasoning)"
    ],
    "provider": "Meta",
    "context_window": 128000,
    "open_source": true,
    "arena_votes": null,
    "license_type": "NVIDIA Open Model License Agreement",
    "creator": null,
    "intelligence_score": 19.271,
    "coding_score": 12.81,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": null,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": 425.1513,
    "terminalbench_hard": 0.0455,
    "tau2": 0.266,
    "lcr": 28.0,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": 5.55,
    "gpqa": 61.45,
    "scicode": 29.3,
    "ifbench": 34.95,
    "aime25": 42.35,
    "critpt": 0.0,
    "mmmu_pro": null,
    "livecodebench": 51.35,
    "sources": [
      "Artificial Analysis"
    ],
    "source_count": 1,
    "data_tier": 2,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.907023+00:00",
        "confidence": 0.79,
        "raw_name": "Llama Nemotron Super 49B v1.5 (Non-reasoning)",
        "raw_scores": {
          "intelligence_score": 14.610711920891266,
          "coding_score": 10.47,
          "context_window": 128000,
          "gdpval": 434.0387129526175,
          "terminalbench_hard": 0.038,
          "tau2": 0.251,
          "lcr": 22.0,
          "hle": 4.3,
          "gpqa": 48.1,
          "scicode": 23.8,
          "ifbench": 32.9,
          "aime25": 8.0,
          "critpt": 0.0,
          "livecodebench": 29.0
        }
      },
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.907064+00:00",
        "confidence": 0.79,
        "raw_name": "Llama Nemotron Super 49B v1.5 (Reasoning)",
        "raw_scores": {
          "intelligence_score": 23.931245612475355,
          "coding_score": 15.15,
          "context_window": 128000,
          "gdpval": 416.26379173547036,
          "terminalbench_hard": 0.053,
          "tau2": 0.281,
          "lcr": 34.0,
          "hle": 6.8,
          "gpqa": 74.8,
          "scicode": 34.8,
          "ifbench": 37.0,
          "aime25": 76.7,
          "critpt": 0.0,
          "livecodebench": 73.7
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 14.610711920891266,
        "coding_score": 10.47,
        "gdpval": 434.0387129526175,
        "terminalbench_hard": 0.038,
        "tau2": 0.251,
        "lcr": 22.0,
        "hle": 4.3,
        "gpqa": 48.1,
        "scicode": 23.8,
        "ifbench": 32.9,
        "aime25": 8.0,
        "critpt": 0.0,
        "livecodebench": 29.0
      }
    },
    "confidence_score": 0.79,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "llama-33-nemotron-super-49b-v1",
    "canonical_name": "Llama 3.3 Nemotron Super 49B v1 (Reasoning)",
    "model_name": "Llama 3.3 Nemotron Super 49B v1 (Reasoning)",
    "aliases": [
      "Llama 3.3 Nemotron Super 49B v1 (Non-reasoning)",
      "Llama 3.3 Nemotron Super 49B v1 (Reasoning)"
    ],
    "provider": "Meta",
    "context_window": 128000,
    "open_source": true,
    "arena_votes": null,
    "license_type": "NVIDIA Open Model License Agreement",
    "creator": null,
    "intelligence_score": 16.4192,
    "coding_score": 8.525,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": null,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": 0.0,
    "tau2": 0.269,
    "lcr": 14.15,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": 5.0,
    "gpqa": 58.0,
    "scicode": 25.55,
    "ifbench": 38.8,
    "aime25": 31.2,
    "critpt": 0.0,
    "mmmu_pro": null,
    "livecodebench": 27.85,
    "sources": [
      "Artificial Analysis"
    ],
    "source_count": 1,
    "data_tier": 2,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.907141+00:00",
        "confidence": 0.79,
        "raw_name": "Llama 3.3 Nemotron Super 49B v1 (Reasoning)",
        "raw_scores": {
          "intelligence_score": 18.49174710104611,
          "coding_score": 9.41,
          "context_window": 128000,
          "terminalbench_hard": 0.0,
          "tau2": 0.269,
          "lcr": 17.0,
          "hle": 6.5,
          "gpqa": 64.3,
          "scicode": 28.2,
          "ifbench": 38.1,
          "aime25": 54.7,
          "critpt": 0.0,
          "livecodebench": 27.7
        }
      },
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.907269+00:00",
        "confidence": 0.79,
        "raw_name": "Llama 3.3 Nemotron Super 49B v1 (Non-reasoning)",
        "raw_scores": {
          "intelligence_score": 14.34668262331532,
          "coding_score": 7.64,
          "context_window": 128000,
          "terminalbench_hard": 0.0,
          "lcr": 11.3,
          "hle": 3.5,
          "gpqa": 51.7,
          "scicode": 22.9,
          "ifbench": 39.5,
          "aime25": 7.7,
          "critpt": 0.0,
          "livecodebench": 28.0
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 18.49174710104611,
        "coding_score": 9.41,
        "terminalbench_hard": 0.0,
        "tau2": 0.269,
        "lcr": 17.0,
        "hle": 6.5,
        "gpqa": 64.3,
        "scicode": 28.2,
        "ifbench": 38.1,
        "aime25": 54.7,
        "critpt": 0.0,
        "livecodebench": 27.7
      }
    },
    "confidence_score": 0.79,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "llama-31-nemotron-nano-4b-v11",
    "canonical_name": "Llama 3.1 Nemotron Nano 4B v1.1 (Reasoning)",
    "model_name": "Llama 3.1 Nemotron Nano 4B v1.1 (Reasoning)",
    "aliases": [
      "Llama 3.1 Nemotron Nano 4B v1.1 (Reasoning)"
    ],
    "provider": "Meta",
    "context_window": 128000,
    "open_source": true,
    "arena_votes": null,
    "license_type": "NVIDIA Open Model License Agreement",
    "creator": null,
    "intelligence_score": 14.4337,
    "coding_score": null,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": null,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": null,
    "tau2": 0.117,
    "lcr": 0.0,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": 5.1,
    "gpqa": 40.8,
    "scicode": 10.1,
    "ifbench": 25.5,
    "aime25": 50.0,
    "critpt": null,
    "mmmu_pro": null,
    "livecodebench": 49.3,
    "sources": [
      "Artificial Analysis"
    ],
    "source_count": 1,
    "data_tier": 2,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.907231+00:00",
        "confidence": 0.72,
        "raw_name": "Llama 3.1 Nemotron Nano 4B v1.1 (Reasoning)",
        "raw_scores": {
          "intelligence_score": 14.433728654243223,
          "context_window": 128000,
          "tau2": 0.117,
          "lcr": 0.0,
          "hle": 5.1,
          "gpqa": 40.8,
          "scicode": 10.1,
          "ifbench": 25.5,
          "aime25": 50.0,
          "livecodebench": 49.3
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 14.433728654243223,
        "tau2": 0.117,
        "lcr": 0.0,
        "hle": 5.1,
        "gpqa": 40.8,
        "scicode": 10.1,
        "ifbench": 25.5,
        "aime25": 50.0,
        "livecodebench": 49.3
      }
    },
    "confidence_score": 0.72,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "llama-31-nemotron-ultra-253b-v1",
    "canonical_name": "Llama 3.1 Nemotron Ultra 253B v1 (Reasoning)",
    "model_name": "Llama 3.1 Nemotron Ultra 253B v1 (Reasoning)",
    "aliases": [
      "Llama 3.1 Nemotron Ultra 253B v1 (Reasoning)",
      "llama-3.1-nemotron-ultra-253b-v1"
    ],
    "provider": "Nvidia",
    "context_window": 128000,
    "open_source": true,
    "arena_votes": 2546,
    "license_type": "NVIDIA Open Model License Agreement",
    "creator": null,
    "intelligence_score": 20.0222,
    "coding_score": 13.09,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 1347.23,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": 284.8831,
    "terminalbench_hard": 0.023,
    "tau2": 0.114,
    "lcr": 7.3,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": 8.1,
    "gpqa": 72.8,
    "scicode": 34.7,
    "ifbench": 38.2,
    "aime25": 63.7,
    "critpt": 0.0,
    "mmmu_pro": null,
    "livecodebench": 64.1,
    "sources": [
      "Artificial Analysis",
      "LMSYS Chatbot Arena"
    ],
    "source_count": 2,
    "data_tier": 1,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.907310+00:00",
        "confidence": 0.79,
        "raw_name": "Llama 3.1 Nemotron Ultra 253B v1 (Reasoning)",
        "raw_scores": {
          "intelligence_score": 20.02216586108181,
          "coding_score": 13.09,
          "context_window": 128000,
          "gdpval": 284.8831475488837,
          "terminalbench_hard": 0.023,
          "tau2": 0.114,
          "lcr": 7.3,
          "hle": 8.1,
          "gpqa": 72.8,
          "scicode": 34.7,
          "ifbench": 38.2,
          "aime25": 63.7,
          "critpt": 0.0,
          "livecodebench": 64.1
        }
      },
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.893093+00:00",
        "confidence": 1.0,
        "raw_name": "llama-3.1-nemotron-ultra-253b-v1",
        "raw_scores": {
          "arena_elo": 1347.23,
          "arena_votes": 2546
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 20.02216586108181,
        "coding_score": 13.09,
        "gdpval": 284.8831475488837,
        "terminalbench_hard": 0.023,
        "tau2": 0.114,
        "lcr": 7.3,
        "hle": 8.1,
        "gpqa": 72.8,
        "scicode": 34.7,
        "ifbench": 38.2,
        "aime25": 63.7,
        "critpt": 0.0,
        "livecodebench": 64.1
      },
      "LMSYS Chatbot Arena": {
        "arena_elo": 1347.23
      }
    },
    "confidence_score": 0.895,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "kimi-linear-48b-a3b-instruct",
    "canonical_name": "Kimi Linear 48B A3B Instruct",
    "model_name": "Kimi Linear 48B A3B Instruct",
    "aliases": [
      "Kimi Linear 48B A3B Instruct"
    ],
    "provider": "Moonshot",
    "context_window": 1000000,
    "open_source": true,
    "arena_votes": null,
    "license_type": "MIT",
    "creator": null,
    "intelligence_score": 14.4146,
    "coding_score": 14.21,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": null,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": 0.0,
    "latency_seconds": 0.0,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": 0.0,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": 0.114,
    "tau2": 0.0,
    "lcr": 25.7,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": 2.7,
    "gpqa": 41.2,
    "scicode": 19.9,
    "ifbench": 28.1,
    "aime25": 36.3,
    "critpt": null,
    "mmmu_pro": null,
    "livecodebench": 37.8,
    "sources": [
      "Artificial Analysis"
    ],
    "source_count": 1,
    "data_tier": 2,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.907349+00:00",
        "confidence": 1.0,
        "raw_name": "Kimi Linear 48B A3B Instruct",
        "raw_scores": {
          "intelligence_score": 14.414576519674707,
          "coding_score": 14.21,
          "blended_cost_per_1m": 0.0,
          "latency_seconds": 0.0,
          "tokens_per_second": 0.0,
          "context_window": 1000000,
          "terminalbench_hard": 0.114,
          "tau2": 0.0,
          "lcr": 25.7,
          "hle": 2.7,
          "gpqa": 41.2,
          "scicode": 19.9,
          "ifbench": 28.1,
          "aime25": 36.3,
          "livecodebench": 37.8
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 14.414576519674707,
        "coding_score": 14.21,
        "terminalbench_hard": 0.114,
        "tau2": 0.0,
        "lcr": 25.7,
        "hle": 2.7,
        "gpqa": 41.2,
        "scicode": 19.9,
        "ifbench": 28.1,
        "aime25": 36.3,
        "livecodebench": 37.8
      }
    },
    "confidence_score": 1.0,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "kimi-k25",
    "canonical_name": "Kimi K2.5 (Reasoning)",
    "model_name": "Kimi K2.5 (Reasoning)",
    "aliases": [
      "Kimi K2.5 (Non-reasoning)",
      "Kimi K2.5 (Reasoning)",
      "kimi-k2.5-thinking"
    ],
    "provider": "Moonshot",
    "context_window": 256000,
    "open_source": true,
    "arena_votes": 11075,
    "license_type": "Modified MIT License",
    "creator": null,
    "intelligence_score": 42.04,
    "coding_score": 32.685,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 1451.66,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": 1281.7235,
    "terminalbench_hard": 0.2685,
    "tau2": 0.886,
    "lcr": 62.15,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": 20.85,
    "gpqa": 83.4,
    "scicode": 44.3,
    "ifbench": 56.95,
    "aime25": null,
    "critpt": 1.85,
    "mmmu_pro": 74.25,
    "livecodebench": null,
    "sources": [
      "Artificial Analysis",
      "LMSYS Chatbot Arena"
    ],
    "source_count": 2,
    "data_tier": 1,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.907385+00:00",
        "confidence": 0.65,
        "raw_name": "Kimi K2.5 (Reasoning)",
        "raw_scores": {
          "intelligence_score": 46.81,
          "coding_score": 39.55,
          "context_window": 256000,
          "gdpval": 1282.084467616367,
          "terminalbench_hard": 0.348,
          "tau2": 0.959,
          "lcr": 65.3,
          "hle": 29.4,
          "gpqa": 87.9,
          "scicode": 49.0,
          "ifbench": 70.2,
          "critpt": 3.1,
          "mmmu_pro": 75.4
        }
      },
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.907420+00:00",
        "confidence": 0.65,
        "raw_name": "Kimi K2.5 (Non-reasoning)",
        "raw_scores": {
          "intelligence_score": 37.27,
          "coding_score": 25.82,
          "context_window": 256000,
          "gdpval": 1281.3624434816163,
          "terminalbench_hard": 0.189,
          "tau2": 0.813,
          "lcr": 59.0,
          "hle": 12.3,
          "gpqa": 78.9,
          "scicode": 39.6,
          "ifbench": 43.7,
          "critpt": 0.6,
          "mmmu_pro": 73.1
        }
      },
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.891534+00:00",
        "confidence": 1.0,
        "raw_name": "kimi-k2.5-thinking",
        "raw_scores": {
          "arena_elo": 1451.66,
          "arena_votes": 11075
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 46.81,
        "coding_score": 39.55,
        "gdpval": 1282.084467616367,
        "terminalbench_hard": 0.348,
        "tau2": 0.959,
        "lcr": 65.3,
        "hle": 29.4,
        "gpqa": 87.9,
        "scicode": 49.0,
        "ifbench": 70.2,
        "critpt": 3.1,
        "mmmu_pro": 75.4
      },
      "LMSYS Chatbot Arena": {
        "arena_elo": 1451.66
      }
    },
    "confidence_score": 0.7667,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "step3-vl-10b",
    "canonical_name": "Step3 VL 10B",
    "model_name": "Step3 VL 10B",
    "aliases": [
      "Step3 VL 10B"
    ],
    "provider": null,
    "context_window": 65536,
    "open_source": true,
    "arena_votes": null,
    "license_type": "Apache 2.0",
    "creator": null,
    "intelligence_score": null,
    "coding_score": 13.91,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": null,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": 0.0,
    "latency_seconds": 0.0,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": 0.0,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": 313.7637,
    "terminalbench_hard": 0.053,
    "tau2": 0.161,
    "lcr": 0.0,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": 10.2,
    "gpqa": 69.0,
    "scicode": 31.1,
    "ifbench": 50.2,
    "aime25": null,
    "critpt": 0.0,
    "mmmu_pro": 64.0,
    "livecodebench": null,
    "sources": [
      "Artificial Analysis"
    ],
    "source_count": 1,
    "data_tier": 2,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.907457+00:00",
        "confidence": 0.79,
        "raw_name": "Step3 VL 10B",
        "raw_scores": {
          "coding_score": 13.91,
          "blended_cost_per_1m": 0.0,
          "latency_seconds": 0.0,
          "tokens_per_second": 0.0,
          "context_window": 65536,
          "gdpval": 313.76370980188904,
          "terminalbench_hard": 0.053,
          "tau2": 0.161,
          "lcr": 0.0,
          "hle": 10.2,
          "gpqa": 69.0,
          "scicode": 31.1,
          "ifbench": 50.2,
          "critpt": 0.0,
          "mmmu_pro": 64.0
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "coding_score": 13.91,
        "gdpval": 313.76370980188904,
        "terminalbench_hard": 0.053,
        "tau2": 0.161,
        "lcr": 0.0,
        "hle": 10.2,
        "gpqa": 69.0,
        "scicode": 31.1,
        "ifbench": 50.2,
        "critpt": 0.0,
        "mmmu_pro": 64.0
      }
    },
    "confidence_score": 0.79,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "molmo-7b-d",
    "canonical_name": "Molmo 7B-D",
    "model_name": "Molmo 7B-D",
    "aliases": [
      "Molmo 7B-D"
    ],
    "provider": null,
    "context_window": 4096,
    "open_source": true,
    "arena_votes": null,
    "license_type": "Apache 2.0",
    "creator": null,
    "intelligence_score": 9.2476,
    "coding_score": 1.2,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": null,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": 0.0,
    "latency_seconds": 0.0,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": 0.0,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": 0.0,
    "tau2": 0.0,
    "lcr": 0.0,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": 5.1,
    "gpqa": 24.0,
    "scicode": 3.6,
    "ifbench": 19.7,
    "aime25": 0.0,
    "critpt": null,
    "mmmu_pro": 24.5,
    "livecodebench": 3.9,
    "sources": [
      "Artificial Analysis"
    ],
    "source_count": 1,
    "data_tier": 2,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.907497+00:00",
        "confidence": 1.0,
        "raw_name": "Molmo 7B-D",
        "raw_scores": {
          "intelligence_score": 9.247608272030813,
          "coding_score": 1.2,
          "blended_cost_per_1m": 0.0,
          "latency_seconds": 0.0,
          "tokens_per_second": 0.0,
          "context_window": 4096,
          "terminalbench_hard": 0.0,
          "tau2": 0.0,
          "lcr": 0.0,
          "hle": 5.1,
          "gpqa": 24.0,
          "scicode": 3.6,
          "ifbench": 19.7,
          "aime25": 0.0,
          "mmmu_pro": 24.5,
          "livecodebench": 3.9
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 9.247608272030813,
        "coding_score": 1.2,
        "terminalbench_hard": 0.0,
        "tau2": 0.0,
        "lcr": 0.0,
        "hle": 5.1,
        "gpqa": 24.0,
        "scicode": 3.6,
        "ifbench": 19.7,
        "aime25": 0.0,
        "mmmu_pro": 24.5,
        "livecodebench": 3.9
      }
    },
    "confidence_score": 1.0,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "olmo-3-7b-instruct",
    "canonical_name": "Olmo 3 7B Instruct",
    "model_name": "Olmo 3 7B Instruct",
    "aliases": [
      "Olmo 3 7B Instruct"
    ],
    "provider": null,
    "context_window": 65536,
    "open_source": true,
    "arena_votes": null,
    "license_type": "Apache 2.0",
    "creator": null,
    "intelligence_score": 12.9889,
    "coding_score": 3.43,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": null,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": 337.3302,
    "terminalbench_hard": 0.0,
    "tau2": 0.126,
    "lcr": 0.0,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": 5.8,
    "gpqa": 40.0,
    "scicode": 10.3,
    "ifbench": 32.8,
    "aime25": 41.3,
    "critpt": 0.0,
    "mmmu_pro": null,
    "livecodebench": 26.6,
    "sources": [
      "Artificial Analysis"
    ],
    "source_count": 1,
    "data_tier": 2,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.907537+00:00",
        "confidence": 0.79,
        "raw_name": "Olmo 3 7B Instruct",
        "raw_scores": {
          "intelligence_score": 12.988897484456922,
          "coding_score": 3.43,
          "context_window": 65536,
          "gdpval": 337.33024160500554,
          "terminalbench_hard": 0.0,
          "tau2": 0.126,
          "lcr": 0.0,
          "hle": 5.8,
          "gpqa": 40.0,
          "scicode": 10.3,
          "ifbench": 32.8,
          "aime25": 41.3,
          "critpt": 0.0,
          "livecodebench": 26.6
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 12.988897484456922,
        "coding_score": 3.43,
        "gdpval": 337.33024160500554,
        "terminalbench_hard": 0.0,
        "tau2": 0.126,
        "lcr": 0.0,
        "hle": 5.8,
        "gpqa": 40.0,
        "scicode": 10.3,
        "ifbench": 32.8,
        "aime25": 41.3,
        "critpt": 0.0,
        "livecodebench": 26.6
      }
    },
    "confidence_score": 0.79,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "molmo2-8b",
    "canonical_name": "Molmo2-8B",
    "model_name": "Molmo2-8B",
    "aliases": [
      "Molmo2-8B"
    ],
    "provider": null,
    "context_window": 36864,
    "open_source": true,
    "arena_votes": null,
    "license_type": "Apache 2.0",
    "creator": null,
    "intelligence_score": null,
    "coding_score": 4.44,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": null,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": 0.0,
    "latency_seconds": 0.41,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": 113.0,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": 500.0,
    "terminalbench_hard": 0.0,
    "tau2": 0.0,
    "lcr": 0.0,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": 4.4,
    "gpqa": 42.5,
    "scicode": 13.3,
    "ifbench": 26.9,
    "aime25": null,
    "critpt": 0.0,
    "mmmu_pro": 37.5,
    "livecodebench": null,
    "sources": [
      "Artificial Analysis"
    ],
    "source_count": 1,
    "data_tier": 2,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.907571+00:00",
        "confidence": 0.79,
        "raw_name": "Molmo2-8B",
        "raw_scores": {
          "coding_score": 4.44,
          "blended_cost_per_1m": 0.0,
          "latency_seconds": 0.41,
          "tokens_per_second": 113.0,
          "context_window": 36864,
          "gdpval": 500.0,
          "terminalbench_hard": 0.0,
          "tau2": 0.0,
          "lcr": 0.0,
          "hle": 4.4,
          "gpqa": 42.5,
          "scicode": 13.3,
          "ifbench": 26.9,
          "critpt": 0.0,
          "mmmu_pro": 37.5
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "coding_score": 4.44,
        "gdpval": 500.0,
        "terminalbench_hard": 0.0,
        "tau2": 0.0,
        "lcr": 0.0,
        "hle": 4.4,
        "gpqa": 42.5,
        "scicode": 13.3,
        "ifbench": 26.9,
        "critpt": 0.0,
        "mmmu_pro": 37.5
      }
    },
    "confidence_score": 0.79,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "olmo-3-7b-think",
    "canonical_name": "Olmo 3 7B Think",
    "model_name": "Olmo 3 7B Think",
    "aliases": [
      "Olmo 3 7B Think"
    ],
    "provider": null,
    "context_window": 65536,
    "open_source": true,
    "arena_votes": null,
    "license_type": "Apache 2.0",
    "creator": null,
    "intelligence_score": 16.7961,
    "coding_score": 7.57,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": null,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": 0.14,
    "latency_seconds": 0.42,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": 68.0,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": 0.008,
    "tau2": 0.0,
    "lcr": 0.0,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": 5.7,
    "gpqa": 51.6,
    "scicode": 21.2,
    "ifbench": 41.5,
    "aime25": 70.7,
    "critpt": 0.0,
    "mmmu_pro": null,
    "livecodebench": 61.7,
    "sources": [
      "Artificial Analysis"
    ],
    "source_count": 1,
    "data_tier": 2,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.907613+00:00",
        "confidence": 1.0,
        "raw_name": "Olmo 3 7B Think",
        "raw_scores": {
          "intelligence_score": 16.796070376777948,
          "coding_score": 7.57,
          "blended_cost_per_1m": 0.14,
          "latency_seconds": 0.42,
          "tokens_per_second": 68.0,
          "context_window": 65536,
          "terminalbench_hard": 0.008,
          "tau2": 0.0,
          "lcr": 0.0,
          "hle": 5.7,
          "gpqa": 51.6,
          "scicode": 21.2,
          "ifbench": 41.5,
          "aime25": 70.7,
          "critpt": 0.0,
          "livecodebench": 61.7
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 16.796070376777948,
        "coding_score": 7.57,
        "terminalbench_hard": 0.008,
        "tau2": 0.0,
        "lcr": 0.0,
        "hle": 5.7,
        "gpqa": 51.6,
        "scicode": 21.2,
        "ifbench": 41.5,
        "aime25": 70.7,
        "critpt": 0.0,
        "livecodebench": 61.7
      }
    },
    "confidence_score": 1.0,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "olmo-31-32b-instruct",
    "canonical_name": "Olmo 3.1 32B Instruct",
    "model_name": "Olmo 3.1 32B Instruct",
    "aliases": [
      "Olmo 3.1 32B Instruct",
      "olmo-3.1-32b-instruct"
    ],
    "provider": "Ai2",
    "context_window": 65536,
    "open_source": true,
    "arena_votes": 12252,
    "license_type": "Apache 2.0",
    "creator": null,
    "intelligence_score": null,
    "coding_score": 5.56,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 1330.45,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": 0.3,
    "latency_seconds": 0.26,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": 44.0,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": 403.9209,
    "terminalbench_hard": 0.0,
    "tau2": 0.213,
    "lcr": 0.0,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": 4.9,
    "gpqa": 53.9,
    "scicode": 16.7,
    "ifbench": 39.2,
    "aime25": null,
    "critpt": 0.0,
    "mmmu_pro": null,
    "livecodebench": null,
    "sources": [
      "Artificial Analysis",
      "LMSYS Chatbot Arena"
    ],
    "source_count": 2,
    "data_tier": 1,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.907647+00:00",
        "confidence": 0.79,
        "raw_name": "Olmo 3.1 32B Instruct",
        "raw_scores": {
          "coding_score": 5.56,
          "blended_cost_per_1m": 0.3,
          "latency_seconds": 0.26,
          "tokens_per_second": 44.0,
          "context_window": 65536,
          "gdpval": 403.92088814772615,
          "terminalbench_hard": 0.0,
          "tau2": 0.213,
          "lcr": 0.0,
          "hle": 4.9,
          "gpqa": 53.9,
          "scicode": 16.7,
          "ifbench": 39.2,
          "critpt": 0.0
        }
      },
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.893405+00:00",
        "confidence": 1.0,
        "raw_name": "olmo-3.1-32b-instruct",
        "raw_scores": {
          "arena_elo": 1330.45,
          "arena_votes": 12252
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "coding_score": 5.56,
        "gdpval": 403.92088814772615,
        "terminalbench_hard": 0.0,
        "tau2": 0.213,
        "lcr": 0.0,
        "hle": 4.9,
        "gpqa": 53.9,
        "scicode": 16.7,
        "ifbench": 39.2,
        "critpt": 0.0
      },
      "LMSYS Chatbot Arena": {
        "arena_elo": 1330.45
      }
    },
    "confidence_score": 0.895,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "olmo-31-32b-think",
    "canonical_name": "Olmo 3.1 32B Think",
    "model_name": "Olmo 3.1 32B Think",
    "aliases": [
      "Olmo 3.1 32B Think",
      "olmo-3.1-32b-think"
    ],
    "provider": "Ai2",
    "context_window": 65500,
    "open_source": true,
    "arena_votes": 8442,
    "license_type": "Apache 2.0",
    "creator": null,
    "intelligence_score": null,
    "coding_score": 9.76,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 1285.07,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": 0.0,
    "latency_seconds": 0.49,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": 74.0,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": 0.0,
    "tau2": 0.0,
    "lcr": 0.0,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": 6.0,
    "gpqa": 59.1,
    "scicode": 29.3,
    "ifbench": 66.0,
    "aime25": 77.3,
    "critpt": 0.0,
    "mmmu_pro": null,
    "livecodebench": 69.5,
    "sources": [
      "Artificial Analysis",
      "LMSYS Chatbot Arena"
    ],
    "source_count": 2,
    "data_tier": 1,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.907684+00:00",
        "confidence": 0.9300000000000002,
        "raw_name": "Olmo 3.1 32B Think",
        "raw_scores": {
          "coding_score": 9.76,
          "blended_cost_per_1m": 0.0,
          "latency_seconds": 0.49,
          "tokens_per_second": 74.0,
          "context_window": 65500,
          "terminalbench_hard": 0.0,
          "tau2": 0.0,
          "lcr": 0.0,
          "hle": 6.0,
          "gpqa": 59.1,
          "scicode": 29.3,
          "ifbench": 66.0,
          "aime25": 77.3,
          "critpt": 0.0,
          "livecodebench": 69.5
        }
      },
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.894061+00:00",
        "confidence": 1.0,
        "raw_name": "olmo-3.1-32b-think",
        "raw_scores": {
          "arena_elo": 1285.07,
          "arena_votes": 8442
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "coding_score": 9.76,
        "terminalbench_hard": 0.0,
        "tau2": 0.0,
        "lcr": 0.0,
        "hle": 6.0,
        "gpqa": 59.1,
        "scicode": 29.3,
        "ifbench": 66.0,
        "aime25": 77.3,
        "critpt": 0.0,
        "livecodebench": 69.5
      },
      "LMSYS Chatbot Arena": {
        "arena_elo": 1285.07
      }
    },
    "confidence_score": 0.965,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "granite-40-1b",
    "canonical_name": "Granite 4.0 1B",
    "model_name": "Granite 4.0 1B",
    "aliases": [
      "Granite 4.0 1B"
    ],
    "provider": null,
    "context_window": 128000,
    "open_source": true,
    "arena_votes": null,
    "license_type": "Apache 2.0",
    "creator": null,
    "intelligence_score": 10.2626,
    "coding_score": 2.89,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": null,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": 0.0,
    "latency_seconds": 0.0,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": 0.0,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": 322.8581,
    "terminalbench_hard": 0.0,
    "tau2": 0.228,
    "lcr": 4.0,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": 5.1,
    "gpqa": 28.1,
    "scicode": 8.7,
    "ifbench": 20.5,
    "aime25": 6.3,
    "critpt": 0.0,
    "mmmu_pro": null,
    "livecodebench": 4.7,
    "sources": [
      "Artificial Analysis"
    ],
    "source_count": 1,
    "data_tier": 2,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.907727+00:00",
        "confidence": 1.0,
        "raw_name": "Granite 4.0 1B",
        "raw_scores": {
          "intelligence_score": 10.262638320202154,
          "coding_score": 2.89,
          "blended_cost_per_1m": 0.0,
          "latency_seconds": 0.0,
          "tokens_per_second": 0.0,
          "context_window": 128000,
          "gdpval": 322.8581047810237,
          "terminalbench_hard": 0.0,
          "tau2": 0.228,
          "lcr": 4.0,
          "hle": 5.1,
          "gpqa": 28.1,
          "scicode": 8.7,
          "ifbench": 20.5,
          "aime25": 6.3,
          "critpt": 0.0,
          "livecodebench": 4.7
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 10.262638320202154,
        "coding_score": 2.89,
        "gdpval": 322.8581047810237,
        "terminalbench_hard": 0.0,
        "tau2": 0.228,
        "lcr": 4.0,
        "hle": 5.1,
        "gpqa": 28.1,
        "scicode": 8.7,
        "ifbench": 20.5,
        "aime25": 6.3,
        "critpt": 0.0,
        "livecodebench": 4.7
      }
    },
    "confidence_score": 1.0,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "granite-40-micro",
    "canonical_name": "Granite 4.0 Micro",
    "model_name": "Granite 4.0 Micro",
    "aliases": [
      "Granite 4.0 Micro"
    ],
    "provider": null,
    "context_window": 128000,
    "open_source": true,
    "arena_votes": null,
    "license_type": "Apache 2.0",
    "creator": null,
    "intelligence_score": 11.0942,
    "coding_score": 4.98,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": null,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": 0.0,
    "latency_seconds": 0.0,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": 0.0,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": 351.1175,
    "terminalbench_hard": 0.015,
    "tau2": 0.126,
    "lcr": 4.0,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": 5.1,
    "gpqa": 33.6,
    "scicode": 11.9,
    "ifbench": 24.8,
    "aime25": 6.0,
    "critpt": 0.0,
    "mmmu_pro": null,
    "livecodebench": 18.0,
    "sources": [
      "Artificial Analysis"
    ],
    "source_count": 1,
    "data_tier": 2,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.907770+00:00",
        "confidence": 1.0,
        "raw_name": "Granite 4.0 Micro",
        "raw_scores": {
          "intelligence_score": 11.09417230526621,
          "coding_score": 4.98,
          "blended_cost_per_1m": 0.0,
          "latency_seconds": 0.0,
          "tokens_per_second": 0.0,
          "context_window": 128000,
          "gdpval": 351.11751708249835,
          "terminalbench_hard": 0.015,
          "tau2": 0.126,
          "lcr": 4.0,
          "hle": 5.1,
          "gpqa": 33.6,
          "scicode": 11.9,
          "ifbench": 24.8,
          "aime25": 6.0,
          "critpt": 0.0,
          "livecodebench": 18.0
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 11.09417230526621,
        "coding_score": 4.98,
        "gdpval": 351.11751708249835,
        "terminalbench_hard": 0.015,
        "tau2": 0.126,
        "lcr": 4.0,
        "hle": 5.1,
        "gpqa": 33.6,
        "scicode": 11.9,
        "ifbench": 24.8,
        "aime25": 6.0,
        "critpt": 0.0,
        "livecodebench": 18.0
      }
    },
    "confidence_score": 1.0,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "granite-40-350m",
    "canonical_name": "Granite 4.0 350M",
    "model_name": "Granite 4.0 350M",
    "aliases": [
      "Granite 4.0 350M"
    ],
    "provider": null,
    "context_window": 32768,
    "open_source": true,
    "arena_votes": null,
    "license_type": "Apache 2.0",
    "creator": null,
    "intelligence_score": 8.8449,
    "coding_score": 0.31,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": null,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": 0.0,
    "latency_seconds": 0.0,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": 0.0,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": 335.491,
    "terminalbench_hard": 0.0,
    "tau2": 0.132,
    "lcr": 0.0,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": 5.7,
    "gpqa": 26.1,
    "scicode": 0.9,
    "ifbench": 15.9,
    "aime25": 0.0,
    "critpt": 0.0,
    "mmmu_pro": null,
    "livecodebench": 2.4,
    "sources": [
      "Artificial Analysis"
    ],
    "source_count": 1,
    "data_tier": 2,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.907812+00:00",
        "confidence": 1.0,
        "raw_name": "Granite 4.0 350M",
        "raw_scores": {
          "intelligence_score": 8.844934770956986,
          "coding_score": 0.31,
          "blended_cost_per_1m": 0.0,
          "latency_seconds": 0.0,
          "tokens_per_second": 0.0,
          "context_window": 32768,
          "gdpval": 335.4909636498867,
          "terminalbench_hard": 0.0,
          "tau2": 0.132,
          "lcr": 0.0,
          "hle": 5.7,
          "gpqa": 26.1,
          "scicode": 0.9,
          "ifbench": 15.9,
          "aime25": 0.0,
          "critpt": 0.0,
          "livecodebench": 2.4
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 8.844934770956986,
        "coding_score": 0.31,
        "gdpval": 335.4909636498867,
        "terminalbench_hard": 0.0,
        "tau2": 0.132,
        "lcr": 0.0,
        "hle": 5.7,
        "gpqa": 26.1,
        "scicode": 0.9,
        "ifbench": 15.9,
        "aime25": 0.0,
        "critpt": 0.0,
        "livecodebench": 2.4
      }
    },
    "confidence_score": 1.0,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "granite-40-h-small",
    "canonical_name": "Granite 4.0 H Small",
    "model_name": "Granite 4.0 H Small",
    "aliases": [
      "Granite 4.0 H Small"
    ],
    "provider": null,
    "context_window": 128000,
    "open_source": true,
    "arena_votes": null,
    "license_type": "Apache 2.0",
    "creator": null,
    "intelligence_score": 13.1866,
    "coding_score": 8.5,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": null,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": 0.11,
    "latency_seconds": 8.76,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": 480.0,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": 393.4524,
    "terminalbench_hard": 0.023,
    "tau2": 0.173,
    "lcr": 9.0,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": 3.7,
    "gpqa": 41.6,
    "scicode": 20.9,
    "ifbench": 31.5,
    "aime25": 13.7,
    "critpt": 0.0,
    "mmmu_pro": null,
    "livecodebench": 25.1,
    "sources": [
      "Artificial Analysis"
    ],
    "source_count": 1,
    "data_tier": 2,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.907855+00:00",
        "confidence": 1.0,
        "raw_name": "Granite 4.0 H Small",
        "raw_scores": {
          "intelligence_score": 13.186584050063225,
          "coding_score": 8.5,
          "blended_cost_per_1m": 0.11,
          "latency_seconds": 8.76,
          "tokens_per_second": 480.0,
          "context_window": 128000,
          "gdpval": 393.4523795257239,
          "terminalbench_hard": 0.023,
          "tau2": 0.173,
          "lcr": 9.0,
          "hle": 3.7,
          "gpqa": 41.6,
          "scicode": 20.9,
          "ifbench": 31.5,
          "aime25": 13.7,
          "critpt": 0.0,
          "livecodebench": 25.1
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 13.186584050063225,
        "coding_score": 8.5,
        "gdpval": 393.4523795257239,
        "terminalbench_hard": 0.023,
        "tau2": 0.173,
        "lcr": 9.0,
        "hle": 3.7,
        "gpqa": 41.6,
        "scicode": 20.9,
        "ifbench": 31.5,
        "aime25": 13.7,
        "critpt": 0.0,
        "livecodebench": 25.1
      }
    },
    "confidence_score": 1.0,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "granite-40-h-350m",
    "canonical_name": "Granite 4.0 H 350M",
    "model_name": "Granite 4.0 H 350M",
    "aliases": [
      "Granite 4.0 H 350M"
    ],
    "provider": null,
    "context_window": 32768,
    "open_source": true,
    "arena_votes": null,
    "license_type": "Apache 2.0",
    "creator": null,
    "intelligence_score": 8.97,
    "coding_score": 0.58,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": null,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": 0.0,
    "latency_seconds": 0.0,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": 0.0,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": 368.1393,
    "terminalbench_hard": 0.0,
    "tau2": 0.146,
    "lcr": 0.0,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": 6.4,
    "gpqa": 25.7,
    "scicode": 1.7,
    "ifbench": 17.6,
    "aime25": 1.3,
    "critpt": 0.0,
    "mmmu_pro": null,
    "livecodebench": 1.9,
    "sources": [
      "Artificial Analysis"
    ],
    "source_count": 1,
    "data_tier": 2,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.907896+00:00",
        "confidence": 1.0,
        "raw_name": "Granite 4.0 H 350M",
        "raw_scores": {
          "intelligence_score": 8.970038363283287,
          "coding_score": 0.58,
          "blended_cost_per_1m": 0.0,
          "latency_seconds": 0.0,
          "tokens_per_second": 0.0,
          "context_window": 32768,
          "gdpval": 368.1393050229142,
          "terminalbench_hard": 0.0,
          "tau2": 0.146,
          "lcr": 0.0,
          "hle": 6.4,
          "gpqa": 25.7,
          "scicode": 1.7,
          "ifbench": 17.6,
          "aime25": 1.3,
          "critpt": 0.0,
          "livecodebench": 1.9
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 8.970038363283287,
        "coding_score": 0.58,
        "gdpval": 368.1393050229142,
        "terminalbench_hard": 0.0,
        "tau2": 0.146,
        "lcr": 0.0,
        "hle": 6.4,
        "gpqa": 25.7,
        "scicode": 1.7,
        "ifbench": 17.6,
        "aime25": 1.3,
        "critpt": 0.0,
        "livecodebench": 1.9
      }
    },
    "confidence_score": 1.0,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "granite-40-h-1b",
    "canonical_name": "Granite 4.0 H 1B",
    "model_name": "Granite 4.0 H 1B",
    "aliases": [
      "Granite 4.0 H 1B"
    ],
    "provider": null,
    "context_window": 128000,
    "open_source": true,
    "arena_votes": null,
    "license_type": "Apache 2.0",
    "creator": null,
    "intelligence_score": 10.3858,
    "coding_score": 2.74,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": null,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": 0.0,
    "latency_seconds": 0.0,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": 0.0,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": 331.4278,
    "terminalbench_hard": 0.0,
    "tau2": 0.196,
    "lcr": 6.3,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": 5.0,
    "gpqa": 26.3,
    "scicode": 8.2,
    "ifbench": 26.2,
    "aime25": 6.3,
    "critpt": 0.0,
    "mmmu_pro": null,
    "livecodebench": 11.5,
    "sources": [
      "Artificial Analysis"
    ],
    "source_count": 1,
    "data_tier": 2,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.907939+00:00",
        "confidence": 1.0,
        "raw_name": "Granite 4.0 H 1B",
        "raw_scores": {
          "intelligence_score": 10.385752893854013,
          "coding_score": 2.74,
          "blended_cost_per_1m": 0.0,
          "latency_seconds": 0.0,
          "tokens_per_second": 0.0,
          "context_window": 128000,
          "gdpval": 331.4277590797194,
          "terminalbench_hard": 0.0,
          "tau2": 0.196,
          "lcr": 6.3,
          "hle": 5.0,
          "gpqa": 26.3,
          "scicode": 8.2,
          "ifbench": 26.2,
          "aime25": 6.3,
          "critpt": 0.0,
          "livecodebench": 11.5
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 10.385752893854013,
        "coding_score": 2.74,
        "gdpval": 331.4277590797194,
        "terminalbench_hard": 0.0,
        "tau2": 0.196,
        "lcr": 6.3,
        "hle": 5.0,
        "gpqa": 26.3,
        "scicode": 8.2,
        "ifbench": 26.2,
        "aime25": 6.3,
        "critpt": 0.0,
        "livecodebench": 11.5
      }
    },
    "confidence_score": 1.0,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "mercury-2",
    "canonical_name": "Mercury 2",
    "model_name": "Mercury 2",
    "aliases": [
      "Mercury 2"
    ],
    "provider": null,
    "context_window": 128000,
    "open_source": null,
    "arena_votes": null,
    "license_type": null,
    "creator": null,
    "intelligence_score": 32.82,
    "coding_score": 30.56,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": null,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": 0.38,
    "latency_seconds": 0.0,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": 0.0,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": 982.2256,
    "terminalbench_hard": 0.265,
    "tau2": 0.708,
    "lcr": 36.3,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": 15.5,
    "gpqa": 77.0,
    "scicode": 38.7,
    "ifbench": 69.8,
    "aime25": null,
    "critpt": 0.8,
    "mmmu_pro": null,
    "livecodebench": null,
    "sources": [
      "Artificial Analysis"
    ],
    "source_count": 1,
    "data_tier": 2,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.907976+00:00",
        "confidence": 0.8600000000000001,
        "raw_name": "Mercury 2",
        "raw_scores": {
          "intelligence_score": 32.82,
          "coding_score": 30.56,
          "blended_cost_per_1m": 0.38,
          "latency_seconds": 0.0,
          "tokens_per_second": 0.0,
          "context_window": 128000,
          "gdpval": 982.2256210155097,
          "terminalbench_hard": 0.265,
          "tau2": 0.708,
          "lcr": 36.3,
          "hle": 15.5,
          "gpqa": 77.0,
          "scicode": 38.7,
          "ifbench": 69.8,
          "critpt": 0.8
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 32.82,
        "coding_score": 30.56,
        "gdpval": 982.2256210155097,
        "terminalbench_hard": 0.265,
        "tau2": 0.708,
        "lcr": 36.3,
        "hle": 15.5,
        "gpqa": 77.0,
        "scicode": 38.7,
        "ifbench": 69.8,
        "critpt": 0.8
      }
    },
    "confidence_score": 0.86,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "reka-flash-3",
    "canonical_name": "Reka Flash 3",
    "model_name": "Reka Flash 3",
    "aliases": [
      "Reka Flash 3"
    ],
    "provider": null,
    "context_window": 128000,
    "open_source": true,
    "arena_votes": null,
    "license_type": "Apache 2.0",
    "creator": null,
    "intelligence_score": 14.3498,
    "coding_score": 8.91,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": null,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": 0.35,
    "latency_seconds": 1.34,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": 49.0,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": 0.0,
    "tau2": 0.0,
    "lcr": 0.0,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": 5.1,
    "gpqa": 52.9,
    "scicode": 26.7,
    "ifbench": 30.4,
    "aime25": 33.7,
    "critpt": 0.0,
    "mmmu_pro": null,
    "livecodebench": 43.5,
    "sources": [
      "Artificial Analysis"
    ],
    "source_count": 1,
    "data_tier": 2,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.908016+00:00",
        "confidence": 1.0,
        "raw_name": "Reka Flash 3",
        "raw_scores": {
          "intelligence_score": 14.349784905144217,
          "coding_score": 8.91,
          "blended_cost_per_1m": 0.35,
          "latency_seconds": 1.34,
          "tokens_per_second": 49.0,
          "context_window": 128000,
          "terminalbench_hard": 0.0,
          "tau2": 0.0,
          "lcr": 0.0,
          "hle": 5.1,
          "gpqa": 52.9,
          "scicode": 26.7,
          "ifbench": 30.4,
          "aime25": 33.7,
          "critpt": 0.0,
          "livecodebench": 43.5
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 14.349784905144217,
        "coding_score": 8.91,
        "terminalbench_hard": 0.0,
        "tau2": 0.0,
        "lcr": 0.0,
        "hle": 5.1,
        "gpqa": 52.9,
        "scicode": 26.7,
        "ifbench": 30.4,
        "aime25": 33.7,
        "critpt": 0.0,
        "livecodebench": 43.5
      }
    },
    "confidence_score": 1.0,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "hermes-4---llama-31-405b",
    "canonical_name": "Hermes 4 - Llama-3.1 405B (Reasoning)",
    "model_name": "Hermes 4 - Llama-3.1 405B (Reasoning)",
    "aliases": [
      "Hermes 4 - Llama-3.1 405B (Non-reasoning)",
      "Hermes 4 - Llama-3.1 405B (Reasoning)"
    ],
    "provider": "Meta",
    "context_window": 128000,
    "open_source": true,
    "arena_votes": null,
    "license_type": "META LLAMA 3 COMMUNITY LICENSE AGREEMENT",
    "creator": null,
    "intelligence_score": 19.4245,
    "coding_score": 17.045,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": null,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": 587.4721,
    "terminalbench_hard": 0.106,
    "tau2": 0.244,
    "lcr": 20.35,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": 7.25,
    "gpqa": 63.15,
    "scicode": 29.9,
    "ifbench": 33.75,
    "aime25": 42.5,
    "critpt": 0.15,
    "mmmu_pro": null,
    "livecodebench": 61.6,
    "sources": [
      "Artificial Analysis"
    ],
    "source_count": 1,
    "data_tier": 2,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.908057+00:00",
        "confidence": 0.79,
        "raw_name": "Hermes 4 - Llama-3.1 405B (Reasoning)",
        "raw_scores": {
          "intelligence_score": 21.724919221240167,
          "coding_score": 15.99,
          "context_window": 128000,
          "gdpval": 621.2865290592075,
          "terminalbench_hard": 0.114,
          "tau2": 0.222,
          "lcr": 20.7,
          "hle": 10.3,
          "gpqa": 72.7,
          "scicode": 25.2,
          "ifbench": 32.7,
          "aime25": 69.7,
          "critpt": 0.3,
          "livecodebench": 68.6
        }
      },
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.908179+00:00",
        "confidence": 0.79,
        "raw_name": "Hermes 4 - Llama-3.1 405B (Non-reasoning)",
        "raw_scores": {
          "intelligence_score": 17.124153510875814,
          "coding_score": 18.1,
          "context_window": 128000,
          "gdpval": 553.6576105995117,
          "terminalbench_hard": 0.098,
          "tau2": 0.266,
          "lcr": 20.0,
          "hle": 4.2,
          "gpqa": 53.6,
          "scicode": 34.6,
          "ifbench": 34.8,
          "aime25": 15.3,
          "critpt": 0.0,
          "livecodebench": 54.6
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 21.724919221240167,
        "coding_score": 15.99,
        "gdpval": 621.2865290592075,
        "terminalbench_hard": 0.114,
        "tau2": 0.222,
        "lcr": 20.7,
        "hle": 10.3,
        "gpqa": 72.7,
        "scicode": 25.2,
        "ifbench": 32.7,
        "aime25": 69.7,
        "critpt": 0.3,
        "livecodebench": 68.6
      }
    },
    "confidence_score": 0.79,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "hermes-4---llama-31-70b",
    "canonical_name": "Hermes 4 - Llama-3.1 70B (Reasoning)",
    "model_name": "Hermes 4 - Llama-3.1 70B (Reasoning)",
    "aliases": [
      "Hermes 4 - Llama-3.1 70B (Non-reasoning)",
      "Hermes 4 - Llama-3.1 70B (Reasoning)"
    ],
    "provider": "Meta",
    "context_window": 128000,
    "open_source": true,
    "arena_votes": null,
    "license_type": "META LLAMA 3 COMMUNITY LICENSE AGREEMENT",
    "creator": null,
    "intelligence_score": 16.9689,
    "coding_score": 11.815,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": null,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": 580.159,
    "terminalbench_hard": 0.0225,
    "tau2": 0.2205,
    "lcr": 4.35,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": 5.75,
    "gpqa": 59.5,
    "scicode": 30.9,
    "ifbench": 30.15,
    "aime25": 40.0,
    "critpt": 0.0,
    "mmmu_pro": null,
    "livecodebench": 46.1,
    "sources": [
      "Artificial Analysis"
    ],
    "source_count": 1,
    "data_tier": 2,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.908098+00:00",
        "confidence": 0.79,
        "raw_name": "Hermes 4 - Llama-3.1 70B (Reasoning)",
        "raw_scores": {
          "intelligence_score": 20.386235707526104,
          "coding_score": 14.41,
          "context_window": 128000,
          "gdpval": 584.0691961943945,
          "terminalbench_hard": 0.045,
          "tau2": 0.225,
          "lcr": 6.7,
          "hle": 7.9,
          "gpqa": 69.9,
          "scicode": 34.1,
          "ifbench": 31.3,
          "aime25": 68.7,
          "critpt": 0.0,
          "livecodebench": 65.3
        }
      },
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.908138+00:00",
        "confidence": 0.79,
        "raw_name": "Hermes 4 - Llama-3.1 70B (Non-reasoning)",
        "raw_scores": {
          "intelligence_score": 13.551468975216904,
          "coding_score": 9.22,
          "context_window": 128000,
          "gdpval": 576.2488432528579,
          "terminalbench_hard": 0.0,
          "tau2": 0.216,
          "lcr": 2.0,
          "hle": 3.6,
          "gpqa": 49.1,
          "scicode": 27.7,
          "ifbench": 29.0,
          "aime25": 11.3,
          "critpt": 0.0,
          "livecodebench": 26.9
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 20.386235707526104,
        "coding_score": 14.41,
        "gdpval": 584.0691961943945,
        "terminalbench_hard": 0.045,
        "tau2": 0.225,
        "lcr": 6.7,
        "hle": 7.9,
        "gpqa": 69.9,
        "scicode": 34.1,
        "ifbench": 31.3,
        "aime25": 68.7,
        "critpt": 0.0,
        "livecodebench": 65.3
      }
    },
    "confidence_score": 0.79,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "deephermes-3---llama-31-8b-preview",
    "canonical_name": "DeepHermes 3 - Llama-3.1 8B Preview (Non-reasoning)",
    "model_name": "DeepHermes 3 - Llama-3.1 8B Preview (Non-reasoning)",
    "aliases": [
      "DeepHermes 3 - Llama-3.1 8B Preview (Non-reasoning)"
    ],
    "provider": "Meta",
    "context_window": 128000,
    "open_source": true,
    "arena_votes": null,
    "license_type": "LLAMA 3.1 COMMUNITY LICENSE AGREEMENT",
    "creator": null,
    "intelligence_score": 7.5781,
    "coding_score": null,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": null,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": null,
    "tau2": null,
    "lcr": null,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": 4.3,
    "gpqa": 27.0,
    "scicode": 9.1,
    "ifbench": null,
    "aime25": null,
    "critpt": null,
    "mmmu_pro": null,
    "livecodebench": 8.5,
    "sources": [
      "Artificial Analysis"
    ],
    "source_count": 1,
    "data_tier": 2,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.908222+00:00",
        "confidence": 0.65,
        "raw_name": "DeepHermes 3 - Llama-3.1 8B Preview (Non-reasoning)",
        "raw_scores": {
          "intelligence_score": 7.578083680871105,
          "context_window": 128000,
          "hle": 4.3,
          "gpqa": 27.0,
          "scicode": 9.1,
          "livecodebench": 8.5
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 7.578083680871105,
        "hle": 4.3,
        "gpqa": 27.0,
        "scicode": 9.1,
        "livecodebench": 8.5
      }
    },
    "confidence_score": 0.65,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "deephermes-3---mistral-24b-preview",
    "canonical_name": "DeepHermes 3 - Mistral 24B Preview (Non-reasoning)",
    "model_name": "DeepHermes 3 - Mistral 24B Preview (Non-reasoning)",
    "aliases": [
      "DeepHermes 3 - Mistral 24B Preview (Non-reasoning)"
    ],
    "provider": "Mistral AI",
    "context_window": 32000,
    "open_source": true,
    "arena_votes": null,
    "license_type": "Apache 2.0",
    "creator": null,
    "intelligence_score": 10.8883,
    "coding_score": null,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": null,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": null,
    "tau2": null,
    "lcr": null,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": 3.9,
    "gpqa": 38.2,
    "scicode": 22.8,
    "ifbench": null,
    "aime25": null,
    "critpt": null,
    "mmmu_pro": null,
    "livecodebench": 19.5,
    "sources": [
      "Artificial Analysis"
    ],
    "source_count": 1,
    "data_tier": 2,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.908252+00:00",
        "confidence": 0.65,
        "raw_name": "DeepHermes 3 - Mistral 24B Preview (Non-reasoning)",
        "raw_scores": {
          "intelligence_score": 10.888270503865678,
          "context_window": 32000,
          "hle": 3.9,
          "gpqa": 38.2,
          "scicode": 22.8,
          "livecodebench": 19.5
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 10.888270503865678,
        "hle": 3.9,
        "gpqa": 38.2,
        "scicode": 22.8,
        "livecodebench": 19.5
      }
    },
    "confidence_score": 0.65,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "k-exaone",
    "canonical_name": "K-EXAONE (Reasoning)",
    "model_name": "K-EXAONE (Reasoning)",
    "aliases": [
      "K-EXAONE (Non-reasoning)",
      "K-EXAONE (Reasoning)"
    ],
    "provider": null,
    "context_window": 256000,
    "open_source": true,
    "arena_votes": null,
    "license_type": "K Exaone",
    "creator": null,
    "intelligence_score": 27.9669,
    "coding_score": 20.5929,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": null,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": 844.003,
    "terminalbench_hard": 0.1512,
    "tau2": 0.6705,
    "lcr": 51.5517,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": 9.4285,
    "gpqa": 74.104,
    "scicode": 31.4993,
    "ifbench": 52.7318,
    "aime25": 68.2232,
    "critpt": 0.5755,
    "mmmu_pro": null,
    "livecodebench": 76.8,
    "sources": [
      "Artificial Analysis"
    ],
    "source_count": 1,
    "data_tier": 2,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.908291+00:00",
        "confidence": 0.79,
        "raw_name": "K-EXAONE (Reasoning)",
        "raw_scores": {
          "intelligence_score": 32.12,
          "coding_score": 27.03,
          "context_window": 256000,
          "gdpval": 867.4114063431075,
          "terminalbench_hard": 0.227,
          "tau2": 0.743,
          "lcr": 55.7,
          "hle": 13.1,
          "gpqa": 78.3,
          "scicode": 35.6,
          "ifbench": 64.7,
          "aime25": 90.3,
          "critpt": 1.1,
          "livecodebench": 76.8
        }
      },
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.908328+00:00",
        "confidence": 0.72,
        "raw_name": "K-EXAONE (Non-reasoning)",
        "raw_scores": {
          "intelligence_score": 23.41,
          "coding_score": 13.53,
          "context_window": 256000,
          "gdpval": 818.3188659250966,
          "terminalbench_hard": 0.068,
          "tau2": 0.591,
          "lcr": 47.0,
          "hle": 5.4,
          "gpqa": 69.5,
          "scicode": 27.0,
          "ifbench": 39.6,
          "aime25": 44.0,
          "critpt": 0.0
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 32.12,
        "coding_score": 27.03,
        "gdpval": 867.4114063431075,
        "terminalbench_hard": 0.227,
        "tau2": 0.743,
        "lcr": 55.7,
        "hle": 13.1,
        "gpqa": 78.3,
        "scicode": 35.6,
        "ifbench": 64.7,
        "aime25": 90.3,
        "critpt": 1.1,
        "livecodebench": 76.8
      }
    },
    "confidence_score": 0.755,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "exaone-40-12b",
    "canonical_name": "Exaone 4.0 1.2B (Non-reasoning)",
    "model_name": "Exaone 4.0 1.2B (Non-reasoning)",
    "aliases": [
      "Exaone 4.0 1.2B (Non-reasoning)",
      "Exaone 4.0 1.2B (Reasoning)"
    ],
    "provider": null,
    "context_window": 64000,
    "open_source": false,
    "arena_votes": null,
    "license_type": "EXAONE AI Model License Agreement 1.2 - NC",
    "creator": null,
    "intelligence_score": 13.5306,
    "coding_score": 2.78,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": null,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": 358.7117,
    "terminalbench_hard": 0.0,
    "tau2": 0.1845,
    "lcr": 0.0,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": 5.8,
    "gpqa": 46.95,
    "scicode": 8.35,
    "ifbench": 24.15,
    "aime25": 37.15,
    "critpt": 0.0,
    "mmmu_pro": null,
    "livecodebench": 40.45,
    "sources": [
      "Artificial Analysis"
    ],
    "source_count": 1,
    "data_tier": 2,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.908369+00:00",
        "confidence": 0.79,
        "raw_name": "Exaone 4.0 1.2B (Non-reasoning)",
        "raw_scores": {
          "intelligence_score": 12.423315047214832,
          "coding_score": 2.47,
          "context_window": 64000,
          "gdpval": 358.73620283242406,
          "terminalbench_hard": 0.0,
          "tau2": 0.205,
          "lcr": 0.0,
          "hle": 5.8,
          "gpqa": 42.4,
          "scicode": 7.4,
          "ifbench": 25.3,
          "aime25": 24.0,
          "critpt": 0.0,
          "livecodebench": 29.3
        }
      },
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.908410+00:00",
        "confidence": 0.79,
        "raw_name": "Exaone 4.0 1.2B (Reasoning)",
        "raw_scores": {
          "intelligence_score": 14.637839944967833,
          "coding_score": 3.09,
          "context_window": 64000,
          "gdpval": 358.6872665017387,
          "terminalbench_hard": 0.0,
          "tau2": 0.164,
          "lcr": 0.0,
          "hle": 5.8,
          "gpqa": 51.5,
          "scicode": 9.3,
          "ifbench": 23.0,
          "aime25": 50.3,
          "critpt": 0.0,
          "livecodebench": 51.6
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 12.423315047214832,
        "coding_score": 2.47,
        "gdpval": 358.73620283242406,
        "terminalbench_hard": 0.0,
        "tau2": 0.205,
        "lcr": 0.0,
        "hle": 5.8,
        "gpqa": 42.4,
        "scicode": 7.4,
        "ifbench": 25.3,
        "aime25": 24.0,
        "critpt": 0.0,
        "livecodebench": 29.3
      }
    },
    "confidence_score": 0.79,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "exaone-40-32b",
    "canonical_name": "EXAONE 4.0 32B (Non-reasoning)",
    "model_name": "EXAONE 4.0 32B (Non-reasoning)",
    "aliases": [
      "EXAONE 4.0 32B (Non-reasoning)",
      "EXAONE 4.0 32B (Reasoning)"
    ],
    "provider": null,
    "context_window": 131000,
    "open_source": false,
    "arena_votes": null,
    "license_type": "EXAONE AI Model License Agreement 1.2 - NC",
    "creator": null,
    "intelligence_score": 19.2392,
    "coding_score": 11.7,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": null,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": 471.6465,
    "terminalbench_hard": 0.0265,
    "tau2": 0.107,
    "lcr": 11.0,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": 7.7,
    "gpqa": 68.35,
    "scicode": 29.8,
    "ifbench": 34.9,
    "aime25": 59.65,
    "critpt": 0.0,
    "mmmu_pro": null,
    "livecodebench": 60.95,
    "sources": [
      "Artificial Analysis"
    ],
    "source_count": 1,
    "data_tier": 2,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.908449+00:00",
        "confidence": 0.79,
        "raw_name": "EXAONE 4.0 32B (Non-reasoning)",
        "raw_scores": {
          "intelligence_score": 16.128694503015343,
          "coding_score": 9.42,
          "context_window": 131000,
          "gdpval": 387.09060825810616,
          "terminalbench_hard": 0.015,
          "tau2": 0.041,
          "lcr": 8.0,
          "hle": 4.9,
          "gpqa": 62.8,
          "scicode": 25.2,
          "ifbench": 33.5,
          "aime25": 39.3,
          "critpt": 0.0,
          "livecodebench": 47.2
        }
      },
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.908490+00:00",
        "confidence": 0.79,
        "raw_name": "EXAONE 4.0 32B (Reasoning)",
        "raw_scores": {
          "intelligence_score": 22.349745923183455,
          "coding_score": 13.98,
          "context_window": 131000,
          "gdpval": 556.2024299357337,
          "terminalbench_hard": 0.038,
          "tau2": 0.173,
          "lcr": 14.0,
          "hle": 10.5,
          "gpqa": 73.9,
          "scicode": 34.4,
          "ifbench": 36.3,
          "aime25": 80.0,
          "critpt": 0.0,
          "livecodebench": 74.7
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 16.128694503015343,
        "coding_score": 9.42,
        "gdpval": 387.09060825810616,
        "terminalbench_hard": 0.015,
        "tau2": 0.041,
        "lcr": 8.0,
        "hle": 4.9,
        "gpqa": 62.8,
        "scicode": 25.2,
        "ifbench": 33.5,
        "aime25": 39.3,
        "critpt": 0.0,
        "livecodebench": 47.2
      }
    },
    "confidence_score": 0.79,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "mimo-v2-flash",
    "canonical_name": "MiMo-V2-Flash (Non-reasoning)",
    "model_name": "MiMo-V2-Flash (Non-reasoning)",
    "aliases": [
      "MiMo-V2-Flash (Feb 2026)",
      "MiMo-V2-Flash (Non-reasoning)",
      "MiMo-V2-Flash (Reasoning)",
      "mimo-v2-flash (non-thinking)",
      "mimo-v2-flash (thinking)"
    ],
    "provider": "Xiaomi",
    "context_window": 256000,
    "open_source": true,
    "arena_votes": 19661,
    "license_type": "MIT",
    "creator": null,
    "intelligence_score": 37.1441,
    "coding_score": 30.4527,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 1388.7,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": 0.15,
    "latency_seconds": 1.23,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": 154.0,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": 1075.2605,
    "terminalbench_hard": 0.2838,
    "tau2": 0.9081,
    "lcr": 53.1947,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": 16.4709,
    "gpqa": 78.0607,
    "scicode": 34.6414,
    "ifbench": 59.0111,
    "aime25": 82.0,
    "critpt": 2.4143,
    "mmmu_pro": null,
    "livecodebench": 63.5,
    "sources": [
      "Artificial Analysis",
      "LMSYS Chatbot Arena"
    ],
    "source_count": 2,
    "data_tier": 1,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.908530+00:00",
        "confidence": 0.79,
        "raw_name": "MiMo-V2-Flash (Non-reasoning)",
        "raw_scores": {
          "intelligence_score": 30.35,
          "coding_score": 25.81,
          "context_window": 256000,
          "gdpval": 1085.6435252122733,
          "terminalbench_hard": 0.258,
          "tau2": 0.839,
          "lcr": 31.3,
          "hle": 8.0,
          "gpqa": 65.6,
          "scicode": 25.9,
          "ifbench": 39.9,
          "aime25": 67.7,
          "critpt": 0.0,
          "livecodebench": 40.2
        }
      },
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.908567+00:00",
        "confidence": 0.8600000000000001,
        "raw_name": "MiMo-V2-Flash (Feb 2026)",
        "raw_scores": {
          "intelligence_score": 41.46,
          "coding_score": 33.48,
          "blended_cost_per_1m": 0.15,
          "latency_seconds": 1.23,
          "tokens_per_second": 154.0,
          "context_window": 256000,
          "gdpval": 1032.2411901179282,
          "terminalbench_hard": 0.311,
          "tau2": 0.933,
          "lcr": 64.3,
          "hle": 20.0,
          "gpqa": 83.5,
          "scicode": 38.3,
          "ifbench": 71.8,
          "critpt": 2.9
        }
      },
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.917639+00:00",
        "confidence": 0.79,
        "raw_name": "MiMo-V2-Flash (Reasoning)",
        "raw_scores": {
          "intelligence_score": 39.24,
          "coding_score": 31.8,
          "context_window": 256000,
          "gdpval": 1111.708745572593,
          "terminalbench_hard": 0.28,
          "tau2": 0.95,
          "lcr": 63.0,
          "hle": 21.1,
          "gpqa": 84.6,
          "scicode": 39.4,
          "ifbench": 64.2,
          "aime25": 96.3,
          "critpt": 4.3,
          "livecodebench": 86.8
        }
      },
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.892410+00:00",
        "confidence": 1.0,
        "raw_name": "mimo-v2-flash (non-thinking)",
        "raw_scores": {
          "arena_elo": 1390.49,
          "arena_votes": 19661
        }
      },
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.892601+00:00",
        "confidence": 1.0,
        "raw_name": "mimo-v2-flash (thinking)",
        "raw_scores": {
          "arena_elo": 1386.91,
          "arena_votes": 10870
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 30.35,
        "coding_score": 25.81,
        "gdpval": 1085.6435252122733,
        "terminalbench_hard": 0.258,
        "tau2": 0.839,
        "lcr": 31.3,
        "hle": 8.0,
        "gpqa": 65.6,
        "scicode": 25.9,
        "ifbench": 39.9,
        "aime25": 67.7,
        "critpt": 0.0,
        "livecodebench": 40.2
      },
      "LMSYS Chatbot Arena": {
        "arena_elo": 1390.49
      }
    },
    "confidence_score": 0.888,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "ernie-45-300b-a47b",
    "canonical_name": "ERNIE 4.5 300B A47B",
    "model_name": "ERNIE 4.5 300B A47B",
    "aliases": [
      "ERNIE 4.5 300B A47B"
    ],
    "provider": null,
    "context_window": 131072,
    "open_source": true,
    "arena_votes": null,
    "license_type": "Apache 2.0",
    "creator": null,
    "intelligence_score": 17.2563,
    "coding_score": 14.53,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": null,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": 0.48,
    "latency_seconds": 1.98,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": 20.0,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": 0.061,
    "tau2": 0.0,
    "lcr": 2.3,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": 3.5,
    "gpqa": 81.1,
    "scicode": 31.5,
    "ifbench": 39.1,
    "aime25": 41.3,
    "critpt": 0.0,
    "mmmu_pro": null,
    "livecodebench": 46.7,
    "sources": [
      "Artificial Analysis"
    ],
    "source_count": 1,
    "data_tier": 2,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.908609+00:00",
        "confidence": 1.0,
        "raw_name": "ERNIE 4.5 300B A47B",
        "raw_scores": {
          "intelligence_score": 17.256259598074,
          "coding_score": 14.53,
          "blended_cost_per_1m": 0.48,
          "latency_seconds": 1.98,
          "tokens_per_second": 20.0,
          "context_window": 131072,
          "terminalbench_hard": 0.061,
          "tau2": 0.0,
          "lcr": 2.3,
          "hle": 3.5,
          "gpqa": 81.1,
          "scicode": 31.5,
          "ifbench": 39.1,
          "aime25": 41.3,
          "critpt": 0.0,
          "livecodebench": 46.7
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 17.256259598074,
        "coding_score": 14.53,
        "terminalbench_hard": 0.061,
        "tau2": 0.0,
        "lcr": 2.3,
        "hle": 3.5,
        "gpqa": 81.1,
        "scicode": 31.5,
        "ifbench": 39.1,
        "aime25": 41.3,
        "critpt": 0.0,
        "livecodebench": 46.7
      }
    },
    "confidence_score": 1.0,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "ernie-50-thinking-preview",
    "canonical_name": "ERNIE 5.0 Thinking Preview",
    "model_name": "ERNIE 5.0 Thinking Preview",
    "aliases": [
      "ERNIE 5.0 Thinking Preview"
    ],
    "provider": null,
    "context_window": 128000,
    "open_source": null,
    "arena_votes": null,
    "license_type": null,
    "creator": null,
    "intelligence_score": 29.09,
    "coding_score": 29.17,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": null,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": 0.0,
    "latency_seconds": 0.0,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": 0.0,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": 824.3648,
    "terminalbench_hard": 0.25,
    "tau2": 0.839,
    "lcr": 6.7,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": 12.7,
    "gpqa": 77.7,
    "scicode": 37.5,
    "ifbench": 41.4,
    "aime25": 85.0,
    "critpt": 1.4,
    "mmmu_pro": 64.6,
    "livecodebench": 81.2,
    "sources": [
      "Artificial Analysis"
    ],
    "source_count": 1,
    "data_tier": 2,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.908654+00:00",
        "confidence": 1.0,
        "raw_name": "ERNIE 5.0 Thinking Preview",
        "raw_scores": {
          "intelligence_score": 29.09,
          "coding_score": 29.17,
          "blended_cost_per_1m": 0.0,
          "latency_seconds": 0.0,
          "tokens_per_second": 0.0,
          "context_window": 128000,
          "gdpval": 824.3647869034141,
          "terminalbench_hard": 0.25,
          "tau2": 0.839,
          "lcr": 6.7,
          "hle": 12.7,
          "gpqa": 77.7,
          "scicode": 37.5,
          "ifbench": 41.4,
          "aime25": 85.0,
          "critpt": 1.4,
          "mmmu_pro": 64.6,
          "livecodebench": 81.2
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 29.09,
        "coding_score": 29.17,
        "gdpval": 824.3647869034141,
        "terminalbench_hard": 0.25,
        "tau2": 0.839,
        "lcr": 6.7,
        "hle": 12.7,
        "gpqa": 77.7,
        "scicode": 37.5,
        "ifbench": 41.4,
        "aime25": 85.0,
        "critpt": 1.4,
        "mmmu_pro": 64.6,
        "livecodebench": 81.2
      }
    },
    "confidence_score": 1.0,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "cogito-v21",
    "canonical_name": "Cogito v2.1 (Reasoning)",
    "model_name": "Cogito v2.1 (Reasoning)",
    "aliases": [
      "Cogito v2.1 (Reasoning)"
    ],
    "provider": null,
    "context_window": 128000,
    "open_source": true,
    "arena_votes": null,
    "license_type": "MIT",
    "creator": null,
    "intelligence_score": null,
    "coding_score": 24.77,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": null,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": 0.167,
    "tau2": null,
    "lcr": 21.7,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": 11.0,
    "gpqa": 76.8,
    "scicode": 41.0,
    "ifbench": 46.3,
    "aime25": 72.7,
    "critpt": 0.0,
    "mmmu_pro": null,
    "livecodebench": 68.8,
    "sources": [
      "Artificial Analysis"
    ],
    "source_count": 1,
    "data_tier": 2,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.908688+00:00",
        "confidence": 0.72,
        "raw_name": "Cogito v2.1 (Reasoning)",
        "raw_scores": {
          "coding_score": 24.77,
          "context_window": 128000,
          "terminalbench_hard": 0.167,
          "lcr": 21.7,
          "hle": 11.0,
          "gpqa": 76.8,
          "scicode": 41.0,
          "ifbench": 46.3,
          "aime25": 72.7,
          "critpt": 0.0,
          "livecodebench": 68.8
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "coding_score": 24.77,
        "terminalbench_hard": 0.167,
        "lcr": 21.7,
        "hle": 11.0,
        "gpqa": 76.8,
        "scicode": 41.0,
        "ifbench": 46.3,
        "aime25": 72.7,
        "critpt": 0.0,
        "livecodebench": 68.8
      }
    },
    "confidence_score": 0.72,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "llama-65b",
    "canonical_name": "Llama 65B",
    "model_name": "Llama 65B",
    "aliases": [
      "Llama 65B"
    ],
    "provider": "Meta",
    "context_window": 2048,
    "open_source": null,
    "arena_votes": null,
    "license_type": null,
    "creator": null,
    "intelligence_score": 7.4139,
    "coding_score": null,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": null,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": 0.0,
    "latency_seconds": 0.0,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": 0.0,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": null,
    "tau2": null,
    "lcr": null,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": null,
    "gpqa": null,
    "scicode": null,
    "ifbench": null,
    "aime25": null,
    "critpt": null,
    "mmmu_pro": null,
    "livecodebench": null,
    "sources": [
      "Artificial Analysis"
    ],
    "source_count": 1,
    "data_tier": 2,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.908711+00:00",
        "confidence": 0.65,
        "raw_name": "Llama 65B",
        "raw_scores": {
          "intelligence_score": 7.413887665229345,
          "blended_cost_per_1m": 0.0,
          "latency_seconds": 0.0,
          "tokens_per_second": 0.0,
          "context_window": 2048
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 7.413887665229345
      }
    },
    "confidence_score": 0.65,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "kat-coder-pro-v1",
    "canonical_name": "KAT-Coder-Pro V1",
    "model_name": "KAT-Coder-Pro V1",
    "aliases": [
      "KAT-Coder-Pro V1"
    ],
    "provider": null,
    "context_window": 256000,
    "open_source": null,
    "arena_votes": null,
    "license_type": null,
    "creator": null,
    "intelligence_score": 36.03,
    "coding_score": 18.25,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": null,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": 0.53,
    "latency_seconds": 1.81,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": 55.0,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": 839.5761,
    "terminalbench_hard": 0.091,
    "tau2": 0.886,
    "lcr": 74.0,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": 33.4,
    "gpqa": 76.4,
    "scicode": 36.6,
    "ifbench": 68.4,
    "aime25": 94.7,
    "critpt": 0.0,
    "mmmu_pro": null,
    "livecodebench": 74.7,
    "sources": [
      "Artificial Analysis"
    ],
    "source_count": 1,
    "data_tier": 2,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.908753+00:00",
        "confidence": 1.0,
        "raw_name": "KAT-Coder-Pro V1",
        "raw_scores": {
          "intelligence_score": 36.03,
          "coding_score": 18.25,
          "blended_cost_per_1m": 0.53,
          "latency_seconds": 1.81,
          "tokens_per_second": 55.0,
          "context_window": 256000,
          "gdpval": 839.5761111532631,
          "terminalbench_hard": 0.091,
          "tau2": 0.886,
          "lcr": 74.0,
          "hle": 33.4,
          "gpqa": 76.4,
          "scicode": 36.6,
          "ifbench": 68.4,
          "aime25": 94.7,
          "critpt": 0.0,
          "livecodebench": 74.7
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 36.03,
        "coding_score": 18.25,
        "gdpval": 839.5761111532631,
        "terminalbench_hard": 0.091,
        "tau2": 0.886,
        "lcr": 74.0,
        "hle": 33.4,
        "gpqa": 76.4,
        "scicode": 36.6,
        "ifbench": 68.4,
        "aime25": 94.7,
        "critpt": 0.0,
        "livecodebench": 74.7
      }
    },
    "confidence_score": 1.0,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "intellect-3",
    "canonical_name": "INTELLECT-3",
    "model_name": "INTELLECT-3",
    "aliases": [
      "INTELLECT-3",
      "intellect-3"
    ],
    "provider": "Prime Intellect",
    "context_window": 131100,
    "open_source": true,
    "arena_votes": 5290,
    "license_type": "MIT",
    "creator": null,
    "intelligence_score": 23.7921,
    "coding_score": 19.1,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 1356.04,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": 0.0,
    "latency_seconds": 0.0,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": 0.0,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": 797.6237,
    "terminalbench_hard": 0.091,
    "tau2": 0.266,
    "lcr": 32.3,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": 12.1,
    "gpqa": 76.1,
    "scicode": 39.1,
    "ifbench": 34.0,
    "aime25": 88.0,
    "critpt": 0.3,
    "mmmu_pro": null,
    "livecodebench": 77.7,
    "sources": [
      "Artificial Analysis",
      "LMSYS Chatbot Arena"
    ],
    "source_count": 2,
    "data_tier": 1,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.908796+00:00",
        "confidence": 1.0,
        "raw_name": "INTELLECT-3",
        "raw_scores": {
          "intelligence_score": 23.792132359711744,
          "coding_score": 19.1,
          "blended_cost_per_1m": 0.0,
          "latency_seconds": 0.0,
          "tokens_per_second": 0.0,
          "context_window": 131100,
          "gdpval": 797.6237331419795,
          "terminalbench_hard": 0.091,
          "tau2": 0.266,
          "lcr": 32.3,
          "hle": 12.1,
          "gpqa": 76.1,
          "scicode": 39.1,
          "ifbench": 34.0,
          "aime25": 88.0,
          "critpt": 0.3,
          "livecodebench": 77.7
        }
      },
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.892953+00:00",
        "confidence": 1.0,
        "raw_name": "intellect-3",
        "raw_scores": {
          "arena_elo": 1356.04,
          "arena_votes": 5290
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 23.792132359711744,
        "coding_score": 19.1,
        "gdpval": 797.6237331419795,
        "terminalbench_hard": 0.091,
        "tau2": 0.266,
        "lcr": 32.3,
        "hle": 12.1,
        "gpqa": 76.1,
        "scicode": 39.1,
        "ifbench": 34.0,
        "aime25": 88.0,
        "critpt": 0.3,
        "livecodebench": 77.7
      },
      "LMSYS Chatbot Arena": {
        "arena_elo": 1356.04
      }
    },
    "confidence_score": 1.0,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "motif-2-127b-reasoning",
    "canonical_name": "Motif-2-12.7B-Reasoning",
    "model_name": "Motif-2-12.7B-Reasoning",
    "aliases": [
      "Motif-2-12.7B-Reasoning"
    ],
    "provider": null,
    "context_window": 128000,
    "open_source": null,
    "arena_votes": null,
    "license_type": null,
    "creator": null,
    "intelligence_score": 23.8553,
    "coding_score": 11.94,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": null,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": 534.1414,
    "terminalbench_hard": 0.038,
    "tau2": 0.465,
    "lcr": 13.0,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": 8.2,
    "gpqa": 69.5,
    "scicode": 28.2,
    "ifbench": 57.0,
    "aime25": 80.3,
    "critpt": 0.0,
    "mmmu_pro": null,
    "livecodebench": 65.1,
    "sources": [
      "Artificial Analysis"
    ],
    "source_count": 1,
    "data_tier": 2,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.908837+00:00",
        "confidence": 0.79,
        "raw_name": "Motif-2-12.7B-Reasoning",
        "raw_scores": {
          "intelligence_score": 23.855265124432417,
          "coding_score": 11.94,
          "context_window": 128000,
          "gdpval": 534.1413742729939,
          "terminalbench_hard": 0.038,
          "tau2": 0.465,
          "lcr": 13.0,
          "hle": 8.2,
          "gpqa": 69.5,
          "scicode": 28.2,
          "ifbench": 57.0,
          "aime25": 80.3,
          "critpt": 0.0,
          "livecodebench": 65.1
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 23.855265124432417,
        "coding_score": 11.94,
        "gdpval": 534.1413742729939,
        "terminalbench_hard": 0.038,
        "tau2": 0.465,
        "lcr": 13.0,
        "hle": 8.2,
        "gpqa": 69.5,
        "scicode": 28.2,
        "ifbench": 57.0,
        "aime25": 80.3,
        "critpt": 0.0,
        "livecodebench": 65.1
      }
    },
    "confidence_score": 0.79,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "k2-v2",
    "canonical_name": "K2-V2 (medium)",
    "model_name": "K2-V2 (medium)",
    "aliases": [
      "K2-V2 (high)",
      "K2-V2 (low)",
      "K2-V2 (medium)"
    ],
    "provider": null,
    "context_window": 512000,
    "open_source": true,
    "arena_votes": null,
    "license_type": "Apache 2.0",
    "creator": null,
    "intelligence_score": 20.6517,
    "coding_score": 13.5133,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": null,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": 0.0,
    "latency_seconds": 0.0,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": 0.0,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": 545.1732,
    "terminalbench_hard": 0.0753,
    "tau2": 0.245,
    "lcr": 26.7667,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": 6.0333,
    "gpqa": 60.6667,
    "scicode": 25.3667,
    "ifbench": 52.0667,
    "aime25": 59.4333,
    "critpt": 0.0,
    "mmmu_pro": null,
    "livecodebench": 54.2667,
    "sources": [
      "Artificial Analysis"
    ],
    "source_count": 1,
    "data_tier": 2,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.908880+00:00",
        "confidence": 1.0,
        "raw_name": "K2-V2 (medium)",
        "raw_scores": {
          "intelligence_score": 20.839598542514658,
          "coding_score": 13.97,
          "blended_cost_per_1m": 0.0,
          "latency_seconds": 0.0,
          "tokens_per_second": 0.0,
          "context_window": 512000,
          "gdpval": 619.7398896125515,
          "terminalbench_hard": 0.083,
          "tau2": 0.249,
          "lcr": 28.0,
          "hle": 4.4,
          "gpqa": 59.8,
          "scicode": 25.2,
          "ifbench": 55.1,
          "aime25": 64.7,
          "critpt": 0.0,
          "livecodebench": 54.1
        }
      },
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.908922+00:00",
        "confidence": 1.0,
        "raw_name": "K2-V2 (low)",
        "raw_scores": {
          "intelligence_score": 16.469907911796557,
          "coding_score": 10.48,
          "blended_cost_per_1m": 0.0,
          "latency_seconds": 0.0,
          "tokens_per_second": 0.0,
          "context_window": 512000,
          "gdpval": 411.0294647776882,
          "terminalbench_hard": 0.045,
          "tau2": 0.208,
          "lcr": 19.0,
          "hle": 3.9,
          "gpqa": 54.1,
          "scicode": 22.3,
          "ifbench": 41.0,
          "aime25": 35.3,
          "critpt": 0.0,
          "livecodebench": 39.3
        }
      },
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.908964+00:00",
        "confidence": 1.0,
        "raw_name": "K2-V2 (high)",
        "raw_scores": {
          "intelligence_score": 24.645638627933785,
          "coding_score": 16.09,
          "blended_cost_per_1m": 0.0,
          "latency_seconds": 0.0,
          "tokens_per_second": 0.0,
          "context_window": 512000,
          "gdpval": 604.7501813227954,
          "terminalbench_hard": 0.098,
          "tau2": 0.278,
          "lcr": 33.3,
          "hle": 9.8,
          "gpqa": 68.1,
          "scicode": 28.6,
          "ifbench": 60.1,
          "aime25": 78.3,
          "critpt": 0.0,
          "livecodebench": 69.4
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 20.839598542514658,
        "coding_score": 13.97,
        "gdpval": 619.7398896125515,
        "terminalbench_hard": 0.083,
        "tau2": 0.249,
        "lcr": 28.0,
        "hle": 4.4,
        "gpqa": 59.8,
        "scicode": 25.2,
        "ifbench": 55.1,
        "aime25": 64.7,
        "critpt": 0.0,
        "livecodebench": 54.1
      }
    },
    "confidence_score": 1.0,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "k2-think-v2",
    "canonical_name": "K2 Think V2",
    "model_name": "K2 Think V2",
    "aliases": [
      "K2 Think V2"
    ],
    "provider": null,
    "context_window": 262144,
    "open_source": null,
    "arena_votes": null,
    "license_type": null,
    "creator": null,
    "intelligence_score": 24.12,
    "coding_score": 15.54,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": null,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": 0.0,
    "latency_seconds": 0.0,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": 0.0,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": 650.4485,
    "terminalbench_hard": 0.068,
    "tau2": 0.254,
    "lcr": 52.7,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": 9.5,
    "gpqa": 71.3,
    "scicode": 33.0,
    "ifbench": 62.8,
    "aime25": null,
    "critpt": 0.0,
    "mmmu_pro": null,
    "livecodebench": null,
    "sources": [
      "Artificial Analysis"
    ],
    "source_count": 1,
    "data_tier": 2,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.909000+00:00",
        "confidence": 0.8600000000000001,
        "raw_name": "K2 Think V2",
        "raw_scores": {
          "intelligence_score": 24.12,
          "coding_score": 15.54,
          "blended_cost_per_1m": 0.0,
          "latency_seconds": 0.0,
          "tokens_per_second": 0.0,
          "context_window": 262144,
          "gdpval": 650.4484618818886,
          "terminalbench_hard": 0.068,
          "tau2": 0.254,
          "lcr": 52.7,
          "hle": 9.5,
          "gpqa": 71.3,
          "scicode": 33.0,
          "ifbench": 62.8,
          "critpt": 0.0
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 24.12,
        "coding_score": 15.54,
        "gdpval": 650.4484618818886,
        "terminalbench_hard": 0.068,
        "tau2": 0.254,
        "lcr": 52.7,
        "hle": 9.5,
        "gpqa": 71.3,
        "scicode": 33.0,
        "ifbench": 62.8,
        "critpt": 0.0
      }
    },
    "confidence_score": 0.86,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "midm-k-25-pro",
    "canonical_name": "Mi:dm K 2.5 Pro",
    "model_name": "Mi:dm K 2.5 Pro",
    "aliases": [
      "Mi:dm K 2.5 Pro"
    ],
    "provider": null,
    "context_window": 128000,
    "open_source": null,
    "arena_votes": null,
    "license_type": null,
    "creator": null,
    "intelligence_score": 23.06,
    "coding_score": 12.57,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": null,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": 0.0,
    "latency_seconds": 0.0,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": 0.0,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": 684.1916,
    "terminalbench_hard": 0.023,
    "tau2": 0.865,
    "lcr": 9.0,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": 7.7,
    "gpqa": 70.1,
    "scicode": 33.2,
    "ifbench": 49.3,
    "aime25": 76.7,
    "critpt": 0.0,
    "mmmu_pro": null,
    "livecodebench": 65.6,
    "sources": [
      "Artificial Analysis"
    ],
    "source_count": 1,
    "data_tier": 2,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.909040+00:00",
        "confidence": 1.0,
        "raw_name": "Mi:dm K 2.5 Pro",
        "raw_scores": {
          "intelligence_score": 23.06,
          "coding_score": 12.57,
          "blended_cost_per_1m": 0.0,
          "latency_seconds": 0.0,
          "tokens_per_second": 0.0,
          "context_window": 128000,
          "gdpval": 684.19163151999,
          "terminalbench_hard": 0.023,
          "tau2": 0.865,
          "lcr": 9.0,
          "hle": 7.7,
          "gpqa": 70.1,
          "scicode": 33.2,
          "ifbench": 49.3,
          "aime25": 76.7,
          "critpt": 0.0,
          "livecodebench": 65.6
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 23.06,
        "coding_score": 12.57,
        "gdpval": 684.19163151999,
        "terminalbench_hard": 0.023,
        "tau2": 0.865,
        "lcr": 9.0,
        "hle": 7.7,
        "gpqa": 70.1,
        "scicode": 33.2,
        "ifbench": 49.3,
        "aime25": 76.7,
        "critpt": 0.0,
        "livecodebench": 65.6
      }
    },
    "confidence_score": 1.0,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "midm-k-25-pro-preview",
    "canonical_name": "Mi:dm K 2.5 Pro Preview",
    "model_name": "Mi:dm K 2.5 Pro Preview",
    "aliases": [
      "Mi:dm K 2.5 Pro Preview"
    ],
    "provider": null,
    "context_window": 128000,
    "open_source": null,
    "arena_votes": null,
    "license_type": null,
    "creator": null,
    "intelligence_score": null,
    "coding_score": 11.94,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": null,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": 0.0,
    "latency_seconds": 0.0,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": 0.0,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": 0.03,
    "tau2": 0.494,
    "lcr": 11.0,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": 8.8,
    "gpqa": 72.2,
    "scicode": 29.7,
    "ifbench": 45.6,
    "aime25": 78.7,
    "critpt": 0.0,
    "mmmu_pro": null,
    "livecodebench": 57.6,
    "sources": [
      "Artificial Analysis"
    ],
    "source_count": 1,
    "data_tier": 2,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.909079+00:00",
        "confidence": 0.9300000000000002,
        "raw_name": "Mi:dm K 2.5 Pro Preview",
        "raw_scores": {
          "coding_score": 11.94,
          "blended_cost_per_1m": 0.0,
          "latency_seconds": 0.0,
          "tokens_per_second": 0.0,
          "context_window": 128000,
          "terminalbench_hard": 0.03,
          "tau2": 0.494,
          "lcr": 11.0,
          "hle": 8.8,
          "gpqa": 72.2,
          "scicode": 29.7,
          "ifbench": 45.6,
          "aime25": 78.7,
          "critpt": 0.0,
          "livecodebench": 57.6
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "coding_score": 11.94,
        "terminalbench_hard": 0.03,
        "tau2": 0.494,
        "lcr": 11.0,
        "hle": 8.8,
        "gpqa": 72.2,
        "scicode": 29.7,
        "ifbench": 45.6,
        "aime25": 78.7,
        "critpt": 0.0,
        "livecodebench": 57.6
      }
    },
    "confidence_score": 0.93,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "hyperclova-x-seed-think",
    "canonical_name": "HyperCLOVA X SEED Think (32B)",
    "model_name": "HyperCLOVA X SEED Think (32B)",
    "aliases": [
      "HyperCLOVA X SEED Think (32B)"
    ],
    "provider": null,
    "context_window": 128000,
    "open_source": null,
    "arena_votes": null,
    "license_type": null,
    "creator": null,
    "intelligence_score": 23.72,
    "coding_score": 17.53,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": null,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": 0.0,
    "latency_seconds": 0.0,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": 0.0,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": 728.2205,
    "terminalbench_hard": 0.121,
    "tau2": 0.874,
    "lcr": 11.7,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": 5.5,
    "gpqa": 61.5,
    "scicode": 28.4,
    "ifbench": 37.9,
    "aime25": 59.0,
    "critpt": 0.0,
    "mmmu_pro": null,
    "livecodebench": 62.9,
    "sources": [
      "Artificial Analysis"
    ],
    "source_count": 1,
    "data_tier": 2,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.909119+00:00",
        "confidence": 1.0,
        "raw_name": "HyperCLOVA X SEED Think (32B)",
        "raw_scores": {
          "intelligence_score": 23.72,
          "coding_score": 17.53,
          "blended_cost_per_1m": 0.0,
          "latency_seconds": 0.0,
          "tokens_per_second": 0.0,
          "context_window": 128000,
          "gdpval": 728.220500659655,
          "terminalbench_hard": 0.121,
          "tau2": 0.874,
          "lcr": 11.7,
          "hle": 5.5,
          "gpqa": 61.5,
          "scicode": 28.4,
          "ifbench": 37.9,
          "aime25": 59.0,
          "critpt": 0.0,
          "livecodebench": 62.9
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 23.72,
        "coding_score": 17.53,
        "gdpval": 728.220500659655,
        "terminalbench_hard": 0.121,
        "tau2": 0.874,
        "lcr": 11.7,
        "hle": 5.5,
        "gpqa": 61.5,
        "scicode": 28.4,
        "ifbench": 37.9,
        "aime25": 59.0,
        "critpt": 0.0,
        "livecodebench": 62.9
      }
    },
    "confidence_score": 1.0,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "tri-21b-think",
    "canonical_name": "Tri-21B-Think",
    "model_name": "Tri-21B-Think",
    "aliases": [
      "Tri-21B-Think"
    ],
    "provider": null,
    "context_window": 32000,
    "open_source": null,
    "arena_votes": null,
    "license_type": null,
    "creator": null,
    "intelligence_score": null,
    "coding_score": 6.29,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": null,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": 0.0,
    "latency_seconds": 0.0,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": 0.0,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": 420.8594,
    "terminalbench_hard": 0.008,
    "tau2": 0.81,
    "lcr": 11.0,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": 6.1,
    "gpqa": 60.1,
    "scicode": 17.4,
    "ifbench": 54.6,
    "aime25": null,
    "critpt": 0.3,
    "mmmu_pro": null,
    "livecodebench": null,
    "sources": [
      "Artificial Analysis"
    ],
    "source_count": 1,
    "data_tier": 2,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.909155+00:00",
        "confidence": 0.79,
        "raw_name": "Tri-21B-Think",
        "raw_scores": {
          "coding_score": 6.29,
          "blended_cost_per_1m": 0.0,
          "latency_seconds": 0.0,
          "tokens_per_second": 0.0,
          "context_window": 32000,
          "gdpval": 420.8594430700846,
          "terminalbench_hard": 0.008,
          "tau2": 0.81,
          "lcr": 11.0,
          "hle": 6.1,
          "gpqa": 60.1,
          "scicode": 17.4,
          "ifbench": 54.6,
          "critpt": 0.3
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "coding_score": 6.29,
        "gdpval": 420.8594430700846,
        "terminalbench_hard": 0.008,
        "tau2": 0.81,
        "lcr": 11.0,
        "hle": 6.1,
        "gpqa": 60.1,
        "scicode": 17.4,
        "ifbench": 54.6,
        "critpt": 0.3
      }
    },
    "confidence_score": 0.79,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "tri-21b-think-preview",
    "canonical_name": "Tri-21B-think Preview",
    "model_name": "Tri-21B-think Preview",
    "aliases": [
      "Tri-21B-think Preview"
    ],
    "provider": null,
    "context_window": 32000,
    "open_source": null,
    "arena_votes": null,
    "license_type": null,
    "creator": null,
    "intelligence_score": null,
    "coding_score": 7.44,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": null,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": 0.0,
    "latency_seconds": 0.0,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": 0.0,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": 337.0179,
    "terminalbench_hard": 0.023,
    "tau2": 0.933,
    "lcr": 14.7,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": 5.7,
    "gpqa": 53.8,
    "scicode": 17.8,
    "ifbench": 47.1,
    "aime25": null,
    "critpt": 0.0,
    "mmmu_pro": null,
    "livecodebench": null,
    "sources": [
      "Artificial Analysis"
    ],
    "source_count": 1,
    "data_tier": 2,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.909203+00:00",
        "confidence": 0.79,
        "raw_name": "Tri-21B-think Preview",
        "raw_scores": {
          "coding_score": 7.44,
          "blended_cost_per_1m": 0.0,
          "latency_seconds": 0.0,
          "tokens_per_second": 0.0,
          "context_window": 32000,
          "gdpval": 337.0179335195246,
          "terminalbench_hard": 0.023,
          "tau2": 0.933,
          "lcr": 14.7,
          "hle": 5.7,
          "gpqa": 53.8,
          "scicode": 17.8,
          "ifbench": 47.1,
          "critpt": 0.0
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "coding_score": 7.44,
        "gdpval": 337.0179335195246,
        "terminalbench_hard": 0.023,
        "tau2": 0.933,
        "lcr": 14.7,
        "hle": 5.7,
        "gpqa": 53.8,
        "scicode": 17.8,
        "ifbench": 47.1,
        "critpt": 0.0
      }
    },
    "confidence_score": 0.79,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "glm-46v",
    "canonical_name": "GLM-4.6V (Reasoning)",
    "model_name": "GLM-4.6V (Reasoning)",
    "aliases": [
      "GLM-4.6V (Non-reasoning)",
      "GLM-4.6V (Reasoning)",
      "glm-4.6v"
    ],
    "provider": "Z.ai",
    "context_window": 128000,
    "open_source": true,
    "arena_votes": 2785,
    "license_type": "MIT",
    "creator": null,
    "intelligence_score": 18.7131,
    "coding_score": 15.415,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 1377.44,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": 688.0865,
    "terminalbench_hard": 0.087,
    "tau2": 0.3115,
    "lcr": 26.3,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": 6.3,
    "gpqa": 64.25,
    "scicode": 28.8,
    "ifbench": 29.0,
    "aime25": 55.8,
    "critpt": 0.0,
    "mmmu_pro": 45.4,
    "livecodebench": 28.55,
    "sources": [
      "Artificial Analysis",
      "LMSYS Chatbot Arena"
    ],
    "source_count": 2,
    "data_tier": 1,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.909246+00:00",
        "confidence": 0.79,
        "raw_name": "GLM-4.6V (Reasoning)",
        "raw_scores": {
          "intelligence_score": 21.280474303103592,
          "coding_score": 19.74,
          "context_window": 128000,
          "gdpval": 647.1896501043843,
          "terminalbench_hard": 0.144,
          "tau2": 0.316,
          "lcr": 40.3,
          "hle": 8.9,
          "gpqa": 71.9,
          "scicode": 30.4,
          "ifbench": 30.1,
          "aime25": 85.3,
          "critpt": 0.0,
          "mmmu_pro": 48.6,
          "livecodebench": 16.0
        }
      },
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.909395+00:00",
        "confidence": 0.79,
        "raw_name": "GLM-4.6V (Non-reasoning)",
        "raw_scores": {
          "intelligence_score": 16.14579998349155,
          "coding_score": 11.09,
          "context_window": 128000,
          "gdpval": 728.9833912386684,
          "terminalbench_hard": 0.03,
          "tau2": 0.307,
          "lcr": 12.3,
          "hle": 3.7,
          "gpqa": 56.6,
          "scicode": 27.2,
          "ifbench": 27.9,
          "aime25": 26.3,
          "critpt": 0.0,
          "mmmu_pro": 42.2,
          "livecodebench": 41.1
        }
      },
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.892715+00:00",
        "confidence": 1.0,
        "raw_name": "glm-4.6v",
        "raw_scores": {
          "arena_elo": 1377.44,
          "arena_votes": 2785
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 21.280474303103592,
        "coding_score": 19.74,
        "gdpval": 647.1896501043843,
        "terminalbench_hard": 0.144,
        "tau2": 0.316,
        "lcr": 40.3,
        "hle": 8.9,
        "gpqa": 71.9,
        "scicode": 30.4,
        "ifbench": 30.1,
        "aime25": 85.3,
        "critpt": 0.0,
        "mmmu_pro": 48.6,
        "livecodebench": 16.0
      },
      "LMSYS Chatbot Arena": {
        "arena_elo": 1377.44
      }
    },
    "confidence_score": 0.86,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "glm-47-flash",
    "canonical_name": "GLM-4.7-Flash (Reasoning)",
    "model_name": "GLM-4.7-Flash (Reasoning)",
    "aliases": [
      "GLM-4.7-Flash (Non-reasoning)",
      "GLM-4.7-Flash (Reasoning)",
      "glm-4.7-flash"
    ],
    "provider": "Z.ai",
    "context_window": 200000,
    "open_source": true,
    "arena_votes": 10660,
    "license_type": "MIT",
    "creator": null,
    "intelligence_score": 26.11,
    "coding_score": 18.44,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 1365.72,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": 852.8671,
    "terminalbench_hard": 0.129,
    "tau2": 0.953,
    "lcr": 24.85,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": 6.0,
    "gpqa": 51.65,
    "scicode": 29.6,
    "ifbench": 53.55,
    "aime25": null,
    "critpt": 0.15,
    "mmmu_pro": null,
    "livecodebench": null,
    "sources": [
      "Artificial Analysis",
      "LMSYS Chatbot Arena"
    ],
    "source_count": 2,
    "data_tier": 1,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.909284+00:00",
        "confidence": 0.65,
        "raw_name": "GLM-4.7-Flash (Reasoning)",
        "raw_scores": {
          "intelligence_score": 30.15,
          "coding_score": 25.87,
          "context_window": 200000,
          "gdpval": 872.099258680711,
          "terminalbench_hard": 0.22,
          "tau2": 0.988,
          "lcr": 35.0,
          "hle": 7.1,
          "gpqa": 58.1,
          "scicode": 33.7,
          "ifbench": 60.8,
          "critpt": 0.3
        }
      },
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.909318+00:00",
        "confidence": 0.65,
        "raw_name": "GLM-4.7-Flash (Non-reasoning)",
        "raw_scores": {
          "intelligence_score": 22.07,
          "coding_score": 11.01,
          "context_window": 200000,
          "gdpval": 833.6350290962528,
          "terminalbench_hard": 0.038,
          "tau2": 0.918,
          "lcr": 14.7,
          "hle": 4.9,
          "gpqa": 45.2,
          "scicode": 25.5,
          "ifbench": 46.3,
          "critpt": 0.0
        }
      },
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.892853+00:00",
        "confidence": 1.0,
        "raw_name": "glm-4.7-flash",
        "raw_scores": {
          "arena_elo": 1365.72,
          "arena_votes": 10660
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 30.15,
        "coding_score": 25.87,
        "gdpval": 872.099258680711,
        "terminalbench_hard": 0.22,
        "tau2": 0.988,
        "lcr": 35.0,
        "hle": 7.1,
        "gpqa": 58.1,
        "scicode": 33.7,
        "ifbench": 60.8,
        "critpt": 0.3
      },
      "LMSYS Chatbot Arena": {
        "arena_elo": 1365.72
      }
    },
    "confidence_score": 0.7667,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "glm-5",
    "canonical_name": "GLM-5 (Reasoning)",
    "model_name": "GLM-5 (Reasoning)",
    "aliases": [
      "GLM-5 (Non-reasoning)",
      "GLM-5 (Reasoning)",
      "glm-5"
    ],
    "provider": "Z.ai",
    "context_window": 200000,
    "open_source": true,
    "arena_votes": 6466,
    "license_type": "MIT",
    "creator": null,
    "intelligence_score": 45.17,
    "coding_score": 41.605,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 1455.43,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": 1374.3172,
    "terminalbench_hard": 0.413,
    "tau2": 0.978,
    "lcr": 50.15,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": 17.2,
    "gpqa": 74.3,
    "scicode": 42.25,
    "ifbench": 63.75,
    "aime25": null,
    "critpt": 1.0,
    "mmmu_pro": null,
    "livecodebench": null,
    "sources": [
      "Artificial Analysis",
      "LMSYS Chatbot Arena"
    ],
    "source_count": 2,
    "data_tier": 1,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.909352+00:00",
        "confidence": 0.65,
        "raw_name": "GLM-5 (Reasoning)",
        "raw_scores": {
          "intelligence_score": 49.77,
          "coding_score": 44.18,
          "context_window": 200000,
          "gdpval": 1409.6938187257547,
          "terminalbench_hard": 0.432,
          "tau2": 0.982,
          "lcr": 63.3,
          "hle": 27.2,
          "gpqa": 82.0,
          "scicode": 46.2,
          "ifbench": 72.3,
          "critpt": 2.0
        }
      },
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.909428+00:00",
        "confidence": 0.65,
        "raw_name": "GLM-5 (Non-reasoning)",
        "raw_scores": {
          "intelligence_score": 40.57,
          "coding_score": 39.03,
          "context_window": 200000,
          "gdpval": 1338.9404899694362,
          "terminalbench_hard": 0.394,
          "tau2": 0.974,
          "lcr": 37.0,
          "hle": 7.2,
          "gpqa": 66.6,
          "scicode": 38.3,
          "ifbench": 55.2,
          "critpt": 0.0
        }
      },
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.891495+00:00",
        "confidence": 1.0,
        "raw_name": "glm-5",
        "raw_scores": {
          "arena_elo": 1455.43,
          "arena_votes": 6466
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 49.77,
        "coding_score": 44.18,
        "gdpval": 1409.6938187257547,
        "terminalbench_hard": 0.432,
        "tau2": 0.982,
        "lcr": 63.3,
        "hle": 27.2,
        "gpqa": 82.0,
        "scicode": 46.2,
        "ifbench": 72.3,
        "critpt": 2.0
      },
      "LMSYS Chatbot Arena": {
        "arena_elo": 1455.43
      }
    },
    "confidence_score": 0.7667,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "command-a",
    "canonical_name": "Command A",
    "model_name": "Command A",
    "aliases": [
      "Command A",
      "command-a-03-2025"
    ],
    "provider": "Cohere",
    "context_window": 256000,
    "open_source": false,
    "arena_votes": 57098,
    "license_type": "CC-BY-NC 4.0 License with Acceptable Use Addendum",
    "creator": null,
    "intelligence_score": 14.7117,
    "coding_score": 9.88,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 1353.06,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": 4.38,
    "latency_seconds": 0.34,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": 44.0,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": 324.913,
    "terminalbench_hard": 0.008,
    "tau2": 0.152,
    "lcr": 18.0,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": 4.6,
    "gpqa": 52.7,
    "scicode": 28.1,
    "ifbench": 36.5,
    "aime25": 13.0,
    "critpt": 0.0,
    "mmmu_pro": null,
    "livecodebench": 28.7,
    "sources": [
      "Artificial Analysis",
      "LMSYS Chatbot Arena"
    ],
    "source_count": 2,
    "data_tier": 1,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.909472+00:00",
        "confidence": 1.0,
        "raw_name": "Command A",
        "raw_scores": {
          "intelligence_score": 14.711727324474086,
          "coding_score": 9.88,
          "blended_cost_per_1m": 4.38,
          "latency_seconds": 0.34,
          "tokens_per_second": 44.0,
          "context_window": 256000,
          "gdpval": 324.91296193896756,
          "terminalbench_hard": 0.008,
          "tau2": 0.152,
          "lcr": 18.0,
          "hle": 4.6,
          "gpqa": 52.7,
          "scicode": 28.1,
          "ifbench": 36.5,
          "aime25": 13.0,
          "critpt": 0.0,
          "livecodebench": 28.7
        }
      },
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.893016+00:00",
        "confidence": 1.0,
        "raw_name": "command-a-03-2025",
        "raw_scores": {
          "arena_elo": 1353.06,
          "arena_votes": 57098
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 14.711727324474086,
        "coding_score": 9.88,
        "gdpval": 324.91296193896756,
        "terminalbench_hard": 0.008,
        "tau2": 0.152,
        "lcr": 18.0,
        "hle": 4.6,
        "gpqa": 52.7,
        "scicode": 28.1,
        "ifbench": 36.5,
        "aime25": 13.0,
        "critpt": 0.0,
        "livecodebench": 28.7
      },
      "LMSYS Chatbot Arena": {
        "arena_elo": 1353.06
      }
    },
    "confidence_score": 1.0,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "tiny-aya-global",
    "canonical_name": "Tiny Aya Global",
    "model_name": "Tiny Aya Global",
    "aliases": [
      "Tiny Aya Global"
    ],
    "provider": null,
    "context_window": 8192,
    "open_source": null,
    "arena_votes": null,
    "license_type": "Cc By Nc 4.0",
    "creator": null,
    "intelligence_score": null,
    "coding_score": 1.2,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": null,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": 0.0,
    "latency_seconds": 0.0,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": 0.0,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": 0.0,
    "tau2": 0.0,
    "lcr": 0.0,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": 5.2,
    "gpqa": 30.5,
    "scicode": 3.6,
    "ifbench": 20.1,
    "aime25": null,
    "critpt": 0.0,
    "mmmu_pro": null,
    "livecodebench": null,
    "sources": [
      "Artificial Analysis"
    ],
    "source_count": 1,
    "data_tier": 2,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.909504+00:00",
        "confidence": 0.79,
        "raw_name": "Tiny Aya Global",
        "raw_scores": {
          "coding_score": 1.2,
          "blended_cost_per_1m": 0.0,
          "latency_seconds": 0.0,
          "tokens_per_second": 0.0,
          "context_window": 8192,
          "terminalbench_hard": 0.0,
          "tau2": 0.0,
          "lcr": 0.0,
          "hle": 5.2,
          "gpqa": 30.5,
          "scicode": 3.6,
          "ifbench": 20.1,
          "critpt": 0.0
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "coding_score": 1.2,
        "terminalbench_hard": 0.0,
        "tau2": 0.0,
        "lcr": 0.0,
        "hle": 5.2,
        "gpqa": 30.5,
        "scicode": 3.6,
        "ifbench": 20.1,
        "critpt": 0.0
      }
    },
    "confidence_score": 0.79,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "apriel-v16-15b-thinker",
    "canonical_name": "Apriel-v1.6-15B-Thinker",
    "model_name": "Apriel-v1.6-15B-Thinker",
    "aliases": [
      "Apriel-v1.6-15B-Thinker"
    ],
    "provider": null,
    "context_window": 128000,
    "open_source": true,
    "arena_votes": null,
    "license_type": "MIT",
    "creator": null,
    "intelligence_score": 27.58,
    "coding_score": 22.02,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": null,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": 0.0,
    "latency_seconds": 0.21,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": 140.0,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": 607.781,
    "terminalbench_hard": 0.144,
    "tau2": 0.693,
    "lcr": 50.3,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": 9.8,
    "gpqa": 73.3,
    "scicode": 37.3,
    "ifbench": 69.1,
    "aime25": 88.0,
    "critpt": 0.3,
    "mmmu_pro": null,
    "livecodebench": 80.7,
    "sources": [
      "Artificial Analysis"
    ],
    "source_count": 1,
    "data_tier": 2,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.909547+00:00",
        "confidence": 1.0,
        "raw_name": "Apriel-v1.6-15B-Thinker",
        "raw_scores": {
          "intelligence_score": 27.58,
          "coding_score": 22.02,
          "blended_cost_per_1m": 0.0,
          "latency_seconds": 0.21,
          "tokens_per_second": 140.0,
          "context_window": 128000,
          "gdpval": 607.7809774169857,
          "terminalbench_hard": 0.144,
          "tau2": 0.693,
          "lcr": 50.3,
          "hle": 9.8,
          "gpqa": 73.3,
          "scicode": 37.3,
          "ifbench": 69.1,
          "aime25": 88.0,
          "critpt": 0.3,
          "livecodebench": 80.7
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 27.58,
        "coding_score": 22.02,
        "gdpval": 607.7809774169857,
        "terminalbench_hard": 0.144,
        "tau2": 0.693,
        "lcr": 50.3,
        "hle": 9.8,
        "gpqa": 73.3,
        "scicode": 37.3,
        "ifbench": 69.1,
        "aime25": 88.0,
        "critpt": 0.3,
        "livecodebench": 80.7
      }
    },
    "confidence_score": 1.0,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "jamba-17-large",
    "canonical_name": "Jamba 1.7 Large",
    "model_name": "Jamba 1.7 Large",
    "aliases": [
      "Jamba 1.7 Large"
    ],
    "provider": null,
    "context_window": 256000,
    "open_source": true,
    "arena_votes": null,
    "license_type": "Jamba Open Model License Agreement",
    "creator": null,
    "intelligence_score": 12.5192,
    "coding_score": 7.77,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": null,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": 3.5,
    "latency_seconds": 0.83,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": 52.0,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": 340.8897,
    "terminalbench_hard": 0.023,
    "tau2": 0.135,
    "lcr": 17.3,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": 3.8,
    "gpqa": 39.0,
    "scicode": 18.8,
    "ifbench": 35.2,
    "aime25": 2.3,
    "critpt": 0.0,
    "mmmu_pro": null,
    "livecodebench": 18.1,
    "sources": [
      "Artificial Analysis"
    ],
    "source_count": 1,
    "data_tier": 2,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.909593+00:00",
        "confidence": 1.0,
        "raw_name": "Jamba 1.7 Large",
        "raw_scores": {
          "intelligence_score": 12.519156020602248,
          "coding_score": 7.77,
          "blended_cost_per_1m": 3.5,
          "latency_seconds": 0.83,
          "tokens_per_second": 52.0,
          "context_window": 256000,
          "gdpval": 340.8897201687371,
          "terminalbench_hard": 0.023,
          "tau2": 0.135,
          "lcr": 17.3,
          "hle": 3.8,
          "gpqa": 39.0,
          "scicode": 18.8,
          "ifbench": 35.2,
          "aime25": 2.3,
          "critpt": 0.0,
          "livecodebench": 18.1
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 12.519156020602248,
        "coding_score": 7.77,
        "gdpval": 340.8897201687371,
        "terminalbench_hard": 0.023,
        "tau2": 0.135,
        "lcr": 17.3,
        "hle": 3.8,
        "gpqa": 39.0,
        "scicode": 18.8,
        "ifbench": 35.2,
        "aime25": 2.3,
        "critpt": 0.0,
        "livecodebench": 18.1
      }
    },
    "confidence_score": 1.0,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "jamba-reasoning-3b",
    "canonical_name": "Jamba Reasoning 3B",
    "model_name": "Jamba Reasoning 3B",
    "aliases": [
      "Jamba Reasoning 3B"
    ],
    "provider": null,
    "context_window": 262000,
    "open_source": true,
    "arena_votes": null,
    "license_type": "Apache 2.0",
    "creator": null,
    "intelligence_score": 12.5657,
    "coding_score": 2.47,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": null,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": 0.0,
    "latency_seconds": 0.0,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": 0.0,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": 310.8425,
    "terminalbench_hard": 0.008,
    "tau2": 0.158,
    "lcr": 7.0,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": 4.6,
    "gpqa": 33.3,
    "scicode": 5.9,
    "ifbench": 52.4,
    "aime25": 10.7,
    "critpt": 0.0,
    "mmmu_pro": null,
    "livecodebench": 21.0,
    "sources": [
      "Artificial Analysis"
    ],
    "source_count": 1,
    "data_tier": 2,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.909637+00:00",
        "confidence": 1.0,
        "raw_name": "Jamba Reasoning 3B",
        "raw_scores": {
          "intelligence_score": 12.565688353543328,
          "coding_score": 2.47,
          "blended_cost_per_1m": 0.0,
          "latency_seconds": 0.0,
          "tokens_per_second": 0.0,
          "context_window": 262000,
          "gdpval": 310.8425056055562,
          "terminalbench_hard": 0.008,
          "tau2": 0.158,
          "lcr": 7.0,
          "hle": 4.6,
          "gpqa": 33.3,
          "scicode": 5.9,
          "ifbench": 52.4,
          "aime25": 10.7,
          "critpt": 0.0,
          "livecodebench": 21.0
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 12.565688353543328,
        "coding_score": 2.47,
        "gdpval": 310.8425056055562,
        "terminalbench_hard": 0.008,
        "tau2": 0.158,
        "lcr": 7.0,
        "hle": 4.6,
        "gpqa": 33.3,
        "scicode": 5.9,
        "ifbench": 52.4,
        "aime25": 10.7,
        "critpt": 0.0,
        "livecodebench": 21.0
      }
    },
    "confidence_score": 1.0,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "jamba-17-mini",
    "canonical_name": "Jamba 1.7 Mini",
    "model_name": "Jamba 1.7 Mini",
    "aliases": [
      "Jamba 1.7 Mini"
    ],
    "provider": null,
    "context_window": 258000,
    "open_source": true,
    "arena_votes": null,
    "license_type": "Jamba Open Model License Agreement",
    "creator": null,
    "intelligence_score": 10.6845,
    "coding_score": 3.09,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": null,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": 0.0,
    "latency_seconds": 0.0,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": 0.0,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": 340.5312,
    "terminalbench_hard": 0.0,
    "tau2": 0.126,
    "lcr": 12.7,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": 4.5,
    "gpqa": 32.2,
    "scicode": 9.3,
    "ifbench": 31.4,
    "aime25": 0.3,
    "critpt": 0.0,
    "mmmu_pro": null,
    "livecodebench": 6.1,
    "sources": [
      "Artificial Analysis"
    ],
    "source_count": 1,
    "data_tier": 2,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.909679+00:00",
        "confidence": 1.0,
        "raw_name": "Jamba 1.7 Mini",
        "raw_scores": {
          "intelligence_score": 10.684455764128451,
          "coding_score": 3.09,
          "blended_cost_per_1m": 0.0,
          "latency_seconds": 0.0,
          "tokens_per_second": 0.0,
          "context_window": 258000,
          "gdpval": 340.53124553054624,
          "terminalbench_hard": 0.0,
          "tau2": 0.126,
          "lcr": 12.7,
          "hle": 4.5,
          "gpqa": 32.2,
          "scicode": 9.3,
          "ifbench": 31.4,
          "aime25": 0.3,
          "critpt": 0.0,
          "livecodebench": 6.1
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 10.684455764128451,
        "coding_score": 3.09,
        "gdpval": 340.53124553054624,
        "terminalbench_hard": 0.0,
        "tau2": 0.126,
        "lcr": 12.7,
        "hle": 4.5,
        "gpqa": 32.2,
        "scicode": 9.3,
        "ifbench": 31.4,
        "aime25": 0.3,
        "critpt": 0.0,
        "livecodebench": 6.1
      }
    },
    "confidence_score": 1.0,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "qwen35-397b",
    "canonical_name": "Qwen3.5 397B A17B (Reasoning)",
    "model_name": "Qwen3.5 397B A17B (Reasoning)",
    "aliases": [
      "Qwen3.5 397B A17B (Non-reasoning)",
      "Qwen3.5 397B A17B (Reasoning)",
      "qwen3.5-397b-a17b"
    ],
    "provider": "Alibaba",
    "context_window": 262144,
    "open_source": true,
    "arena_votes": 4958,
    "license_type": "Apache 2.0",
    "creator": null,
    "intelligence_score": 42.575,
    "coding_score": 39.355,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 1453.57,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": 1233.2446,
    "terminalbench_hard": 0.3825,
    "tau2": 0.8975,
    "lcr": 61.85,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": 23.05,
    "gpqa": 87.7,
    "scicode": 41.55,
    "ifbench": 65.2,
    "aime25": null,
    "critpt": 1.3,
    "mmmu_pro": 52.7,
    "livecodebench": null,
    "sources": [
      "Artificial Analysis",
      "LMSYS Chatbot Arena"
    ],
    "source_count": 2,
    "data_tier": 1,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.909712+00:00",
        "confidence": 0.65,
        "raw_name": "Qwen3.5 397B A17B (Reasoning)",
        "raw_scores": {
          "intelligence_score": 45.05,
          "coding_score": 41.28,
          "context_window": 262144,
          "gdpval": 1207.8158402456686,
          "terminalbench_hard": 0.409,
          "tau2": 0.956,
          "lcr": 65.7,
          "hle": 27.3,
          "gpqa": 89.3,
          "scicode": 42.0,
          "ifbench": 78.8,
          "critpt": 1.7
        }
      },
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.910722+00:00",
        "confidence": 0.65,
        "raw_name": "Qwen3.5 397B A17B (Non-reasoning)",
        "raw_scores": {
          "intelligence_score": 40.1,
          "coding_score": 37.43,
          "context_window": 262144,
          "gdpval": 1258.6733045800597,
          "terminalbench_hard": 0.356,
          "tau2": 0.839,
          "lcr": 58.0,
          "hle": 18.8,
          "gpqa": 86.1,
          "scicode": 41.1,
          "ifbench": 51.6,
          "critpt": 0.9,
          "mmmu_pro": 52.7
        }
      },
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.891508+00:00",
        "confidence": 1.0,
        "raw_name": "qwen3.5-397b-a17b",
        "raw_scores": {
          "arena_elo": 1453.57,
          "arena_votes": 4958
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 45.05,
        "coding_score": 41.28,
        "gdpval": 1207.8158402456686,
        "terminalbench_hard": 0.409,
        "tau2": 0.956,
        "lcr": 65.7,
        "hle": 27.3,
        "gpqa": 89.3,
        "scicode": 42.0,
        "ifbench": 78.8,
        "critpt": 1.7,
        "mmmu_pro": 52.7
      },
      "LMSYS Chatbot Arena": {
        "arena_elo": 1453.57
      }
    },
    "confidence_score": 0.7667,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "qwen3-next-80b",
    "canonical_name": "Qwen3 Next 80B A3B Instruct",
    "model_name": "Qwen3 Next 80B A3B Instruct",
    "aliases": [
      "Qwen3 Next 80B A3B Instruct",
      "qwen3-next-80b-a3b-instruct"
    ],
    "provider": "Alibaba",
    "context_window": 262144,
    "open_source": true,
    "arena_votes": 22670,
    "license_type": "Apache 2.0",
    "creator": null,
    "intelligence_score": 23.6726,
    "coding_score": 15.27,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 1401.65,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": 658.5191,
    "terminalbench_hard": 0.076,
    "tau2": 0.216,
    "lcr": 51.3,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": 7.3,
    "gpqa": 73.8,
    "scicode": 30.7,
    "ifbench": 39.7,
    "aime25": 66.3,
    "critpt": 0.0,
    "mmmu_pro": null,
    "livecodebench": 68.4,
    "sources": [
      "Artificial Analysis",
      "LMSYS Chatbot Arena"
    ],
    "source_count": 2,
    "data_tier": 1,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.909753+00:00",
        "confidence": 0.79,
        "raw_name": "Qwen3 Next 80B A3B Instruct",
        "raw_scores": {
          "intelligence_score": 23.672640047615896,
          "coding_score": 15.27,
          "context_window": 262144,
          "gdpval": 658.5191014905994,
          "terminalbench_hard": 0.076,
          "tau2": 0.216,
          "lcr": 51.3,
          "hle": 7.3,
          "gpqa": 73.8,
          "scicode": 30.7,
          "ifbench": 39.7,
          "aime25": 66.3,
          "critpt": 0.0,
          "livecodebench": 68.4
        }
      },
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.892257+00:00",
        "confidence": 1.0,
        "raw_name": "qwen3-next-80b-a3b-instruct",
        "raw_scores": {
          "arena_elo": 1401.65,
          "arena_votes": 22670
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 23.672640047615896,
        "coding_score": 15.27,
        "gdpval": 658.5191014905994,
        "terminalbench_hard": 0.076,
        "tau2": 0.216,
        "lcr": 51.3,
        "hle": 7.3,
        "gpqa": 73.8,
        "scicode": 30.7,
        "ifbench": 39.7,
        "aime25": 66.3,
        "critpt": 0.0,
        "livecodebench": 68.4
      },
      "LMSYS Chatbot Arena": {
        "arena_elo": 1401.65
      }
    },
    "confidence_score": 0.895,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "qwen3-235b-a22b-2507-instruct",
    "canonical_name": "Qwen3 235B A22B 2507 Instruct",
    "model_name": "Qwen3 235B A22B 2507 Instruct",
    "aliases": [
      "Qwen3 235B A22B 2507 Instruct"
    ],
    "provider": "Alibaba",
    "context_window": 256000,
    "open_source": true,
    "arena_votes": null,
    "license_type": "Apache 2.0",
    "creator": null,
    "intelligence_score": 24.96,
    "coding_score": 22.1,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": null,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": 807.3206,
    "terminalbench_hard": 0.152,
    "tau2": 0.333,
    "lcr": 31.2,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": 10.6,
    "gpqa": 75.3,
    "scicode": 36.0,
    "ifbench": 46.1,
    "aime25": 71.7,
    "critpt": 0.0,
    "mmmu_pro": null,
    "livecodebench": 52.4,
    "sources": [
      "Artificial Analysis"
    ],
    "source_count": 1,
    "data_tier": 2,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.909791+00:00",
        "confidence": 0.79,
        "raw_name": "Qwen3 235B A22B 2507 Instruct",
        "raw_scores": {
          "intelligence_score": 24.96,
          "coding_score": 22.1,
          "context_window": 256000,
          "gdpval": 807.3205760805643,
          "terminalbench_hard": 0.152,
          "tau2": 0.333,
          "lcr": 31.2,
          "hle": 10.6,
          "gpqa": 75.3,
          "scicode": 36.0,
          "ifbench": 46.1,
          "aime25": 71.7,
          "critpt": 0.0,
          "livecodebench": 52.4
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 24.96,
        "coding_score": 22.1,
        "gdpval": 807.3205760805643,
        "terminalbench_hard": 0.152,
        "tau2": 0.333,
        "lcr": 31.2,
        "hle": 10.6,
        "gpqa": 75.3,
        "scicode": 36.0,
        "ifbench": 46.1,
        "aime25": 71.7,
        "critpt": 0.0,
        "livecodebench": 52.4
      }
    },
    "confidence_score": 0.79,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "qwen3-coder-480b-a35b-instruct",
    "canonical_name": "Qwen3 Coder 480B A35B Instruct",
    "model_name": "Qwen3 Coder 480B A35B Instruct",
    "aliases": [
      "Qwen3 Coder 480B A35B Instruct",
      "qwen3-coder-480b-a35b-instruct"
    ],
    "provider": "Alibaba",
    "context_window": 262144,
    "open_source": true,
    "arena_votes": 26406,
    "license_type": "Apache 2.0",
    "creator": null,
    "intelligence_score": 24.77,
    "coding_score": 24.59,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 1386.54,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": 547.0049,
    "terminalbench_hard": 0.189,
    "tau2": 0.436,
    "lcr": 42.3,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": 4.4,
    "gpqa": 61.8,
    "scicode": 35.9,
    "ifbench": 40.5,
    "aime25": 39.3,
    "critpt": 0.0,
    "mmmu_pro": null,
    "livecodebench": 58.5,
    "sources": [
      "Artificial Analysis",
      "LMSYS Chatbot Arena"
    ],
    "source_count": 2,
    "data_tier": 1,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.909830+00:00",
        "confidence": 0.79,
        "raw_name": "Qwen3 Coder 480B A35B Instruct",
        "raw_scores": {
          "intelligence_score": 24.77,
          "coding_score": 24.59,
          "context_window": 262144,
          "gdpval": 547.0048674938267,
          "terminalbench_hard": 0.189,
          "tau2": 0.436,
          "lcr": 42.3,
          "hle": 4.4,
          "gpqa": 61.8,
          "scicode": 35.9,
          "ifbench": 40.5,
          "aime25": 39.3,
          "critpt": 0.0,
          "livecodebench": 58.5
        }
      },
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.892626+00:00",
        "confidence": 1.0,
        "raw_name": "qwen3-coder-480b-a35b-instruct",
        "raw_scores": {
          "arena_elo": 1386.54,
          "arena_votes": 26406
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 24.77,
        "coding_score": 24.59,
        "gdpval": 547.0048674938267,
        "terminalbench_hard": 0.189,
        "tau2": 0.436,
        "lcr": 42.3,
        "hle": 4.4,
        "gpqa": 61.8,
        "scicode": 35.9,
        "ifbench": 40.5,
        "aime25": 39.3,
        "critpt": 0.0,
        "livecodebench": 58.5
      },
      "LMSYS Chatbot Arena": {
        "arena_elo": 1386.54
      }
    },
    "confidence_score": 0.895,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "qwen3-06b",
    "canonical_name": "Qwen3 0.6B (Non-reasoning)",
    "model_name": "Qwen3 0.6B (Non-reasoning)",
    "aliases": [
      "Qwen3 0.6B (Non-reasoning)",
      "Qwen3 0.6B (Reasoning)"
    ],
    "provider": "Alibaba",
    "context_window": 32000,
    "open_source": true,
    "arena_votes": null,
    "license_type": "Apache 2.0",
    "creator": null,
    "intelligence_score": 10.079,
    "coding_score": 1.14,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": null,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": 349.5522,
    "terminalbench_hard": 0.0,
    "tau2": 0.1785,
    "lcr": 0.0,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": 5.45,
    "gpqa": 23.5,
    "scicode": 3.45,
    "ifbench": 22.6,
    "aime25": 14.15,
    "critpt": 0.0,
    "mmmu_pro": null,
    "livecodebench": 9.7,
    "sources": [
      "Artificial Analysis"
    ],
    "source_count": 1,
    "data_tier": 2,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.909870+00:00",
        "confidence": 0.79,
        "raw_name": "Qwen3 0.6B (Non-reasoning)",
        "raw_scores": {
          "intelligence_score": 9.653252960548699,
          "coding_score": 1.35,
          "context_window": 32000,
          "gdpval": 328.4337645294777,
          "terminalbench_hard": 0.0,
          "tau2": 0.146,
          "lcr": 0.0,
          "hle": 5.2,
          "gpqa": 23.1,
          "scicode": 4.1,
          "ifbench": 21.9,
          "aime25": 10.3,
          "critpt": 0.0,
          "livecodebench": 7.3
        }
      },
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.910687+00:00",
        "confidence": 0.79,
        "raw_name": "Qwen3 0.6B (Reasoning)",
        "raw_scores": {
          "intelligence_score": 10.504775390875873,
          "coding_score": 0.93,
          "context_window": 32000,
          "gdpval": 370.6707085281937,
          "terminalbench_hard": 0.0,
          "tau2": 0.211,
          "lcr": 0.0,
          "hle": 5.7,
          "gpqa": 23.9,
          "scicode": 2.8,
          "ifbench": 23.3,
          "aime25": 18.0,
          "critpt": 0.0,
          "livecodebench": 12.1
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 9.653252960548699,
        "coding_score": 1.35,
        "gdpval": 328.4337645294777,
        "terminalbench_hard": 0.0,
        "tau2": 0.146,
        "lcr": 0.0,
        "hle": 5.2,
        "gpqa": 23.1,
        "scicode": 4.1,
        "ifbench": 21.9,
        "aime25": 10.3,
        "critpt": 0.0,
        "livecodebench": 7.3
      }
    },
    "confidence_score": 0.79,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "qwen3-coder-30b-a3b-instruct",
    "canonical_name": "Qwen3 Coder 30B A3B Instruct",
    "model_name": "Qwen3 Coder 30B A3B Instruct",
    "aliases": [
      "Qwen3 Coder 30B A3B Instruct"
    ],
    "provider": "Alibaba",
    "context_window": 262144,
    "open_source": true,
    "arena_votes": null,
    "license_type": "Apache 2.0",
    "creator": null,
    "intelligence_score": 17.4818,
    "coding_score": 19.36,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": null,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": 758.0963,
    "terminalbench_hard": 0.152,
    "tau2": 0.345,
    "lcr": 29.0,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": 4.0,
    "gpqa": 51.6,
    "scicode": 27.8,
    "ifbench": 32.7,
    "aime25": 29.0,
    "critpt": 0.0,
    "mmmu_pro": null,
    "livecodebench": 40.3,
    "sources": [
      "Artificial Analysis"
    ],
    "source_count": 1,
    "data_tier": 2,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.909910+00:00",
        "confidence": 0.79,
        "raw_name": "Qwen3 Coder 30B A3B Instruct",
        "raw_scores": {
          "intelligence_score": 17.48179323717596,
          "coding_score": 19.36,
          "context_window": 262144,
          "gdpval": 758.0962693078651,
          "terminalbench_hard": 0.152,
          "tau2": 0.345,
          "lcr": 29.0,
          "hle": 4.0,
          "gpqa": 51.6,
          "scicode": 27.8,
          "ifbench": 32.7,
          "aime25": 29.0,
          "critpt": 0.0,
          "livecodebench": 40.3
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 17.48179323717596,
        "coding_score": 19.36,
        "gdpval": 758.0962693078651,
        "terminalbench_hard": 0.152,
        "tau2": 0.345,
        "lcr": 29.0,
        "hle": 4.0,
        "gpqa": 51.6,
        "scicode": 27.8,
        "ifbench": 32.7,
        "aime25": 29.0,
        "critpt": 0.0,
        "livecodebench": 40.3
      }
    },
    "confidence_score": 0.79,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "qwen3-vl-235b",
    "canonical_name": "Qwen3 VL 235B A22B Instruct",
    "model_name": "Qwen3 VL 235B A22B Instruct",
    "aliases": [
      "Qwen3 VL 235B A22B Instruct",
      "qwen3-vl-235b-a22b-instruct",
      "qwen3-vl-235b-a22b-thinking"
    ],
    "provider": "Alibaba",
    "context_window": 262144,
    "open_source": true,
    "arena_votes": 11598,
    "license_type": "Apache 2.0",
    "creator": null,
    "intelligence_score": 23.2622,
    "coding_score": 16.51,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 1404.88,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": 676.3597,
    "terminalbench_hard": 0.068,
    "tau2": 0.351,
    "lcr": 31.7,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": 6.3,
    "gpqa": 71.2,
    "scicode": 35.9,
    "ifbench": 42.7,
    "aime25": 70.7,
    "critpt": 0.0,
    "mmmu_pro": 67.6,
    "livecodebench": 59.4,
    "sources": [
      "Artificial Analysis",
      "LMSYS Chatbot Arena"
    ],
    "source_count": 2,
    "data_tier": 1,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.909955+00:00",
        "confidence": 0.79,
        "raw_name": "Qwen3 VL 235B A22B Instruct",
        "raw_scores": {
          "intelligence_score": 23.262205287774076,
          "coding_score": 16.51,
          "context_window": 262144,
          "gdpval": 676.3596725930662,
          "terminalbench_hard": 0.068,
          "tau2": 0.351,
          "lcr": 31.7,
          "hle": 6.3,
          "gpqa": 71.2,
          "scicode": 35.9,
          "ifbench": 42.7,
          "aime25": 70.7,
          "critpt": 0.0,
          "mmmu_pro": 67.6,
          "livecodebench": 59.4
        }
      },
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.892062+00:00",
        "confidence": 1.0,
        "raw_name": "qwen3-vl-235b-a22b-instruct",
        "raw_scores": {
          "arena_elo": 1414.74,
          "arena_votes": 11598
        }
      },
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.892335+00:00",
        "confidence": 1.0,
        "raw_name": "qwen3-vl-235b-a22b-thinking",
        "raw_scores": {
          "arena_elo": 1395.02,
          "arena_votes": 7924
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 23.262205287774076,
        "coding_score": 16.51,
        "gdpval": 676.3596725930662,
        "terminalbench_hard": 0.068,
        "tau2": 0.351,
        "lcr": 31.7,
        "hle": 6.3,
        "gpqa": 71.2,
        "scicode": 35.9,
        "ifbench": 42.7,
        "aime25": 70.7,
        "critpt": 0.0,
        "mmmu_pro": 67.6,
        "livecodebench": 59.4
      },
      "LMSYS Chatbot Arena": {
        "arena_elo": 1414.74
      }
    },
    "confidence_score": 0.93,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "qwen3-235b-a22b-2507",
    "canonical_name": "Qwen3 235B A22B 2507 (Reasoning)",
    "model_name": "Qwen3 235B A22B 2507 (Reasoning)",
    "aliases": [
      "Qwen3 235B A22B 2507 (Reasoning)"
    ],
    "provider": "Alibaba",
    "context_window": 256000,
    "open_source": true,
    "arena_votes": null,
    "license_type": "Apache 2.0",
    "creator": null,
    "intelligence_score": 29.54,
    "coding_score": 23.21,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": null,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": 841.5053,
    "terminalbench_hard": 0.136,
    "tau2": 0.532,
    "lcr": 67.0,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": 15.0,
    "gpqa": 79.0,
    "scicode": 42.4,
    "ifbench": 51.2,
    "aime25": 91.0,
    "critpt": 0.0,
    "mmmu_pro": null,
    "livecodebench": 78.8,
    "sources": [
      "Artificial Analysis"
    ],
    "source_count": 1,
    "data_tier": 2,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.909994+00:00",
        "confidence": 0.79,
        "raw_name": "Qwen3 235B A22B 2507 (Reasoning)",
        "raw_scores": {
          "intelligence_score": 29.54,
          "coding_score": 23.21,
          "context_window": 256000,
          "gdpval": 841.5053200529005,
          "terminalbench_hard": 0.136,
          "tau2": 0.532,
          "lcr": 67.0,
          "hle": 15.0,
          "gpqa": 79.0,
          "scicode": 42.4,
          "ifbench": 51.2,
          "aime25": 91.0,
          "critpt": 0.0,
          "livecodebench": 78.8
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 29.54,
        "coding_score": 23.21,
        "gdpval": 841.5053200529005,
        "terminalbench_hard": 0.136,
        "tau2": 0.532,
        "lcr": 67.0,
        "hle": 15.0,
        "gpqa": 79.0,
        "scicode": 42.4,
        "ifbench": 51.2,
        "aime25": 91.0,
        "critpt": 0.0,
        "livecodebench": 78.8
      }
    },
    "confidence_score": 0.79,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "qwen3-next-80b-a3b",
    "canonical_name": "Qwen3 Next 80B A3B (Reasoning)",
    "model_name": "Qwen3 Next 80B A3B (Reasoning)",
    "aliases": [
      "Qwen3 Next 80B A3B (Reasoning)"
    ],
    "provider": "Alibaba",
    "context_window": 262144,
    "open_source": true,
    "arena_votes": null,
    "license_type": "Apache 2.0",
    "creator": null,
    "intelligence_score": 26.72,
    "coding_score": 19.49,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": null,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": 759.9653,
    "terminalbench_hard": 0.098,
    "tau2": 0.415,
    "lcr": 60.3,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": 11.7,
    "gpqa": 75.9,
    "scicode": 38.8,
    "ifbench": 60.7,
    "aime25": 84.3,
    "critpt": 0.0,
    "mmmu_pro": null,
    "livecodebench": 78.4,
    "sources": [
      "Artificial Analysis"
    ],
    "source_count": 1,
    "data_tier": 2,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.910034+00:00",
        "confidence": 0.79,
        "raw_name": "Qwen3 Next 80B A3B (Reasoning)",
        "raw_scores": {
          "intelligence_score": 26.72,
          "coding_score": 19.49,
          "context_window": 262144,
          "gdpval": 759.9653409123172,
          "terminalbench_hard": 0.098,
          "tau2": 0.415,
          "lcr": 60.3,
          "hle": 11.7,
          "gpqa": 75.9,
          "scicode": 38.8,
          "ifbench": 60.7,
          "aime25": 84.3,
          "critpt": 0.0,
          "livecodebench": 78.4
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 26.72,
        "coding_score": 19.49,
        "gdpval": 759.9653409123172,
        "terminalbench_hard": 0.098,
        "tau2": 0.415,
        "lcr": 60.3,
        "hle": 11.7,
        "gpqa": 75.9,
        "scicode": 38.8,
        "ifbench": 60.7,
        "aime25": 84.3,
        "critpt": 0.0,
        "livecodebench": 78.4
      }
    },
    "confidence_score": 0.79,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "qwen3-max",
    "canonical_name": "Qwen3 Max Thinking (Preview)",
    "model_name": "Qwen3 Max Thinking (Preview)",
    "aliases": [
      "Qwen3 Max",
      "Qwen3 Max (Preview)",
      "Qwen3 Max Thinking",
      "Qwen3 Max Thinking (Preview)",
      "qwen3-max-2025-09-23",
      "qwen3-max-preview"
    ],
    "provider": "Alibaba",
    "context_window": 262144,
    "open_source": null,
    "arena_votes": 27642,
    "license_type": "Proprietary",
    "creator": null,
    "intelligence_score": 32.179,
    "coding_score": 26.5877,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 1429.365,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": 2.4,
    "latency_seconds": 1.9017,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": 37.2228,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": 995.9898,
    "terminalbench_hard": 0.2031,
    "tau2": 0.68,
    "lcr": 52.0363,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": 14.2311,
    "gpqa": 78.872,
    "scicode": 39.1363,
    "ifbench": 53.5497,
    "aime25": 79.3333,
    "critpt": 0.6119,
    "mmmu_pro": null,
    "livecodebench": 65.1,
    "sources": [
      "Artificial Analysis",
      "LMSYS Chatbot Arena"
    ],
    "source_count": 2,
    "data_tier": 1,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.910078+00:00",
        "confidence": 1.0,
        "raw_name": "Qwen3 Max Thinking (Preview)",
        "raw_scores": {
          "intelligence_score": 32.48,
          "coding_score": 24.5,
          "blended_cost_per_1m": 2.4,
          "latency_seconds": 1.84,
          "tokens_per_second": 52.0,
          "context_window": 262144,
          "gdpval": 953.9090954714625,
          "terminalbench_hard": 0.174,
          "tau2": 0.836,
          "lcr": 57.7,
          "hle": 12.0,
          "gpqa": 77.6,
          "scicode": 38.7,
          "ifbench": 53.8,
          "aime25": 82.3,
          "critpt": 0.0,
          "livecodebench": 53.5
        }
      },
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.910487+00:00",
        "confidence": 1.0,
        "raw_name": "Qwen3 Max",
        "raw_scores": {
          "intelligence_score": 31.38,
          "coding_score": 26.41,
          "blended_cost_per_1m": 2.4,
          "latency_seconds": 2.27,
          "tokens_per_second": 27.0,
          "context_window": 262144,
          "gdpval": 1054.9455925547122,
          "terminalbench_hard": 0.205,
          "tau2": 0.743,
          "lcr": 46.7,
          "hle": 11.1,
          "gpqa": 76.4,
          "scicode": 38.3,
          "ifbench": 44.1,
          "aime25": 80.7,
          "critpt": 0.0,
          "livecodebench": 76.7
        }
      },
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.910758+00:00",
        "confidence": 0.8600000000000001,
        "raw_name": "Qwen3 Max Thinking",
        "raw_scores": {
          "intelligence_score": 39.85,
          "coding_score": 30.51,
          "blended_cost_per_1m": 2.4,
          "latency_seconds": 1.71,
          "tokens_per_second": 38.0,
          "context_window": 256000,
          "gdpval": 1150.2972123334084,
          "terminalbench_hard": 0.242,
          "tau2": 0.836,
          "lcr": 66.0,
          "hle": 26.2,
          "gpqa": 86.1,
          "scicode": 43.1,
          "ifbench": 70.7,
          "critpt": 1.7
        }
      },
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.919076+00:00",
        "confidence": 1.0,
        "raw_name": "Qwen3 Max (Preview)",
        "raw_scores": {
          "intelligence_score": 26.08,
          "coding_score": 25.48,
          "blended_cost_per_1m": 2.4,
          "latency_seconds": 1.76,
          "tokens_per_second": 32.0,
          "context_window": 262144,
          "gdpval": 846.4101964151353,
          "terminalbench_hard": 0.197,
          "tau2": 0.327,
          "lcr": 39.7,
          "hle": 9.3,
          "gpqa": 76.4,
          "scicode": 37.0,
          "ifbench": 48.0,
          "aime25": 75.0,
          "critpt": 0.9,
          "livecodebench": 65.1
        }
      },
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.891710+00:00",
        "confidence": 1.0,
        "raw_name": "qwen3-max-preview",
        "raw_scores": {
          "arena_elo": 1434.2,
          "arena_votes": 27642
        }
      },
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.891833+00:00",
        "confidence": 1.0,
        "raw_name": "qwen3-max-2025-09-23",
        "raw_scores": {
          "arena_elo": 1424.53,
          "arena_votes": 9170
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 32.48,
        "coding_score": 24.5,
        "gdpval": 953.9090954714625,
        "terminalbench_hard": 0.174,
        "tau2": 0.836,
        "lcr": 57.7,
        "hle": 12.0,
        "gpqa": 77.6,
        "scicode": 38.7,
        "ifbench": 53.8,
        "aime25": 82.3,
        "critpt": 0.0,
        "livecodebench": 53.5
      },
      "LMSYS Chatbot Arena": {
        "arena_elo": 1434.2
      }
    },
    "confidence_score": 0.9767,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "qwen3-vl-235b-a22b",
    "canonical_name": "Qwen3 VL 235B A22B (Reasoning)",
    "model_name": "Qwen3 VL 235B A22B (Reasoning)",
    "aliases": [
      "Qwen3 VL 235B A22B (Reasoning)"
    ],
    "provider": "Alibaba",
    "context_window": 262144,
    "open_source": true,
    "arena_votes": null,
    "license_type": "Apache 2.0",
    "creator": null,
    "intelligence_score": 27.64,
    "coding_score": 20.89,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": null,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": 746.5249,
    "terminalbench_hard": 0.114,
    "tau2": 0.541,
    "lcr": 58.7,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": 10.1,
    "gpqa": 77.2,
    "scicode": 39.9,
    "ifbench": 56.5,
    "aime25": 88.3,
    "critpt": 0.0,
    "mmmu_pro": 68.7,
    "livecodebench": 64.6,
    "sources": [
      "Artificial Analysis"
    ],
    "source_count": 1,
    "data_tier": 2,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.910119+00:00",
        "confidence": 0.79,
        "raw_name": "Qwen3 VL 235B A22B (Reasoning)",
        "raw_scores": {
          "intelligence_score": 27.64,
          "coding_score": 20.89,
          "context_window": 262144,
          "gdpval": 746.5248516527162,
          "terminalbench_hard": 0.114,
          "tau2": 0.541,
          "lcr": 58.7,
          "hle": 10.1,
          "gpqa": 77.2,
          "scicode": 39.9,
          "ifbench": 56.5,
          "aime25": 88.3,
          "critpt": 0.0,
          "mmmu_pro": 68.7,
          "livecodebench": 64.6
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 27.64,
        "coding_score": 20.89,
        "gdpval": 746.5248516527162,
        "terminalbench_hard": 0.114,
        "tau2": 0.541,
        "lcr": 58.7,
        "hle": 10.1,
        "gpqa": 77.2,
        "scicode": 39.9,
        "ifbench": 56.5,
        "aime25": 88.3,
        "critpt": 0.0,
        "mmmu_pro": 68.7,
        "livecodebench": 64.6
      }
    },
    "confidence_score": 0.79,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "qwen3-vl-8b",
    "canonical_name": "Qwen3 VL 8B (Reasoning)",
    "model_name": "Qwen3 VL 8B (Reasoning)",
    "aliases": [
      "Qwen3 VL 8B (Reasoning)"
    ],
    "provider": "Alibaba",
    "context_window": 256000,
    "open_source": true,
    "arena_votes": null,
    "license_type": "Apache 2.0",
    "creator": null,
    "intelligence_score": 16.66,
    "coding_score": 9.82,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": null,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": 700.9312,
    "terminalbench_hard": 0.038,
    "tau2": 0.225,
    "lcr": 31.0,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": 3.3,
    "gpqa": 57.9,
    "scicode": 21.9,
    "ifbench": 39.9,
    "aime25": 30.7,
    "critpt": 0.3,
    "mmmu_pro": 56.6,
    "livecodebench": 35.3,
    "sources": [
      "Artificial Analysis"
    ],
    "source_count": 1,
    "data_tier": 2,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.910161+00:00",
        "confidence": 0.79,
        "raw_name": "Qwen3 VL 8B (Reasoning)",
        "raw_scores": {
          "intelligence_score": 16.66,
          "coding_score": 9.82,
          "context_window": 256000,
          "gdpval": 700.9312187119397,
          "terminalbench_hard": 0.038,
          "tau2": 0.225,
          "lcr": 31.0,
          "hle": 3.3,
          "gpqa": 57.9,
          "scicode": 21.9,
          "ifbench": 39.9,
          "aime25": 30.7,
          "critpt": 0.3,
          "mmmu_pro": 56.6,
          "livecodebench": 35.3
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 16.66,
        "coding_score": 9.82,
        "gdpval": 700.9312187119397,
        "terminalbench_hard": 0.038,
        "tau2": 0.225,
        "lcr": 31.0,
        "hle": 3.3,
        "gpqa": 57.9,
        "scicode": 21.9,
        "ifbench": 39.9,
        "aime25": 30.7,
        "critpt": 0.3,
        "mmmu_pro": 56.6,
        "livecodebench": 35.3
      }
    },
    "confidence_score": 0.79,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "qwen3-vl-8b-instruct",
    "canonical_name": "Qwen3 VL 8B Instruct",
    "model_name": "Qwen3 VL 8B Instruct",
    "aliases": [
      "Qwen3 VL 8B Instruct"
    ],
    "provider": "Alibaba",
    "context_window": 256000,
    "open_source": true,
    "arena_votes": null,
    "license_type": "Apache 2.0",
    "creator": null,
    "intelligence_score": 14.8095,
    "coding_score": 7.3,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": null,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": 719.5902,
    "terminalbench_hard": 0.023,
    "tau2": 0.292,
    "lcr": 15.3,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": 2.9,
    "gpqa": 42.7,
    "scicode": 17.4,
    "ifbench": 32.3,
    "aime25": 27.3,
    "critpt": 0.0,
    "mmmu_pro": 47.3,
    "livecodebench": 33.2,
    "sources": [
      "Artificial Analysis"
    ],
    "source_count": 1,
    "data_tier": 2,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.910216+00:00",
        "confidence": 0.79,
        "raw_name": "Qwen3 VL 8B Instruct",
        "raw_scores": {
          "intelligence_score": 14.809516085602741,
          "coding_score": 7.3,
          "context_window": 256000,
          "gdpval": 719.5901853438301,
          "terminalbench_hard": 0.023,
          "tau2": 0.292,
          "lcr": 15.3,
          "hle": 2.9,
          "gpqa": 42.7,
          "scicode": 17.4,
          "ifbench": 32.3,
          "aime25": 27.3,
          "critpt": 0.0,
          "mmmu_pro": 47.3,
          "livecodebench": 33.2
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 14.809516085602741,
        "coding_score": 7.3,
        "gdpval": 719.5901853438301,
        "terminalbench_hard": 0.023,
        "tau2": 0.292,
        "lcr": 15.3,
        "hle": 2.9,
        "gpqa": 42.7,
        "scicode": 17.4,
        "ifbench": 32.3,
        "aime25": 27.3,
        "critpt": 0.0,
        "mmmu_pro": 47.3,
        "livecodebench": 33.2
      }
    },
    "confidence_score": 0.79,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "qwen3-coder-next",
    "canonical_name": "Qwen3 Coder Next",
    "model_name": "Qwen3 Coder Next",
    "aliases": [
      "Qwen3 Coder Next"
    ],
    "provider": "Alibaba",
    "context_window": 256000,
    "open_source": true,
    "arena_votes": null,
    "license_type": "Apache 2.0",
    "creator": null,
    "intelligence_score": 28.28,
    "coding_score": 22.89,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": null,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": 0.53,
    "latency_seconds": 0.8,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": 124.0,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": 947.9652,
    "terminalbench_hard": 0.182,
    "tau2": 0.795,
    "lcr": 40.0,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": 9.3,
    "gpqa": 73.7,
    "scicode": 32.3,
    "ifbench": 35.2,
    "aime25": null,
    "critpt": 0.0,
    "mmmu_pro": null,
    "livecodebench": null,
    "sources": [
      "Artificial Analysis"
    ],
    "source_count": 1,
    "data_tier": 2,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.910254+00:00",
        "confidence": 0.8600000000000001,
        "raw_name": "Qwen3 Coder Next",
        "raw_scores": {
          "intelligence_score": 28.28,
          "coding_score": 22.89,
          "blended_cost_per_1m": 0.53,
          "latency_seconds": 0.8,
          "tokens_per_second": 124.0,
          "context_window": 256000,
          "gdpval": 947.9652094977364,
          "terminalbench_hard": 0.182,
          "tau2": 0.795,
          "lcr": 40.0,
          "hle": 9.3,
          "gpqa": 73.7,
          "scicode": 32.3,
          "ifbench": 35.2,
          "critpt": 0.0
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 28.28,
        "coding_score": 22.89,
        "gdpval": 947.9652094977364,
        "terminalbench_hard": 0.182,
        "tau2": 0.795,
        "lcr": 40.0,
        "hle": 9.3,
        "gpqa": 73.7,
        "scicode": 32.3,
        "ifbench": 35.2,
        "critpt": 0.0
      }
    },
    "confidence_score": 0.86,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "qwen3-vl-30b-a3b-instruct",
    "canonical_name": "Qwen3 VL 30B A3B Instruct",
    "model_name": "Qwen3 VL 30B A3B Instruct",
    "aliases": [
      "Qwen3 VL 30B A3B Instruct"
    ],
    "provider": "Alibaba",
    "context_window": 256000,
    "open_source": true,
    "arena_votes": null,
    "license_type": "Apache 2.0",
    "creator": null,
    "intelligence_score": 20.0063,
    "coding_score": 14.3,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": null,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": 538.5619,
    "terminalbench_hard": 0.061,
    "tau2": 0.19,
    "lcr": 23.7,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": 6.4,
    "gpqa": 69.5,
    "scicode": 30.8,
    "ifbench": 33.1,
    "aime25": 72.3,
    "critpt": 0.0,
    "mmmu_pro": 62.1,
    "livecodebench": 47.6,
    "sources": [
      "Artificial Analysis"
    ],
    "source_count": 1,
    "data_tier": 2,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.910299+00:00",
        "confidence": 0.79,
        "raw_name": "Qwen3 VL 30B A3B Instruct",
        "raw_scores": {
          "intelligence_score": 20.00625456480055,
          "coding_score": 14.3,
          "context_window": 256000,
          "gdpval": 538.561863822116,
          "terminalbench_hard": 0.061,
          "tau2": 0.19,
          "lcr": 23.7,
          "hle": 6.4,
          "gpqa": 69.5,
          "scicode": 30.8,
          "ifbench": 33.1,
          "aime25": 72.3,
          "critpt": 0.0,
          "mmmu_pro": 62.1,
          "livecodebench": 47.6
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 20.00625456480055,
        "coding_score": 14.3,
        "gdpval": 538.561863822116,
        "terminalbench_hard": 0.061,
        "tau2": 0.19,
        "lcr": 23.7,
        "hle": 6.4,
        "gpqa": 69.5,
        "scicode": 30.8,
        "ifbench": 33.1,
        "aime25": 72.3,
        "critpt": 0.0,
        "mmmu_pro": 62.1,
        "livecodebench": 47.6
      }
    },
    "confidence_score": 0.79,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "qwen3-vl-30b-a3b",
    "canonical_name": "Qwen3 VL 30B A3B (Reasoning)",
    "model_name": "Qwen3 VL 30B A3B (Reasoning)",
    "aliases": [
      "Qwen3 VL 30B A3B (Reasoning)"
    ],
    "provider": "Alibaba",
    "context_window": 256000,
    "open_source": true,
    "arena_votes": null,
    "license_type": "Apache 2.0",
    "creator": null,
    "intelligence_score": 19.68,
    "coding_score": 13.14,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": null,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": 705.8677,
    "terminalbench_hard": 0.053,
    "tau2": 0.199,
    "lcr": 40.7,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": 8.7,
    "gpqa": 72.0,
    "scicode": 28.8,
    "ifbench": 45.1,
    "aime25": 82.3,
    "critpt": 0.0,
    "mmmu_pro": 61.8,
    "livecodebench": 69.7,
    "sources": [
      "Artificial Analysis"
    ],
    "source_count": 1,
    "data_tier": 2,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.910341+00:00",
        "confidence": 0.79,
        "raw_name": "Qwen3 VL 30B A3B (Reasoning)",
        "raw_scores": {
          "intelligence_score": 19.68,
          "coding_score": 13.14,
          "context_window": 256000,
          "gdpval": 705.8677321471047,
          "terminalbench_hard": 0.053,
          "tau2": 0.199,
          "lcr": 40.7,
          "hle": 8.7,
          "gpqa": 72.0,
          "scicode": 28.8,
          "ifbench": 45.1,
          "aime25": 82.3,
          "critpt": 0.0,
          "mmmu_pro": 61.8,
          "livecodebench": 69.7
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 19.68,
        "coding_score": 13.14,
        "gdpval": 705.8677321471047,
        "terminalbench_hard": 0.053,
        "tau2": 0.199,
        "lcr": 40.7,
        "hle": 8.7,
        "gpqa": 72.0,
        "scicode": 28.8,
        "ifbench": 45.1,
        "aime25": 82.3,
        "critpt": 0.0,
        "mmmu_pro": 61.8,
        "livecodebench": 69.7
      }
    },
    "confidence_score": 0.79,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "qwen3-vl-32b-instruct",
    "canonical_name": "Qwen3 VL 32B Instruct",
    "model_name": "Qwen3 VL 32B Instruct",
    "aliases": [
      "Qwen3 VL 32B Instruct"
    ],
    "provider": "Alibaba",
    "context_window": 256000,
    "open_source": true,
    "arena_votes": null,
    "license_type": "Apache 2.0",
    "creator": null,
    "intelligence_score": 21.3879,
    "coding_score": 15.59,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": null,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": 348.8181,
    "terminalbench_hard": 0.083,
    "tau2": 0.292,
    "lcr": 31.3,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": 6.3,
    "gpqa": 67.1,
    "scicode": 30.1,
    "ifbench": 39.2,
    "aime25": 68.3,
    "critpt": 0.0,
    "mmmu_pro": 64.3,
    "livecodebench": 51.4,
    "sources": [
      "Artificial Analysis"
    ],
    "source_count": 1,
    "data_tier": 2,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.910383+00:00",
        "confidence": 0.79,
        "raw_name": "Qwen3 VL 32B Instruct",
        "raw_scores": {
          "intelligence_score": 21.387891687554983,
          "coding_score": 15.59,
          "context_window": 256000,
          "gdpval": 348.8180583638559,
          "terminalbench_hard": 0.083,
          "tau2": 0.292,
          "lcr": 31.3,
          "hle": 6.3,
          "gpqa": 67.1,
          "scicode": 30.1,
          "ifbench": 39.2,
          "aime25": 68.3,
          "critpt": 0.0,
          "mmmu_pro": 64.3,
          "livecodebench": 51.4
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 21.387891687554983,
        "coding_score": 15.59,
        "gdpval": 348.8180583638559,
        "terminalbench_hard": 0.083,
        "tau2": 0.292,
        "lcr": 31.3,
        "hle": 6.3,
        "gpqa": 67.1,
        "scicode": 30.1,
        "ifbench": 39.2,
        "aime25": 68.3,
        "critpt": 0.0,
        "mmmu_pro": 64.3,
        "livecodebench": 51.4
      }
    },
    "confidence_score": 0.79,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "qwen-chat-14b",
    "canonical_name": "Qwen Chat 14B",
    "model_name": "Qwen Chat 14B",
    "aliases": [
      "Qwen Chat 14B"
    ],
    "provider": "Alibaba",
    "context_window": 8192,
    "open_source": null,
    "arena_votes": null,
    "license_type": null,
    "creator": null,
    "intelligence_score": 7.4139,
    "coding_score": null,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": null,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": 0.0,
    "latency_seconds": 0.0,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": 0.0,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": null,
    "tau2": null,
    "lcr": null,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": null,
    "gpqa": null,
    "scicode": null,
    "ifbench": null,
    "aime25": null,
    "critpt": null,
    "mmmu_pro": null,
    "livecodebench": null,
    "sources": [
      "Artificial Analysis"
    ],
    "source_count": 1,
    "data_tier": 2,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.910405+00:00",
        "confidence": 0.65,
        "raw_name": "Qwen Chat 14B",
        "raw_scores": {
          "intelligence_score": 7.413887665229345,
          "blended_cost_per_1m": 0.0,
          "latency_seconds": 0.0,
          "tokens_per_second": 0.0,
          "context_window": 8192
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 7.413887665229345
      }
    },
    "confidence_score": 0.65,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "qwen3-30b-a3b-2507-instruct",
    "canonical_name": "Qwen3 30B A3B 2507 Instruct",
    "model_name": "Qwen3 30B A3B 2507 Instruct",
    "aliases": [
      "Qwen3 30B A3B 2507 Instruct"
    ],
    "provider": "Alibaba",
    "context_window": 262144,
    "open_source": true,
    "arena_votes": null,
    "license_type": "Apache 2.0",
    "creator": null,
    "intelligence_score": 19.2623,
    "coding_score": 14.19,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": null,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": 554.6581,
    "terminalbench_hard": 0.061,
    "tau2": 0.102,
    "lcr": 22.7,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": 6.8,
    "gpqa": 65.9,
    "scicode": 30.4,
    "ifbench": 33.1,
    "aime25": 66.3,
    "critpt": 0.0,
    "mmmu_pro": null,
    "livecodebench": 51.5,
    "sources": [
      "Artificial Analysis"
    ],
    "source_count": 1,
    "data_tier": 2,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.910444+00:00",
        "confidence": 0.79,
        "raw_name": "Qwen3 30B A3B 2507 Instruct",
        "raw_scores": {
          "intelligence_score": 19.262302603890806,
          "coding_score": 14.19,
          "context_window": 262144,
          "gdpval": 554.658076858799,
          "terminalbench_hard": 0.061,
          "tau2": 0.102,
          "lcr": 22.7,
          "hle": 6.8,
          "gpqa": 65.9,
          "scicode": 30.4,
          "ifbench": 33.1,
          "aime25": 66.3,
          "critpt": 0.0,
          "livecodebench": 51.5
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 19.262302603890806,
        "coding_score": 14.19,
        "gdpval": 554.658076858799,
        "terminalbench_hard": 0.061,
        "tau2": 0.102,
        "lcr": 22.7,
        "hle": 6.8,
        "gpqa": 65.9,
        "scicode": 30.4,
        "ifbench": 33.1,
        "aime25": 66.3,
        "critpt": 0.0,
        "livecodebench": 51.5
      }
    },
    "confidence_score": 0.79,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "qwen3-30b-a3b-2507",
    "canonical_name": "Qwen3 30B A3B 2507 (Reasoning)",
    "model_name": "Qwen3 30B A3B 2507 (Reasoning)",
    "aliases": [
      "Qwen3 30B A3B 2507 (Reasoning)"
    ],
    "provider": "Alibaba",
    "context_window": 262144,
    "open_source": true,
    "arena_votes": null,
    "license_type": "Apache 2.0",
    "creator": null,
    "intelligence_score": 24.6979,
    "coding_score": 14.65,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": null,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": 706.148,
    "terminalbench_hard": 0.053,
    "tau2": 0.281,
    "lcr": 59.0,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": 9.8,
    "gpqa": 70.7,
    "scicode": 33.3,
    "ifbench": 50.7,
    "aime25": 56.3,
    "critpt": 0.3,
    "mmmu_pro": null,
    "livecodebench": 70.7,
    "sources": [
      "Artificial Analysis"
    ],
    "source_count": 1,
    "data_tier": 2,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.910527+00:00",
        "confidence": 0.79,
        "raw_name": "Qwen3 30B A3B 2507 (Reasoning)",
        "raw_scores": {
          "intelligence_score": 24.69794280466268,
          "coding_score": 14.65,
          "context_window": 262144,
          "gdpval": 706.1480487285149,
          "terminalbench_hard": 0.053,
          "tau2": 0.281,
          "lcr": 59.0,
          "hle": 9.8,
          "gpqa": 70.7,
          "scicode": 33.3,
          "ifbench": 50.7,
          "aime25": 56.3,
          "critpt": 0.3,
          "livecodebench": 70.7
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 24.69794280466268,
        "coding_score": 14.65,
        "gdpval": 706.1480487285149,
        "terminalbench_hard": 0.053,
        "tau2": 0.281,
        "lcr": 59.0,
        "hle": 9.8,
        "gpqa": 70.7,
        "scicode": 33.3,
        "ifbench": 50.7,
        "aime25": 56.3,
        "critpt": 0.3,
        "livecodebench": 70.7
      }
    },
    "confidence_score": 0.79,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "qwen3-4b-2507",
    "canonical_name": "Qwen3 4B 2507 (Reasoning)",
    "model_name": "Qwen3 4B 2507 (Reasoning)",
    "aliases": [
      "Qwen3 4B 2507 (Reasoning)"
    ],
    "provider": "Alibaba",
    "context_window": 262144,
    "open_source": true,
    "arena_votes": null,
    "license_type": "Apache 2.0",
    "creator": null,
    "intelligence_score": 22.7803,
    "coding_score": 9.54,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": null,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": 633.3049,
    "terminalbench_hard": 0.015,
    "tau2": 0.254,
    "lcr": 37.7,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": 5.9,
    "gpqa": 66.7,
    "scicode": 25.6,
    "ifbench": 49.8,
    "aime25": 82.7,
    "critpt": 0.0,
    "mmmu_pro": null,
    "livecodebench": 64.1,
    "sources": [
      "Artificial Analysis"
    ],
    "source_count": 1,
    "data_tier": 2,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.910566+00:00",
        "confidence": 0.79,
        "raw_name": "Qwen3 4B 2507 (Reasoning)",
        "raw_scores": {
          "intelligence_score": 22.780273245134985,
          "coding_score": 9.54,
          "context_window": 262144,
          "gdpval": 633.304908109346,
          "terminalbench_hard": 0.015,
          "tau2": 0.254,
          "lcr": 37.7,
          "hle": 5.9,
          "gpqa": 66.7,
          "scicode": 25.6,
          "ifbench": 49.8,
          "aime25": 82.7,
          "critpt": 0.0,
          "livecodebench": 64.1
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 22.780273245134985,
        "coding_score": 9.54,
        "gdpval": 633.304908109346,
        "terminalbench_hard": 0.015,
        "tau2": 0.254,
        "lcr": 37.7,
        "hle": 5.9,
        "gpqa": 66.7,
        "scicode": 25.6,
        "ifbench": 49.8,
        "aime25": 82.7,
        "critpt": 0.0,
        "livecodebench": 64.1
      }
    },
    "confidence_score": 0.79,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "qwen3-17b",
    "canonical_name": "Qwen3 1.7B (Non-reasoning)",
    "model_name": "Qwen3 1.7B (Non-reasoning)",
    "aliases": [
      "Qwen3 1.7B (Non-reasoning)",
      "Qwen3 1.7B (Reasoning)"
    ],
    "provider": "Alibaba",
    "context_window": 32000,
    "open_source": true,
    "arena_votes": null,
    "license_type": "Apache 2.0",
    "creator": null,
    "intelligence_score": 11.8243,
    "coding_score": 1.87,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": null,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": 315.3887,
    "terminalbench_hard": 0.0,
    "tau2": 0.238,
    "lcr": 0.0,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": 5.0,
    "gpqa": 31.95,
    "scicode": 5.6,
    "ifbench": 24.0,
    "aime25": 23.0,
    "critpt": 0.0,
    "mmmu_pro": null,
    "livecodebench": 21.7,
    "sources": [
      "Artificial Analysis"
    ],
    "source_count": 1,
    "data_tier": 2,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.910608+00:00",
        "confidence": 0.79,
        "raw_name": "Qwen3 1.7B (Non-reasoning)",
        "raw_scores": {
          "intelligence_score": 10.580206592118074,
          "coding_score": 2.31,
          "context_window": 32000,
          "gdpval": 306.1725344776787,
          "terminalbench_hard": 0.0,
          "tau2": 0.216,
          "lcr": 0.0,
          "hle": 5.2,
          "gpqa": 28.3,
          "scicode": 6.9,
          "ifbench": 21.1,
          "aime25": 7.3,
          "critpt": 0.0,
          "livecodebench": 12.6
        }
      },
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.910648+00:00",
        "confidence": 0.79,
        "raw_name": "Qwen3 1.7B (Reasoning)",
        "raw_scores": {
          "intelligence_score": 13.068306567976027,
          "coding_score": 1.43,
          "context_window": 32000,
          "gdpval": 324.60494886594734,
          "terminalbench_hard": 0.0,
          "tau2": 0.26,
          "lcr": 0.0,
          "hle": 4.8,
          "gpqa": 35.6,
          "scicode": 4.3,
          "ifbench": 26.9,
          "aime25": 38.7,
          "critpt": 0.0,
          "livecodebench": 30.8
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 10.580206592118074,
        "coding_score": 2.31,
        "gdpval": 306.1725344776787,
        "terminalbench_hard": 0.0,
        "tau2": 0.216,
        "lcr": 0.0,
        "hle": 5.2,
        "gpqa": 28.3,
        "scicode": 6.9,
        "ifbench": 21.1,
        "aime25": 7.3,
        "critpt": 0.0,
        "livecodebench": 12.6
      }
    },
    "confidence_score": 0.79,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "qwen3-omni-30b-a3b-instruct",
    "canonical_name": "Qwen3 Omni 30B A3B Instruct",
    "model_name": "Qwen3 Omni 30B A3B Instruct",
    "aliases": [
      "Qwen3 Omni 30B A3B Instruct"
    ],
    "provider": "Alibaba",
    "context_window": 65536,
    "open_source": true,
    "arena_votes": null,
    "license_type": "Apache 2.0",
    "creator": null,
    "intelligence_score": 16.0605,
    "coding_score": 7.22,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": null,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": 343.9466,
    "terminalbench_hard": 0.015,
    "tau2": 0.164,
    "lcr": 0.0,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": 5.1,
    "gpqa": 62.0,
    "scicode": 18.6,
    "ifbench": 31.2,
    "aime25": 52.3,
    "critpt": 0.0,
    "mmmu_pro": 55.5,
    "livecodebench": 42.2,
    "sources": [
      "Artificial Analysis"
    ],
    "source_count": 1,
    "data_tier": 2,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.910799+00:00",
        "confidence": 0.79,
        "raw_name": "Qwen3 Omni 30B A3B Instruct",
        "raw_scores": {
          "intelligence_score": 16.06045361124703,
          "coding_score": 7.22,
          "context_window": 65536,
          "gdpval": 343.9465557694127,
          "terminalbench_hard": 0.015,
          "tau2": 0.164,
          "lcr": 0.0,
          "hle": 5.1,
          "gpqa": 62.0,
          "scicode": 18.6,
          "ifbench": 31.2,
          "aime25": 52.3,
          "critpt": 0.0,
          "mmmu_pro": 55.5,
          "livecodebench": 42.2
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 16.06045361124703,
        "coding_score": 7.22,
        "gdpval": 343.9465557694127,
        "terminalbench_hard": 0.015,
        "tau2": 0.164,
        "lcr": 0.0,
        "hle": 5.1,
        "gpqa": 62.0,
        "scicode": 18.6,
        "ifbench": 31.2,
        "aime25": 52.3,
        "critpt": 0.0,
        "mmmu_pro": 55.5,
        "livecodebench": 42.2
      }
    },
    "confidence_score": 0.79,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "qwen3-vl-4b-instruct",
    "canonical_name": "Qwen3 VL 4B Instruct",
    "model_name": "Qwen3 VL 4B Instruct",
    "aliases": [
      "Qwen3 VL 4B Instruct"
    ],
    "provider": "Alibaba",
    "context_window": 256000,
    "open_source": true,
    "arena_votes": null,
    "license_type": "Apache 2.0",
    "creator": null,
    "intelligence_score": 14.0786,
    "coding_score": 4.55,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": null,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": 400.1726,
    "terminalbench_hard": 0.0,
    "tau2": 0.234,
    "lcr": 13.0,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": 3.7,
    "gpqa": 37.1,
    "scicode": 13.7,
    "ifbench": 31.8,
    "aime25": 37.0,
    "critpt": 0.0,
    "mmmu_pro": 43.9,
    "livecodebench": 29.0,
    "sources": [
      "Artificial Analysis"
    ],
    "source_count": 1,
    "data_tier": 2,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.910841+00:00",
        "confidence": 0.79,
        "raw_name": "Qwen3 VL 4B Instruct",
        "raw_scores": {
          "intelligence_score": 14.078586469580603,
          "coding_score": 4.55,
          "context_window": 256000,
          "gdpval": 400.1725964089359,
          "terminalbench_hard": 0.0,
          "tau2": 0.234,
          "lcr": 13.0,
          "hle": 3.7,
          "gpqa": 37.1,
          "scicode": 13.7,
          "ifbench": 31.8,
          "aime25": 37.0,
          "critpt": 0.0,
          "mmmu_pro": 43.9,
          "livecodebench": 29.0
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 14.078586469580603,
        "coding_score": 4.55,
        "gdpval": 400.1725964089359,
        "terminalbench_hard": 0.0,
        "tau2": 0.234,
        "lcr": 13.0,
        "hle": 3.7,
        "gpqa": 37.1,
        "scicode": 13.7,
        "ifbench": 31.8,
        "aime25": 37.0,
        "critpt": 0.0,
        "mmmu_pro": 43.9,
        "livecodebench": 29.0
      }
    },
    "confidence_score": 0.79,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "qwen35-35b-a3b",
    "canonical_name": "Qwen3.5 35B A3B (Reasoning)",
    "model_name": "Qwen3.5 35B A3B (Reasoning)",
    "aliases": [
      "Qwen3.5 35B A3B (Reasoning)"
    ],
    "provider": "Alibaba",
    "context_window": 262144,
    "open_source": true,
    "arena_votes": null,
    "license_type": "Apache 2.0",
    "creator": null,
    "intelligence_score": 37.12,
    "coding_score": 30.25,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": null,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": 928.6657,
    "terminalbench_hard": 0.265,
    "tau2": 0.892,
    "lcr": 62.7,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": 19.7,
    "gpqa": 84.5,
    "scicode": 37.7,
    "ifbench": 72.5,
    "aime25": null,
    "critpt": 0.9,
    "mmmu_pro": 72.7,
    "livecodebench": null,
    "sources": [
      "Artificial Analysis"
    ],
    "source_count": 1,
    "data_tier": 2,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.910876+00:00",
        "confidence": 0.65,
        "raw_name": "Qwen3.5 35B A3B (Reasoning)",
        "raw_scores": {
          "intelligence_score": 37.12,
          "coding_score": 30.25,
          "context_window": 262144,
          "gdpval": 928.6656807468282,
          "terminalbench_hard": 0.265,
          "tau2": 0.892,
          "lcr": 62.7,
          "hle": 19.7,
          "gpqa": 84.5,
          "scicode": 37.7,
          "ifbench": 72.5,
          "critpt": 0.9,
          "mmmu_pro": 72.7
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 37.12,
        "coding_score": 30.25,
        "gdpval": 928.6656807468282,
        "terminalbench_hard": 0.265,
        "tau2": 0.892,
        "lcr": 62.7,
        "hle": 19.7,
        "gpqa": 84.5,
        "scicode": 37.7,
        "ifbench": 72.5,
        "critpt": 0.9,
        "mmmu_pro": 72.7
      }
    },
    "confidence_score": 0.65,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "qwen3-vl-32b",
    "canonical_name": "Qwen3 VL 32B (Reasoning)",
    "model_name": "Qwen3 VL 32B (Reasoning)",
    "aliases": [
      "Qwen3 VL 32B (Reasoning)"
    ],
    "provider": "Alibaba",
    "context_window": 256000,
    "open_source": true,
    "arena_votes": null,
    "license_type": "Apache 2.0",
    "creator": null,
    "intelligence_score": 24.72,
    "coding_score": 14.54,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": null,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": 704.5281,
    "terminalbench_hard": 0.076,
    "tau2": 0.456,
    "lcr": 55.3,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": 9.6,
    "gpqa": 73.3,
    "scicode": 28.5,
    "ifbench": 59.4,
    "aime25": 84.7,
    "critpt": 0.0,
    "mmmu_pro": 63.4,
    "livecodebench": 73.8,
    "sources": [
      "Artificial Analysis"
    ],
    "source_count": 1,
    "data_tier": 2,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.910917+00:00",
        "confidence": 0.79,
        "raw_name": "Qwen3 VL 32B (Reasoning)",
        "raw_scores": {
          "intelligence_score": 24.72,
          "coding_score": 14.54,
          "context_window": 256000,
          "gdpval": 704.5280947668384,
          "terminalbench_hard": 0.076,
          "tau2": 0.456,
          "lcr": 55.3,
          "hle": 9.6,
          "gpqa": 73.3,
          "scicode": 28.5,
          "ifbench": 59.4,
          "aime25": 84.7,
          "critpt": 0.0,
          "mmmu_pro": 63.4,
          "livecodebench": 73.8
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 24.72,
        "coding_score": 14.54,
        "gdpval": 704.5280947668384,
        "terminalbench_hard": 0.076,
        "tau2": 0.456,
        "lcr": 55.3,
        "hle": 9.6,
        "gpqa": 73.3,
        "scicode": 28.5,
        "ifbench": 59.4,
        "aime25": 84.7,
        "critpt": 0.0,
        "mmmu_pro": 63.4,
        "livecodebench": 73.8
      }
    },
    "confidence_score": 0.79,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "qwen3-vl-4b",
    "canonical_name": "Qwen3 VL 4B (Reasoning)",
    "model_name": "Qwen3 VL 4B (Reasoning)",
    "aliases": [
      "Qwen3 VL 4B (Reasoning)"
    ],
    "provider": "Alibaba",
    "context_window": 256000,
    "open_source": true,
    "arena_votes": null,
    "license_type": "Apache 2.0",
    "creator": null,
    "intelligence_score": 13.73,
    "coding_score": 6.72,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": null,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": 775.5468,
    "terminalbench_hard": 0.015,
    "tau2": 0.155,
    "lcr": 21.3,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": 4.4,
    "gpqa": 49.4,
    "scicode": 17.1,
    "ifbench": 36.6,
    "aime25": 25.7,
    "critpt": 0.0,
    "mmmu_pro": 52.0,
    "livecodebench": 32.0,
    "sources": [
      "Artificial Analysis"
    ],
    "source_count": 1,
    "data_tier": 2,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.910960+00:00",
        "confidence": 0.79,
        "raw_name": "Qwen3 VL 4B (Reasoning)",
        "raw_scores": {
          "intelligence_score": 13.73,
          "coding_score": 6.72,
          "context_window": 256000,
          "gdpval": 775.5467734090345,
          "terminalbench_hard": 0.015,
          "tau2": 0.155,
          "lcr": 21.3,
          "hle": 4.4,
          "gpqa": 49.4,
          "scicode": 17.1,
          "ifbench": 36.6,
          "aime25": 25.7,
          "critpt": 0.0,
          "mmmu_pro": 52.0,
          "livecodebench": 32.0
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 13.73,
        "coding_score": 6.72,
        "gdpval": 775.5467734090345,
        "terminalbench_hard": 0.015,
        "tau2": 0.155,
        "lcr": 21.3,
        "hle": 4.4,
        "gpqa": 49.4,
        "scicode": 17.1,
        "ifbench": 36.6,
        "aime25": 25.7,
        "critpt": 0.0,
        "mmmu_pro": 52.0,
        "livecodebench": 32.0
      }
    },
    "confidence_score": 0.79,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "qwen3-4b-2507-instruct",
    "canonical_name": "Qwen3 4B 2507 Instruct",
    "model_name": "Qwen3 4B 2507 Instruct",
    "aliases": [
      "Qwen3 4B 2507 Instruct"
    ],
    "provider": "Alibaba",
    "context_window": 262144,
    "open_source": true,
    "arena_votes": null,
    "license_type": "Apache 2.0",
    "creator": null,
    "intelligence_score": 16.1372,
    "coding_score": 9.05,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": null,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": 358.5604,
    "terminalbench_hard": 0.045,
    "tau2": 0.266,
    "lcr": 7.3,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": 4.7,
    "gpqa": 51.7,
    "scicode": 18.1,
    "ifbench": 33.5,
    "aime25": 52.3,
    "critpt": 0.0,
    "mmmu_pro": null,
    "livecodebench": 37.7,
    "sources": [
      "Artificial Analysis"
    ],
    "source_count": 1,
    "data_tier": 2,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.911001+00:00",
        "confidence": 0.79,
        "raw_name": "Qwen3 4B 2507 Instruct",
        "raw_scores": {
          "intelligence_score": 16.137244976777335,
          "coding_score": 9.05,
          "context_window": 262144,
          "gdpval": 358.56042904039805,
          "terminalbench_hard": 0.045,
          "tau2": 0.266,
          "lcr": 7.3,
          "hle": 4.7,
          "gpqa": 51.7,
          "scicode": 18.1,
          "ifbench": 33.5,
          "aime25": 52.3,
          "critpt": 0.0,
          "livecodebench": 37.7
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 16.137244976777335,
        "coding_score": 9.05,
        "gdpval": 358.56042904039805,
        "terminalbench_hard": 0.045,
        "tau2": 0.266,
        "lcr": 7.3,
        "hle": 4.7,
        "gpqa": 51.7,
        "scicode": 18.1,
        "ifbench": 33.5,
        "aime25": 52.3,
        "critpt": 0.0,
        "livecodebench": 37.7
      }
    },
    "confidence_score": 0.79,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "qwen3-omni-30b-a3b",
    "canonical_name": "Qwen3 Omni 30B A3B (Reasoning)",
    "model_name": "Qwen3 Omni 30B A3B (Reasoning)",
    "aliases": [
      "Qwen3 Omni 30B A3B (Reasoning)"
    ],
    "provider": "Alibaba",
    "context_window": 65536,
    "open_source": true,
    "arena_votes": null,
    "license_type": "Apache 2.0",
    "creator": null,
    "intelligence_score": 20.8341,
    "coding_score": 12.71,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": null,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": 545.2832,
    "terminalbench_hard": 0.038,
    "tau2": 0.213,
    "lcr": 0.0,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": 7.3,
    "gpqa": 72.6,
    "scicode": 30.6,
    "ifbench": 43.4,
    "aime25": 74.0,
    "critpt": 0.0,
    "mmmu_pro": 60.2,
    "livecodebench": 67.9,
    "sources": [
      "Artificial Analysis"
    ],
    "source_count": 1,
    "data_tier": 2,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.911041+00:00",
        "confidence": 0.79,
        "raw_name": "Qwen3 Omni 30B A3B (Reasoning)",
        "raw_scores": {
          "intelligence_score": 20.834076780566665,
          "coding_score": 12.71,
          "context_window": 65536,
          "gdpval": 545.2831633660874,
          "terminalbench_hard": 0.038,
          "tau2": 0.213,
          "lcr": 0.0,
          "hle": 7.3,
          "gpqa": 72.6,
          "scicode": 30.6,
          "ifbench": 43.4,
          "aime25": 74.0,
          "critpt": 0.0,
          "mmmu_pro": 60.2,
          "livecodebench": 67.9
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 20.834076780566665,
        "coding_score": 12.71,
        "gdpval": 545.2831633660874,
        "terminalbench_hard": 0.038,
        "tau2": 0.213,
        "lcr": 0.0,
        "hle": 7.3,
        "gpqa": 72.6,
        "scicode": 30.6,
        "ifbench": 43.4,
        "aime25": 74.0,
        "critpt": 0.0,
        "mmmu_pro": 60.2,
        "livecodebench": 67.9
      }
    },
    "confidence_score": 0.79,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "qwen35-122b-a10b",
    "canonical_name": "Qwen3.5 122B A10B (Reasoning)",
    "model_name": "Qwen3.5 122B A10B (Reasoning)",
    "aliases": [
      "Qwen3.5 122B A10B (Reasoning)"
    ],
    "provider": "Alibaba",
    "context_window": 262144,
    "open_source": true,
    "arena_votes": null,
    "license_type": "Apache 2.0",
    "creator": null,
    "intelligence_score": 41.6,
    "coding_score": 34.71,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": null,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": 1145.0005,
    "terminalbench_hard": 0.311,
    "tau2": 0.936,
    "lcr": 66.7,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": 23.4,
    "gpqa": 85.7,
    "scicode": 42.0,
    "ifbench": 75.7,
    "aime25": null,
    "critpt": 0.6,
    "mmmu_pro": 75.0,
    "livecodebench": null,
    "sources": [
      "Artificial Analysis"
    ],
    "source_count": 1,
    "data_tier": 2,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.911078+00:00",
        "confidence": 0.65,
        "raw_name": "Qwen3.5 122B A10B (Reasoning)",
        "raw_scores": {
          "intelligence_score": 41.6,
          "coding_score": 34.71,
          "context_window": 262144,
          "gdpval": 1145.000485676908,
          "terminalbench_hard": 0.311,
          "tau2": 0.936,
          "lcr": 66.7,
          "hle": 23.4,
          "gpqa": 85.7,
          "scicode": 42.0,
          "ifbench": 75.7,
          "critpt": 0.6,
          "mmmu_pro": 75.0
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 41.6,
        "coding_score": 34.71,
        "gdpval": 1145.000485676908,
        "terminalbench_hard": 0.311,
        "tau2": 0.936,
        "lcr": 66.7,
        "hle": 23.4,
        "gpqa": 85.7,
        "scicode": 42.0,
        "ifbench": 75.7,
        "critpt": 0.6,
        "mmmu_pro": 75.0
      }
    },
    "confidence_score": 0.65,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "qwen35-27b",
    "canonical_name": "Qwen3.5 27B (Reasoning)",
    "model_name": "Qwen3.5 27B (Reasoning)",
    "aliases": [
      "Qwen3.5 27B (Reasoning)"
    ],
    "provider": "Alibaba",
    "context_window": 262144,
    "open_source": true,
    "arena_votes": null,
    "license_type": "Apache 2.0",
    "creator": null,
    "intelligence_score": 42.07,
    "coding_score": 34.87,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": null,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": 1204.9155,
    "terminalbench_hard": 0.326,
    "tau2": 0.939,
    "lcr": 67.3,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": 22.2,
    "gpqa": 85.8,
    "scicode": 39.5,
    "ifbench": 75.6,
    "aime25": null,
    "critpt": 0.9,
    "mmmu_pro": 75.0,
    "livecodebench": null,
    "sources": [
      "Artificial Analysis"
    ],
    "source_count": 1,
    "data_tier": 2,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.911112+00:00",
        "confidence": 0.65,
        "raw_name": "Qwen3.5 27B (Reasoning)",
        "raw_scores": {
          "intelligence_score": 42.07,
          "coding_score": 34.87,
          "context_window": 262144,
          "gdpval": 1204.9154998870442,
          "terminalbench_hard": 0.326,
          "tau2": 0.939,
          "lcr": 67.3,
          "hle": 22.2,
          "gpqa": 85.8,
          "scicode": 39.5,
          "ifbench": 75.6,
          "critpt": 0.9,
          "mmmu_pro": 75.0
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 42.07,
        "coding_score": 34.87,
        "gdpval": 1204.9154998870442,
        "terminalbench_hard": 0.326,
        "tau2": 0.939,
        "lcr": 67.3,
        "hle": 22.2,
        "gpqa": 85.8,
        "scicode": 39.5,
        "ifbench": 75.6,
        "critpt": 0.9,
        "mmmu_pro": 75.0
      }
    },
    "confidence_score": 0.65,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "ring-1t",
    "canonical_name": "Ring-1T",
    "model_name": "Ring-1T",
    "aliases": [
      "Ring-1T"
    ],
    "provider": null,
    "context_window": 128000,
    "open_source": true,
    "arena_votes": null,
    "license_type": "MIT",
    "creator": null,
    "intelligence_score": 21.8519,
    "coding_score": 16.78,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": null,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": 0.0,
    "latency_seconds": 0.0,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": 0.0,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": 733.957,
    "terminalbench_hard": 0.068,
    "tau2": 0.263,
    "lcr": 45.7,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": 10.2,
    "gpqa": 77.4,
    "scicode": 36.7,
    "ifbench": 44.6,
    "aime25": 89.3,
    "critpt": 0.6,
    "mmmu_pro": null,
    "livecodebench": 64.3,
    "sources": [
      "Artificial Analysis"
    ],
    "source_count": 1,
    "data_tier": 2,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.911157+00:00",
        "confidence": 1.0,
        "raw_name": "Ring-1T",
        "raw_scores": {
          "intelligence_score": 21.851945421885013,
          "coding_score": 16.78,
          "blended_cost_per_1m": 0.0,
          "latency_seconds": 0.0,
          "tokens_per_second": 0.0,
          "context_window": 128000,
          "gdpval": 733.9569693675483,
          "terminalbench_hard": 0.068,
          "tau2": 0.263,
          "lcr": 45.7,
          "hle": 10.2,
          "gpqa": 77.4,
          "scicode": 36.7,
          "ifbench": 44.6,
          "aime25": 89.3,
          "critpt": 0.6,
          "livecodebench": 64.3
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 21.851945421885013,
        "coding_score": 16.78,
        "gdpval": 733.9569693675483,
        "terminalbench_hard": 0.068,
        "tau2": 0.263,
        "lcr": 45.7,
        "hle": 10.2,
        "gpqa": 77.4,
        "scicode": 36.7,
        "ifbench": 44.6,
        "aime25": 89.3,
        "critpt": 0.6,
        "livecodebench": 64.3
      }
    },
    "confidence_score": 1.0,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "ling-1t",
    "canonical_name": "Ling-1T",
    "model_name": "Ling-1T",
    "aliases": [
      "Ling-1T"
    ],
    "provider": null,
    "context_window": 128000,
    "open_source": true,
    "arena_votes": null,
    "license_type": "MIT",
    "creator": null,
    "intelligence_score": 23.6413,
    "coding_score": 18.8,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": null,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": 0.0,
    "latency_seconds": 0.0,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": 0.0,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": 456.701,
    "terminalbench_hard": 0.106,
    "tau2": 0.327,
    "lcr": 34.7,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": 7.2,
    "gpqa": 71.9,
    "scicode": 35.2,
    "ifbench": 34.8,
    "aime25": 71.3,
    "critpt": 0.6,
    "mmmu_pro": null,
    "livecodebench": 67.7,
    "sources": [
      "Artificial Analysis"
    ],
    "source_count": 1,
    "data_tier": 2,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.911218+00:00",
        "confidence": 1.0,
        "raw_name": "Ling-1T",
        "raw_scores": {
          "intelligence_score": 23.641294570517292,
          "coding_score": 18.8,
          "blended_cost_per_1m": 0.0,
          "latency_seconds": 0.0,
          "tokens_per_second": 0.0,
          "context_window": 128000,
          "gdpval": 456.7010207144617,
          "terminalbench_hard": 0.106,
          "tau2": 0.327,
          "lcr": 34.7,
          "hle": 7.2,
          "gpqa": 71.9,
          "scicode": 35.2,
          "ifbench": 34.8,
          "aime25": 71.3,
          "critpt": 0.6,
          "livecodebench": 67.7
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 23.641294570517292,
        "coding_score": 18.8,
        "gdpval": 456.7010207144617,
        "terminalbench_hard": 0.106,
        "tau2": 0.327,
        "lcr": 34.7,
        "hle": 7.2,
        "gpqa": 71.9,
        "scicode": 35.2,
        "ifbench": 34.8,
        "aime25": 71.3,
        "critpt": 0.6,
        "livecodebench": 67.7
      }
    },
    "confidence_score": 1.0,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "ling-flash-20",
    "canonical_name": "Ling-flash-2.0",
    "model_name": "Ling-flash-2.0",
    "aliases": [
      "Ling-flash-2.0",
      "ling-flash-2.0"
    ],
    "provider": "Ant Group",
    "context_window": 128000,
    "open_source": true,
    "arena_votes": 6995,
    "license_type": "MIT",
    "creator": null,
    "intelligence_score": 19.9163,
    "coding_score": 16.72,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 1346.92,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": 0.25,
    "latency_seconds": 1.58,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": 60.0,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": 473.8996,
    "terminalbench_hard": 0.106,
    "tau2": 0.208,
    "lcr": 15.0,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": 6.3,
    "gpqa": 65.7,
    "scicode": 28.9,
    "ifbench": 34.4,
    "aime25": 65.3,
    "critpt": 0.0,
    "mmmu_pro": null,
    "livecodebench": 58.9,
    "sources": [
      "Artificial Analysis",
      "LMSYS Chatbot Arena"
    ],
    "source_count": 2,
    "data_tier": 1,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.911265+00:00",
        "confidence": 1.0,
        "raw_name": "Ling-flash-2.0",
        "raw_scores": {
          "intelligence_score": 19.916329097188502,
          "coding_score": 16.72,
          "blended_cost_per_1m": 0.25,
          "latency_seconds": 1.58,
          "tokens_per_second": 60.0,
          "context_window": 128000,
          "gdpval": 473.89964852660137,
          "terminalbench_hard": 0.106,
          "tau2": 0.208,
          "lcr": 15.0,
          "hle": 6.3,
          "gpqa": 65.7,
          "scicode": 28.9,
          "ifbench": 34.4,
          "aime25": 65.3,
          "critpt": 0.0,
          "livecodebench": 58.9
        }
      },
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.893133+00:00",
        "confidence": 1.0,
        "raw_name": "ling-flash-2.0",
        "raw_scores": {
          "arena_elo": 1346.92,
          "arena_votes": 6995
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 19.916329097188502,
        "coding_score": 16.72,
        "gdpval": 473.89964852660137,
        "terminalbench_hard": 0.106,
        "tau2": 0.208,
        "lcr": 15.0,
        "hle": 6.3,
        "gpqa": 65.7,
        "scicode": 28.9,
        "ifbench": 34.4,
        "aime25": 65.3,
        "critpt": 0.0,
        "livecodebench": 58.9
      },
      "LMSYS Chatbot Arena": {
        "arena_elo": 1346.92
      }
    },
    "confidence_score": 1.0,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "ring-flash-20",
    "canonical_name": "Ring-flash-2.0",
    "model_name": "Ring-flash-2.0",
    "aliases": [
      "Ring-flash-2.0",
      "ring-flash-2.0"
    ],
    "provider": "Ant Group",
    "context_window": 128000,
    "open_source": true,
    "arena_votes": 7148,
    "license_type": "MIT",
    "creator": null,
    "intelligence_score": 20.5817,
    "coding_score": 10.64,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 1320.45,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": 0.25,
    "latency_seconds": 1.44,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": 81.0,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": 0.076,
    "tau2": 0.0,
    "lcr": 21.0,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": 8.9,
    "gpqa": 72.5,
    "scicode": 16.8,
    "ifbench": 43.3,
    "aime25": 83.7,
    "critpt": 0.3,
    "mmmu_pro": null,
    "livecodebench": 62.8,
    "sources": [
      "Artificial Analysis",
      "LMSYS Chatbot Arena"
    ],
    "source_count": 2,
    "data_tier": 1,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.911314+00:00",
        "confidence": 1.0,
        "raw_name": "Ring-flash-2.0",
        "raw_scores": {
          "intelligence_score": 20.581651045250553,
          "coding_score": 10.64,
          "blended_cost_per_1m": 0.25,
          "latency_seconds": 1.44,
          "tokens_per_second": 81.0,
          "context_window": 128000,
          "terminalbench_hard": 0.076,
          "tau2": 0.0,
          "lcr": 21.0,
          "hle": 8.9,
          "gpqa": 72.5,
          "scicode": 16.8,
          "ifbench": 43.3,
          "aime25": 83.7,
          "critpt": 0.3,
          "livecodebench": 62.8
        }
      },
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.893596+00:00",
        "confidence": 1.0,
        "raw_name": "ring-flash-2.0",
        "raw_scores": {
          "arena_elo": 1320.45,
          "arena_votes": 7148
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 20.581651045250553,
        "coding_score": 10.64,
        "terminalbench_hard": 0.076,
        "tau2": 0.0,
        "lcr": 21.0,
        "hle": 8.9,
        "gpqa": 72.5,
        "scicode": 16.8,
        "ifbench": 43.3,
        "aime25": 83.7,
        "critpt": 0.3,
        "livecodebench": 62.8
      },
      "LMSYS Chatbot Arena": {
        "arena_elo": 1320.45
      }
    },
    "confidence_score": 1.0,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "ling-mini-20",
    "canonical_name": "Ling-mini-2.0",
    "model_name": "Ling-mini-2.0",
    "aliases": [
      "Ling-mini-2.0"
    ],
    "provider": null,
    "context_window": 131000,
    "open_source": true,
    "arena_votes": null,
    "license_type": "MIT",
    "creator": null,
    "intelligence_score": 15.0908,
    "coding_score": 5.02,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": null,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": 0.12,
    "latency_seconds": 1.53,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": 149.0,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": 312.9156,
    "terminalbench_hard": 0.008,
    "tau2": 0.132,
    "lcr": 6.7,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": 5.0,
    "gpqa": 56.2,
    "scicode": 13.5,
    "ifbench": 23.6,
    "aime25": 49.3,
    "critpt": 0.0,
    "mmmu_pro": null,
    "livecodebench": 42.9,
    "sources": [
      "Artificial Analysis"
    ],
    "source_count": 1,
    "data_tier": 2,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.911359+00:00",
        "confidence": 1.0,
        "raw_name": "Ling-mini-2.0",
        "raw_scores": {
          "intelligence_score": 15.090794964025125,
          "coding_score": 5.02,
          "blended_cost_per_1m": 0.12,
          "latency_seconds": 1.53,
          "tokens_per_second": 149.0,
          "context_window": 131000,
          "gdpval": 312.91558184155053,
          "terminalbench_hard": 0.008,
          "tau2": 0.132,
          "lcr": 6.7,
          "hle": 5.0,
          "gpqa": 56.2,
          "scicode": 13.5,
          "ifbench": 23.6,
          "aime25": 49.3,
          "critpt": 0.0,
          "livecodebench": 42.9
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 15.090794964025125,
        "coding_score": 5.02,
        "gdpval": 312.91558184155053,
        "terminalbench_hard": 0.008,
        "tau2": 0.132,
        "lcr": 6.7,
        "hle": 5.0,
        "gpqa": 56.2,
        "scicode": 13.5,
        "ifbench": 23.6,
        "aime25": 49.3,
        "critpt": 0.0,
        "livecodebench": 42.9
      }
    },
    "confidence_score": 1.0,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "doubao-seed-code",
    "canonical_name": "Doubao Seed Code",
    "model_name": "Doubao Seed Code",
    "aliases": [
      "Doubao Seed Code"
    ],
    "provider": null,
    "context_window": 256000,
    "open_source": null,
    "arena_votes": null,
    "license_type": null,
    "creator": null,
    "intelligence_score": 33.52,
    "coding_score": 31.26,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": null,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": 0.0,
    "latency_seconds": 0.0,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": 0.0,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": 1004.5043,
    "terminalbench_hard": 0.265,
    "tau2": 0.582,
    "lcr": 65.3,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": 13.3,
    "gpqa": 76.4,
    "scicode": 40.7,
    "ifbench": 51.4,
    "aime25": 79.3,
    "critpt": 0.3,
    "mmmu_pro": 68.1,
    "livecodebench": 76.6,
    "sources": [
      "Artificial Analysis"
    ],
    "source_count": 1,
    "data_tier": 2,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.911403+00:00",
        "confidence": 1.0,
        "raw_name": "Doubao Seed Code",
        "raw_scores": {
          "intelligence_score": 33.52,
          "coding_score": 31.26,
          "blended_cost_per_1m": 0.0,
          "latency_seconds": 0.0,
          "tokens_per_second": 0.0,
          "context_window": 256000,
          "gdpval": 1004.5042623704735,
          "terminalbench_hard": 0.265,
          "tau2": 0.582,
          "lcr": 65.3,
          "hle": 13.3,
          "gpqa": 76.4,
          "scicode": 40.7,
          "ifbench": 51.4,
          "aime25": 79.3,
          "critpt": 0.3,
          "mmmu_pro": 68.1,
          "livecodebench": 76.6
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 33.52,
        "coding_score": 31.26,
        "gdpval": 1004.5042623704735,
        "terminalbench_hard": 0.265,
        "tau2": 0.582,
        "lcr": 65.3,
        "hle": 13.3,
        "gpqa": 76.4,
        "scicode": 40.7,
        "ifbench": 51.4,
        "aime25": 79.3,
        "critpt": 0.3,
        "mmmu_pro": 68.1,
        "livecodebench": 76.6
      }
    },
    "confidence_score": 1.0,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "o1",
    "canonical_name": "o1",
    "model_name": "o1",
    "aliases": [
      "o1",
      "o1-2024-12-17"
    ],
    "provider": "OpenAI",
    "context_window": 200000,
    "open_source": null,
    "arena_votes": 27822,
    "license_type": "Proprietary",
    "creator": null,
    "intelligence_score": 30.75,
    "coding_score": 20.51,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 1401.92,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": 26.25,
    "latency_seconds": 17.58,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": 144.0,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": 768.7746,
    "terminalbench_hard": 0.129,
    "tau2": 0.626,
    "lcr": 59.3,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": 7.7,
    "gpqa": 74.7,
    "scicode": 35.8,
    "ifbench": 70.3,
    "aime25": null,
    "critpt": 0.3,
    "mmmu_pro": null,
    "livecodebench": 67.9,
    "sources": [
      "Artificial Analysis",
      "LMSYS Chatbot Arena"
    ],
    "source_count": 2,
    "data_tier": 1,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.911445+00:00",
        "confidence": 0.9300000000000002,
        "raw_name": "o1",
        "raw_scores": {
          "intelligence_score": 30.75,
          "coding_score": 20.51,
          "blended_cost_per_1m": 26.25,
          "latency_seconds": 17.58,
          "tokens_per_second": 144.0,
          "context_window": 200000,
          "gdpval": 768.7746059813725,
          "terminalbench_hard": 0.129,
          "tau2": 0.626,
          "lcr": 59.3,
          "hle": 7.7,
          "gpqa": 74.7,
          "scicode": 35.8,
          "ifbench": 70.3,
          "critpt": 0.3,
          "livecodebench": 67.9
        }
      },
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.892244+00:00",
        "confidence": 1.0,
        "raw_name": "o1-2024-12-17",
        "raw_scores": {
          "arena_elo": 1401.92,
          "arena_votes": 27822
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 30.75,
        "coding_score": 20.51,
        "gdpval": 768.7746059813725,
        "terminalbench_hard": 0.129,
        "tau2": 0.626,
        "lcr": 59.3,
        "hle": 7.7,
        "gpqa": 74.7,
        "scicode": 35.8,
        "ifbench": 70.3,
        "critpt": 0.3,
        "livecodebench": 67.9
      },
      "LMSYS Chatbot Arena": {
        "arena_elo": 1401.92
      }
    },
    "confidence_score": 0.965,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "o1-preview",
    "canonical_name": "o1-preview",
    "model_name": "o1-preview",
    "aliases": [
      "o1-preview"
    ],
    "provider": "OpenAI",
    "context_window": 128000,
    "open_source": null,
    "arena_votes": 31120,
    "license_type": "Proprietary",
    "creator": null,
    "intelligence_score": 23.7429,
    "coding_score": 34.0453,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 1388.16,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": 28.88,
    "latency_seconds": 0.0,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": 0.0,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": null,
    "tau2": null,
    "lcr": null,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": null,
    "gpqa": null,
    "scicode": null,
    "ifbench": null,
    "aime25": null,
    "critpt": null,
    "mmmu_pro": null,
    "livecodebench": null,
    "sources": [
      "Artificial Analysis",
      "LMSYS Chatbot Arena"
    ],
    "source_count": 2,
    "data_tier": 1,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.911469+00:00",
        "confidence": 0.72,
        "raw_name": "o1-preview",
        "raw_scores": {
          "intelligence_score": 23.742890445410186,
          "coding_score": 34.045327,
          "blended_cost_per_1m": 28.88,
          "latency_seconds": 0.0,
          "tokens_per_second": 0.0,
          "context_window": 128000
        }
      },
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.892588+00:00",
        "confidence": 1.0,
        "raw_name": "o1-preview",
        "raw_scores": {
          "arena_elo": 1388.16,
          "arena_votes": 31120
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 23.742890445410186,
        "coding_score": 34.045327
      },
      "LMSYS Chatbot Arena": {
        "arena_elo": 1388.16
      }
    },
    "confidence_score": 0.86,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "o1-mini",
    "canonical_name": "o1-mini",
    "model_name": "o1-mini",
    "aliases": [
      "o1-mini"
    ],
    "provider": "OpenAI",
    "context_window": 128000,
    "open_source": null,
    "arena_votes": 51986,
    "license_type": "Proprietary",
    "creator": null,
    "intelligence_score": 20.3869,
    "coding_score": null,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 1336.76,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": 0.0,
    "latency_seconds": 0.0,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": 0.0,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": null,
    "tau2": null,
    "lcr": null,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": 4.9,
    "gpqa": 60.3,
    "scicode": 32.3,
    "ifbench": null,
    "aime25": null,
    "critpt": null,
    "mmmu_pro": null,
    "livecodebench": 57.6,
    "sources": [
      "Artificial Analysis",
      "LMSYS Chatbot Arena"
    ],
    "source_count": 2,
    "data_tier": 1,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.911502+00:00",
        "confidence": 0.8600000000000001,
        "raw_name": "o1-mini",
        "raw_scores": {
          "intelligence_score": 20.38686032302532,
          "blended_cost_per_1m": 0.0,
          "latency_seconds": 0.0,
          "tokens_per_second": 0.0,
          "context_window": 128000,
          "hle": 4.9,
          "gpqa": 60.3,
          "scicode": 32.3,
          "livecodebench": 57.6
        }
      },
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.893306+00:00",
        "confidence": 1.0,
        "raw_name": "o1-mini",
        "raw_scores": {
          "arena_elo": 1336.76,
          "arena_votes": 51986
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 20.38686032302532,
        "hle": 4.9,
        "gpqa": 60.3,
        "scicode": 32.3,
        "livecodebench": 57.6
      },
      "LMSYS Chatbot Arena": {
        "arena_elo": 1336.76
      }
    },
    "confidence_score": 0.93,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "gpt-4o",
    "canonical_name": "GPT-4o (Aug '24)",
    "model_name": "GPT-4o (Aug '24)",
    "aliases": [
      "GPT-4o (Aug '24)",
      "GPT-4o (ChatGPT)",
      "GPT-4o (March 2025, chatgpt-4o-latest)",
      "GPT-4o (May '24)",
      "GPT-4o (Nov '24)",
      "chatgpt-4o-latest-20250326",
      "gpt-4o-2024-05-13"
    ],
    "provider": "OpenAI",
    "context_window": 128000,
    "open_source": null,
    "arena_votes": 112863,
    "license_type": "Proprietary",
    "creator": null,
    "intelligence_score": 15.4595,
    "coding_score": 19.0895,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 1394.235,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": 0.0,
    "latency_seconds": 0.0,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": 0.0,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": 412.0881,
    "terminalbench_hard": 0.083,
    "tau2": 0.2691,
    "lcr": 29.1609,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": 3.5385,
    "gpqa": 55.0294,
    "scicode": 33.4559,
    "ifbench": 35.1106,
    "aime25": 15.3934,
    "critpt": 0.0,
    "mmmu_pro": 56.3,
    "livecodebench": 34.5366,
    "sources": [
      "Artificial Analysis",
      "LMSYS Chatbot Arena"
    ],
    "source_count": 2,
    "data_tier": 1,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.911543+00:00",
        "confidence": 0.72,
        "raw_name": "GPT-4o (Aug '24)",
        "raw_scores": {
          "intelligence_score": 15.57013961773193,
          "coding_score": 16.59,
          "context_window": 128000,
          "gdpval": 427.0118817813925,
          "terminalbench_hard": 0.083,
          "tau2": 0.289,
          "lcr": 35.0,
          "hle": 2.9,
          "gpqa": 52.1,
          "scicode": 33.1,
          "ifbench": 36.0,
          "critpt": 0.0,
          "mmmu_pro": 56.3,
          "livecodebench": 31.7
        }
      },
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.911573+00:00",
        "confidence": 0.72,
        "raw_name": "GPT-4o (May '24)",
        "raw_scores": {
          "intelligence_score": 14.498529042479218,
          "coding_score": 24.2436893,
          "context_window": 128000,
          "hle": 2.8,
          "gpqa": 52.6,
          "scicode": 30.9,
          "livecodebench": 33.4
        }
      },
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.911643+00:00",
        "confidence": 0.79,
        "raw_name": "GPT-4o (Nov '24)",
        "raw_scores": {
          "intelligence_score": 14.762496625016151,
          "coding_score": 16.67,
          "context_window": 128000,
          "gdpval": 398.48662509611574,
          "terminalbench_hard": 0.083,
          "tau2": 0.251,
          "lcr": 0.0,
          "hle": 3.3,
          "gpqa": 54.3,
          "scicode": 33.3,
          "ifbench": 34.3,
          "aime25": 6.0,
          "critpt": 0.0,
          "livecodebench": 30.9
        }
      },
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.911881+00:00",
        "confidence": 0.79,
        "raw_name": "GPT-4o (ChatGPT)",
        "raw_scores": {
          "intelligence_score": 14.107047509866186,
          "blended_cost_per_1m": 0.0,
          "latency_seconds": 0.0,
          "tokens_per_second": 0.0,
          "context_window": 128000,
          "lcr": 53.0,
          "hle": 3.7,
          "gpqa": 51.1,
          "scicode": 33.4
        }
      },
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.912285+00:00",
        "confidence": 0.72,
        "raw_name": "GPT-4o (March 2025, chatgpt-4o-latest)",
        "raw_scores": {
          "intelligence_score": 18.558508422577216,
          "context_window": 128000,
          "hle": 5.0,
          "gpqa": 65.5,
          "scicode": 36.6,
          "aime25": 25.7,
          "livecodebench": 42.5
        }
      },
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.891647+00:00",
        "confidence": 1.0,
        "raw_name": "chatgpt-4o-latest-20250326",
        "raw_scores": {
          "arena_elo": 1442.76,
          "arena_votes": 82938
        }
      },
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.893170+00:00",
        "confidence": 1.0,
        "raw_name": "gpt-4o-2024-05-13",
        "raw_scores": {
          "arena_elo": 1345.71,
          "arena_votes": 112863
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 15.57013961773193,
        "coding_score": 16.59,
        "gdpval": 427.0118817813925,
        "terminalbench_hard": 0.083,
        "tau2": 0.289,
        "lcr": 35.0,
        "hle": 2.9,
        "gpqa": 52.1,
        "scicode": 33.1,
        "ifbench": 36.0,
        "critpt": 0.0,
        "mmmu_pro": 56.3,
        "livecodebench": 31.7,
        "aime25": 6.0
      },
      "LMSYS Chatbot Arena": {
        "arena_elo": 1442.76
      }
    },
    "confidence_score": 0.82,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "gpt-4-turbo",
    "canonical_name": "GPT-4 Turbo",
    "model_name": "GPT-4 Turbo",
    "aliases": [
      "GPT-4 Turbo",
      "gpt-4-turbo-2024-04-09"
    ],
    "provider": "OpenAI",
    "context_window": 128000,
    "open_source": null,
    "arena_votes": 98130,
    "license_type": "Proprietary",
    "creator": null,
    "intelligence_score": 13.7153,
    "coding_score": 21.4873,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 1324.13,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": 15.0,
    "latency_seconds": 1.15,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": 28.0,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": null,
    "tau2": null,
    "lcr": null,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": 3.3,
    "gpqa": null,
    "scicode": 31.9,
    "ifbench": null,
    "aime25": null,
    "critpt": null,
    "mmmu_pro": null,
    "livecodebench": 29.1,
    "sources": [
      "Artificial Analysis",
      "LMSYS Chatbot Arena"
    ],
    "source_count": 2,
    "data_tier": 1,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.911604+00:00",
        "confidence": 0.8600000000000001,
        "raw_name": "GPT-4 Turbo",
        "raw_scores": {
          "intelligence_score": 13.715301370059112,
          "coding_score": 21.4873245,
          "blended_cost_per_1m": 15.0,
          "latency_seconds": 1.15,
          "tokens_per_second": 28.0,
          "context_window": 128000,
          "hle": 3.3,
          "scicode": 31.9,
          "livecodebench": 29.1
        }
      },
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.893493+00:00",
        "confidence": 1.0,
        "raw_name": "gpt-4-turbo-2024-04-09",
        "raw_scores": {
          "arena_elo": 1324.13,
          "arena_votes": 98130
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 13.715301370059112,
        "coding_score": 21.4873245,
        "hle": 3.3,
        "scicode": 31.9,
        "livecodebench": 29.1
      },
      "LMSYS Chatbot Arena": {
        "arena_elo": 1324.13
      }
    },
    "confidence_score": 0.93,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "gpt-4o-mini",
    "canonical_name": "GPT-4o mini",
    "model_name": "GPT-4o mini",
    "aliases": [
      "GPT-4o mini",
      "gpt-4o-mini-2024-07-18"
    ],
    "provider": "OpenAI",
    "context_window": 128000,
    "open_source": null,
    "arena_votes": 68794,
    "license_type": "Proprietary",
    "creator": null,
    "intelligence_score": 12.6473,
    "coding_score": null,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 1317.68,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": 0.26,
    "latency_seconds": 0.56,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": 53.0,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": null,
    "tau2": null,
    "lcr": null,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": 4.0,
    "gpqa": 42.6,
    "scicode": 22.9,
    "ifbench": 31.0,
    "aime25": 14.7,
    "critpt": null,
    "mmmu_pro": 41.5,
    "livecodebench": 23.4,
    "sources": [
      "Artificial Analysis",
      "LMSYS Chatbot Arena"
    ],
    "source_count": 2,
    "data_tier": 1,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.911684+00:00",
        "confidence": 0.9300000000000002,
        "raw_name": "GPT-4o mini",
        "raw_scores": {
          "intelligence_score": 12.647336534486156,
          "blended_cost_per_1m": 0.26,
          "latency_seconds": 0.56,
          "tokens_per_second": 53.0,
          "context_window": 128000,
          "hle": 4.0,
          "gpqa": 42.6,
          "scicode": 22.9,
          "ifbench": 31.0,
          "aime25": 14.7,
          "mmmu_pro": 41.5,
          "livecodebench": 23.4
        }
      },
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.893659+00:00",
        "confidence": 1.0,
        "raw_name": "gpt-4o-mini-2024-07-18",
        "raw_scores": {
          "arena_elo": 1317.68,
          "arena_votes": 68794
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 12.647336534486156,
        "hle": 4.0,
        "gpqa": 42.6,
        "scicode": 22.9,
        "ifbench": 31.0,
        "aime25": 14.7,
        "mmmu_pro": 41.5,
        "livecodebench": 23.4
      },
      "LMSYS Chatbot Arena": {
        "arena_elo": 1317.68
      }
    },
    "confidence_score": 0.965,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "gpt-35-turbo",
    "canonical_name": "GPT-3.5 Turbo",
    "model_name": "GPT-3.5 Turbo",
    "aliases": [
      "GPT-3.5 Turbo",
      "GPT-3.5 Turbo (0613)"
    ],
    "provider": "OpenAI",
    "context_window": 4096,
    "open_source": null,
    "arena_votes": null,
    "license_type": null,
    "creator": null,
    "intelligence_score": 8.9897,
    "coding_score": 10.6542,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": null,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": 0.4325,
    "latency_seconds": 0.2364,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": 49.0146,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": null,
    "tau2": null,
    "lcr": null,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": null,
    "gpqa": 29.7,
    "scicode": null,
    "ifbench": null,
    "aime25": null,
    "critpt": null,
    "mmmu_pro": null,
    "livecodebench": null,
    "sources": [
      "Artificial Analysis"
    ],
    "source_count": 1,
    "data_tier": 2,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.911713+00:00",
        "confidence": 0.79,
        "raw_name": "GPT-3.5 Turbo",
        "raw_scores": {
          "intelligence_score": 8.989676386007632,
          "coding_score": 10.6541808,
          "blended_cost_per_1m": 0.75,
          "latency_seconds": 0.41,
          "tokens_per_second": 85.0,
          "context_window": 4096,
          "gpqa": 29.7
        }
      },
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.912597+00:00",
        "confidence": 0.5800000000000001,
        "raw_name": "GPT-3.5 Turbo (0613)",
        "raw_scores": {
          "blended_cost_per_1m": 0.0,
          "latency_seconds": 0.0,
          "tokens_per_second": 0.0,
          "context_window": 4096
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 8.989676386007632,
        "coding_score": 10.6541808,
        "gpqa": 29.7
      }
    },
    "confidence_score": 0.685,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "gpt-51",
    "canonical_name": "GPT-5.1 (high)",
    "model_name": "GPT-5.1 (high)",
    "aliases": [
      "GPT-5.1 (Non-reasoning)",
      "GPT-5.1 (high)",
      "gpt-5.1",
      "gpt-5.1-high"
    ],
    "provider": "OpenAI",
    "context_window": 400000,
    "open_source": null,
    "arena_votes": 36738,
    "license_type": "Proprietary",
    "creator": null,
    "intelligence_score": 38.7496,
    "coding_score": 37.0374,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 1446.86,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": 3.44,
    "latency_seconds": 36.22,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": 97.0,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": 1128.4057,
    "terminalbench_hard": 0.3544,
    "tau2": 0.6628,
    "lcr": 61.3184,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": 17.0994,
    "gpqa": 77.1492,
    "scicode": 40.2989,
    "ifbench": 59.7922,
    "aime25": 69.2849,
    "critpt": 2.7374,
    "mmmu_pro": 69.7184,
    "livecodebench": 70.2939,
    "sources": [
      "Artificial Analysis",
      "LMSYS Chatbot Arena"
    ],
    "source_count": 2,
    "data_tier": 1,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.911758+00:00",
        "confidence": 1.0,
        "raw_name": "GPT-5.1 (high)",
        "raw_scores": {
          "intelligence_score": 47.7,
          "coding_score": 44.73,
          "blended_cost_per_1m": 3.44,
          "latency_seconds": 36.22,
          "tokens_per_second": 97.0,
          "context_window": 400000,
          "gdpval": 1229.8461795815315,
          "terminalbench_hard": 0.455,
          "tau2": 0.819,
          "lcr": 75.0,
          "hle": 26.5,
          "gpqa": 87.3,
          "scicode": 43.3,
          "ifbench": 72.9,
          "aime25": 94.0,
          "critpt": 4.9,
          "mmmu_pro": 75.5,
          "livecodebench": 86.8
        }
      },
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.912238+00:00",
        "confidence": 0.79,
        "raw_name": "GPT-5.1 (Non-reasoning)",
        "raw_scores": {
          "intelligence_score": 27.42,
          "coding_score": 27.3,
          "context_window": 400000,
          "gdpval": 1000.0,
          "terminalbench_hard": 0.227,
          "tau2": 0.465,
          "lcr": 44.0,
          "hle": 5.2,
          "gpqa": 64.3,
          "scicode": 36.5,
          "ifbench": 43.2,
          "aime25": 38.0,
          "critpt": 0.0,
          "mmmu_pro": 62.4,
          "livecodebench": 49.4
        }
      },
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.891482+00:00",
        "confidence": 1.0,
        "raw_name": "gpt-5.1-high",
        "raw_scores": {
          "arena_elo": 1456.77,
          "arena_votes": 34379
        }
      },
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.891685+00:00",
        "confidence": 1.0,
        "raw_name": "gpt-5.1",
        "raw_scores": {
          "arena_elo": 1436.95,
          "arena_votes": 36738
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 47.7,
        "coding_score": 44.73,
        "gdpval": 1229.8461795815315,
        "terminalbench_hard": 0.455,
        "tau2": 0.819,
        "lcr": 75.0,
        "hle": 26.5,
        "gpqa": 87.3,
        "scicode": 43.3,
        "ifbench": 72.9,
        "aime25": 94.0,
        "critpt": 4.9,
        "mmmu_pro": 75.5,
        "livecodebench": 86.8
      },
      "LMSYS Chatbot Arena": {
        "arena_elo": 1456.77
      }
    },
    "confidence_score": 0.9475,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "gpt-41",
    "canonical_name": "GPT-4.1",
    "model_name": "GPT-4.1",
    "aliases": [
      "GPT-4.1",
      "gpt-4.1-2025-04-14"
    ],
    "provider": "OpenAI",
    "context_window": 1000000,
    "open_source": null,
    "arena_votes": 51837,
    "license_type": "Proprietary",
    "creator": null,
    "intelligence_score": 26.28,
    "coding_score": 21.78,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 1413.4,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": 3.5,
    "latency_seconds": 0.54,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": 88.0,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": 810.4084,
    "terminalbench_hard": 0.136,
    "tau2": 0.471,
    "lcr": 61.0,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": 4.6,
    "gpqa": 66.6,
    "scicode": 38.1,
    "ifbench": 43.0,
    "aime25": 34.7,
    "critpt": 0.0,
    "mmmu_pro": 61.2,
    "livecodebench": 45.7,
    "sources": [
      "Artificial Analysis",
      "LMSYS Chatbot Arena"
    ],
    "source_count": 2,
    "data_tier": 1,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.911804+00:00",
        "confidence": 1.0,
        "raw_name": "GPT-4.1",
        "raw_scores": {
          "intelligence_score": 26.28,
          "coding_score": 21.78,
          "blended_cost_per_1m": 3.5,
          "latency_seconds": 0.54,
          "tokens_per_second": 88.0,
          "context_window": 1000000,
          "gdpval": 810.4083780582944,
          "terminalbench_hard": 0.136,
          "tau2": 0.471,
          "lcr": 61.0,
          "hle": 4.6,
          "gpqa": 66.6,
          "scicode": 38.1,
          "ifbench": 43.0,
          "aime25": 34.7,
          "critpt": 0.0,
          "mmmu_pro": 61.2,
          "livecodebench": 45.7
        }
      },
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.892074+00:00",
        "confidence": 1.0,
        "raw_name": "gpt-4.1-2025-04-14",
        "raw_scores": {
          "arena_elo": 1413.4,
          "arena_votes": 51837
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 26.28,
        "coding_score": 21.78,
        "gdpval": 810.4083780582944,
        "terminalbench_hard": 0.136,
        "tau2": 0.471,
        "lcr": 61.0,
        "hle": 4.6,
        "gpqa": 66.6,
        "scicode": 38.1,
        "ifbench": 43.0,
        "aime25": 34.7,
        "critpt": 0.0,
        "mmmu_pro": 61.2,
        "livecodebench": 45.7
      },
      "LMSYS Chatbot Arena": {
        "arena_elo": 1413.4
      }
    },
    "confidence_score": 1.0,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "gpt-5",
    "canonical_name": "GPT-5 (minimal)",
    "model_name": "GPT-5 (minimal)",
    "aliases": [
      "GPT-5 (ChatGPT)",
      "GPT-5 (high)",
      "GPT-5 (low)",
      "GPT-5 (medium)",
      "GPT-5 (minimal)",
      "gpt-5-chat",
      "gpt-5-high"
    ],
    "provider": "OpenAI",
    "context_window": 400000,
    "open_source": null,
    "arena_votes": 32346,
    "license_type": "Proprietary",
    "creator": null,
    "intelligence_score": 34.1095,
    "coding_score": 30.39,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 1430.08,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": 3.44,
    "latency_seconds": 38.174,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": 105.0,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": 976.8048,
    "terminalbench_hard": 0.2562,
    "tau2": 0.645,
    "lcr": 59.16,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": 15.92,
    "gpqa": 77.26,
    "scicode": 39.94,
    "ifbench": 60.18,
    "aime25": 69.8,
    "critpt": 1.7,
    "mmmu_pro": 71.1,
    "livecodebench": 68.26,
    "sources": [
      "Artificial Analysis",
      "LMSYS Chatbot Arena"
    ],
    "source_count": 2,
    "data_tier": 1,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.911850+00:00",
        "confidence": 1.0,
        "raw_name": "GPT-5 (minimal)",
        "raw_scores": {
          "intelligence_score": 22.85282981786174,
          "coding_score": 25.05,
          "blended_cost_per_1m": 3.44,
          "latency_seconds": 0.98,
          "tokens_per_second": 71.0,
          "context_window": 400000,
          "gdpval": 444.44118480220664,
          "terminalbench_hard": 0.182,
          "tau2": 0.67,
          "lcr": 25.0,
          "hle": 5.4,
          "gpqa": 67.3,
          "scicode": 38.8,
          "ifbench": 45.6,
          "aime25": 31.7,
          "critpt": 0.0,
          "mmmu_pro": 62.1,
          "livecodebench": 55.8
        }
      },
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.911972+00:00",
        "confidence": 1.0,
        "raw_name": "GPT-5 (medium)",
        "raw_scores": {
          "intelligence_score": 42.03,
          "coding_score": 38.95,
          "blended_cost_per_1m": 3.44,
          "latency_seconds": 55.67,
          "tokens_per_second": 86.0,
          "context_window": 400000,
          "gdpval": 1009.3131193294901,
          "terminalbench_hard": 0.379,
          "tau2": 0.865,
          "lcr": 72.8,
          "hle": 23.5,
          "gpqa": 84.2,
          "scicode": 41.1,
          "ifbench": 70.6,
          "aime25": 91.7,
          "critpt": 0.0,
          "mmmu_pro": 74.3,
          "livecodebench": 70.3
        }
      },
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.912068+00:00",
        "confidence": 1.0,
        "raw_name": "GPT-5 (low)",
        "raw_scores": {
          "intelligence_score": 39.2,
          "coding_score": 30.72,
          "blended_cost_per_1m": 3.44,
          "latency_seconds": 26.17,
          "tokens_per_second": 97.0,
          "context_window": 400000,
          "gdpval": 1150.726491665914,
          "terminalbench_hard": 0.265,
          "tau2": 0.842,
          "lcr": 58.7,
          "hle": 18.4,
          "gpqa": 80.8,
          "scicode": 39.1,
          "ifbench": 66.6,
          "aime25": 83.0,
          "critpt": 1.1,
          "mmmu_pro": 73.8,
          "livecodebench": 76.3
        }
      },
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.912183+00:00",
        "confidence": 1.0,
        "raw_name": "GPT-5 (high)",
        "raw_scores": {
          "intelligence_score": 44.63,
          "coding_score": 36.03,
          "blended_cost_per_1m": 3.44,
          "latency_seconds": 107.57,
          "tokens_per_second": 91.0,
          "context_window": 400000,
          "gdpval": 1302.7383944420806,
          "terminalbench_hard": 0.326,
          "tau2": 0.848,
          "lcr": 75.6,
          "hle": 26.5,
          "gpqa": 85.4,
          "scicode": 42.9,
          "ifbench": 73.1,
          "aime25": 94.3,
          "critpt": 5.7,
          "mmmu_pro": 74.2,
          "livecodebench": 84.6
        }
      },
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.912796+00:00",
        "confidence": 1.0,
        "raw_name": "GPT-5 (ChatGPT)",
        "raw_scores": {
          "intelligence_score": 21.834580028932926,
          "coding_score": 21.2,
          "blended_cost_per_1m": 3.44,
          "latency_seconds": 0.48,
          "tokens_per_second": 180.0,
          "context_window": 128000,
          "terminalbench_hard": 0.129,
          "tau2": 0.0,
          "lcr": 63.7,
          "hle": 5.8,
          "gpqa": 68.6,
          "scicode": 37.8,
          "ifbench": 45.0,
          "aime25": 48.3,
          "livecodebench": 54.3
        }
      },
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.891735+00:00",
        "confidence": 1.0,
        "raw_name": "gpt-5-high",
        "raw_scores": {
          "arena_elo": 1434.12,
          "arena_votes": 32346
        }
      },
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.891808+00:00",
        "confidence": 1.0,
        "raw_name": "gpt-5-chat",
        "raw_scores": {
          "arena_elo": 1426.04,
          "arena_votes": 31603
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 22.85282981786174,
        "coding_score": 25.05,
        "gdpval": 444.44118480220664,
        "terminalbench_hard": 0.182,
        "tau2": 0.67,
        "lcr": 25.0,
        "hle": 5.4,
        "gpqa": 67.3,
        "scicode": 38.8,
        "ifbench": 45.6,
        "aime25": 31.7,
        "critpt": 0.0,
        "mmmu_pro": 62.1,
        "livecodebench": 55.8
      },
      "LMSYS Chatbot Arena": {
        "arena_elo": 1434.12
      }
    },
    "confidence_score": 1.0,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "gpt-5-codex",
    "canonical_name": "GPT-5 Codex (high)",
    "model_name": "GPT-5 Codex (high)",
    "aliases": [
      "GPT-5 Codex (high)"
    ],
    "provider": "OpenAI",
    "context_window": 400000,
    "open_source": null,
    "arena_votes": null,
    "license_type": null,
    "creator": null,
    "intelligence_score": 44.63,
    "coding_score": 38.87,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": null,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": 3.44,
    "latency_seconds": 12.87,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": 312.0,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": 1216.8529,
    "terminalbench_hard": 0.379,
    "tau2": 0.868,
    "lcr": 69.0,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": 25.6,
    "gpqa": 83.7,
    "scicode": 40.9,
    "ifbench": 74.1,
    "aime25": 98.7,
    "critpt": 5.1,
    "mmmu_pro": 73.8,
    "livecodebench": 84.0,
    "sources": [
      "Artificial Analysis"
    ],
    "source_count": 1,
    "data_tier": 2,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.911926+00:00",
        "confidence": 1.0,
        "raw_name": "GPT-5 Codex (high)",
        "raw_scores": {
          "intelligence_score": 44.63,
          "coding_score": 38.87,
          "blended_cost_per_1m": 3.44,
          "latency_seconds": 12.87,
          "tokens_per_second": 312.0,
          "context_window": 400000,
          "gdpval": 1216.8528769958966,
          "terminalbench_hard": 0.379,
          "tau2": 0.868,
          "lcr": 69.0,
          "hle": 25.6,
          "gpqa": 83.7,
          "scicode": 40.9,
          "ifbench": 74.1,
          "aime25": 98.7,
          "critpt": 5.1,
          "mmmu_pro": 73.8,
          "livecodebench": 84.0
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 44.63,
        "coding_score": 38.87,
        "gdpval": 1216.8528769958966,
        "terminalbench_hard": 0.379,
        "tau2": 0.868,
        "lcr": 69.0,
        "hle": 25.6,
        "gpqa": 83.7,
        "scicode": 40.9,
        "ifbench": 74.1,
        "aime25": 98.7,
        "critpt": 5.1,
        "mmmu_pro": 73.8,
        "livecodebench": 84.0
      }
    },
    "confidence_score": 1.0,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "gpt-41-mini",
    "canonical_name": "GPT-4.1 mini",
    "model_name": "GPT-4.1 mini",
    "aliases": [
      "GPT-4.1 mini",
      "gpt-4.1-mini-2025-04-14"
    ],
    "provider": "OpenAI",
    "context_window": 1000000,
    "open_source": null,
    "arena_votes": 40313,
    "license_type": "Proprietary",
    "creator": null,
    "intelligence_score": 22.2493,
    "coding_score": 18.52,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 1381.85,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": 0.7,
    "latency_seconds": 0.5,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": 64.0,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": 662.0541,
    "terminalbench_hard": 0.076,
    "tau2": 0.529,
    "lcr": 42.3,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": 4.6,
    "gpqa": 66.4,
    "scicode": 40.4,
    "ifbench": 38.3,
    "aime25": 46.3,
    "critpt": 0.0,
    "mmmu_pro": 58.7,
    "livecodebench": 48.3,
    "sources": [
      "Artificial Analysis",
      "LMSYS Chatbot Arena"
    ],
    "source_count": 2,
    "data_tier": 1,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.912021+00:00",
        "confidence": 1.0,
        "raw_name": "GPT-4.1 mini",
        "raw_scores": {
          "intelligence_score": 22.24928677193938,
          "coding_score": 18.52,
          "blended_cost_per_1m": 0.7,
          "latency_seconds": 0.5,
          "tokens_per_second": 64.0,
          "context_window": 1000000,
          "gdpval": 662.0541133712074,
          "terminalbench_hard": 0.076,
          "tau2": 0.529,
          "lcr": 42.3,
          "hle": 4.6,
          "gpqa": 66.4,
          "scicode": 40.4,
          "ifbench": 38.3,
          "aime25": 46.3,
          "critpt": 0.0,
          "mmmu_pro": 58.7,
          "livecodebench": 48.3
        }
      },
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.892690+00:00",
        "confidence": 1.0,
        "raw_name": "gpt-4.1-mini-2025-04-14",
        "raw_scores": {
          "arena_elo": 1381.85,
          "arena_votes": 40313
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 22.24928677193938,
        "coding_score": 18.52,
        "gdpval": 662.0541133712074,
        "terminalbench_hard": 0.076,
        "tau2": 0.529,
        "lcr": 42.3,
        "hle": 4.6,
        "gpqa": 66.4,
        "scicode": 40.4,
        "ifbench": 38.3,
        "aime25": 46.3,
        "critpt": 0.0,
        "mmmu_pro": 58.7,
        "livecodebench": 48.3
      },
      "LMSYS Chatbot Arena": {
        "arena_elo": 1381.85
      }
    },
    "confidence_score": 1.0,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "o3-pro",
    "canonical_name": "o3-pro",
    "model_name": "o3-pro",
    "aliases": [
      "o3-pro"
    ],
    "provider": "OpenAI",
    "context_window": 200000,
    "open_source": null,
    "arena_votes": null,
    "license_type": null,
    "creator": null,
    "intelligence_score": 40.6899,
    "coding_score": null,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": null,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": 35.0,
    "latency_seconds": 141.14,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": 22.0,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": null,
    "tau2": null,
    "lcr": null,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": null,
    "gpqa": 84.5,
    "scicode": null,
    "ifbench": null,
    "aime25": null,
    "critpt": null,
    "mmmu_pro": null,
    "livecodebench": null,
    "sources": [
      "Artificial Analysis"
    ],
    "source_count": 1,
    "data_tier": 2,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.912138+00:00",
        "confidence": 0.72,
        "raw_name": "o3-pro",
        "raw_scores": {
          "intelligence_score": 40.68990197347841,
          "blended_cost_per_1m": 35.0,
          "latency_seconds": 141.14,
          "tokens_per_second": 22.0,
          "context_window": 200000,
          "gpqa": 84.5
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 40.68990197347841,
        "gpqa": 84.5
      }
    },
    "confidence_score": 0.72,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "o3-mini",
    "canonical_name": "o3-mini (high)",
    "model_name": "o3-mini (high)",
    "aliases": [
      "o3-mini",
      "o3-mini (high)"
    ],
    "provider": "OpenAI",
    "context_window": 200000,
    "open_source": null,
    "arena_votes": 58451,
    "license_type": "Proprietary",
    "creator": null,
    "intelligence_score": 25.5366,
    "coding_score": 17.58,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 1348.1,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": 1.93,
    "latency_seconds": 28.07,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": 134.5,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": 780.6233,
    "terminalbench_hard": 0.0645,
    "tau2": 0.3,
    "lcr": 39.3,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": 10.5,
    "gpqa": 76.05,
    "scicode": 39.85,
    "ifbench": 67.1,
    "aime25": null,
    "critpt": 0.3,
    "mmmu_pro": null,
    "livecodebench": 72.55,
    "sources": [
      "Artificial Analysis",
      "LMSYS Chatbot Arena"
    ],
    "source_count": 2,
    "data_tier": 1,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.912380+00:00",
        "confidence": 0.9300000000000002,
        "raw_name": "o3-mini (high)",
        "raw_scores": {
          "intelligence_score": 25.21,
          "coding_score": 17.3,
          "blended_cost_per_1m": 1.93,
          "latency_seconds": 41.22,
          "tokens_per_second": 141.0,
          "context_window": 200000,
          "gdpval": 780.6233250748116,
          "terminalbench_hard": 0.061,
          "tau2": 0.313,
          "lcr": 39.3,
          "hle": 12.3,
          "gpqa": 77.3,
          "scicode": 39.8,
          "ifbench": 67.1,
          "critpt": 0.3,
          "livecodebench": 73.4
        }
      },
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.912417+00:00",
        "confidence": 0.9300000000000002,
        "raw_name": "o3-mini",
        "raw_scores": {
          "intelligence_score": 25.863248068693565,
          "coding_score": 17.86,
          "blended_cost_per_1m": 1.93,
          "latency_seconds": 14.92,
          "tokens_per_second": 128.0,
          "context_window": 200000,
          "terminalbench_hard": 0.068,
          "tau2": 0.287,
          "hle": 8.7,
          "gpqa": 74.8,
          "scicode": 39.9,
          "livecodebench": 71.7
        }
      },
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.893068+00:00",
        "confidence": 1.0,
        "raw_name": "o3-mini",
        "raw_scores": {
          "arena_elo": 1348.1,
          "arena_votes": 58451
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 25.21,
        "coding_score": 17.3,
        "gdpval": 780.6233250748116,
        "terminalbench_hard": 0.061,
        "tau2": 0.313,
        "lcr": 39.3,
        "hle": 12.3,
        "gpqa": 77.3,
        "scicode": 39.8,
        "ifbench": 67.1,
        "critpt": 0.3,
        "livecodebench": 73.4
      },
      "LMSYS Chatbot Arena": {
        "arena_elo": 1348.1
      }
    },
    "confidence_score": 0.9533,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "gpt-4",
    "canonical_name": "GPT-4",
    "model_name": "GPT-4",
    "aliases": [
      "GPT-4"
    ],
    "provider": "OpenAI",
    "context_window": 8192,
    "open_source": null,
    "arena_votes": null,
    "license_type": null,
    "creator": null,
    "intelligence_score": 12.7543,
    "coding_score": 13.1424,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": null,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": 37.5,
    "latency_seconds": 0.73,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": 25.0,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": null,
    "tau2": null,
    "lcr": null,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": null,
    "gpqa": null,
    "scicode": null,
    "ifbench": null,
    "aime25": null,
    "critpt": null,
    "mmmu_pro": null,
    "livecodebench": null,
    "sources": [
      "Artificial Analysis"
    ],
    "source_count": 1,
    "data_tier": 2,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.912488+00:00",
        "confidence": 0.72,
        "raw_name": "GPT-4",
        "raw_scores": {
          "intelligence_score": 12.754307113238253,
          "coding_score": 13.142408,
          "blended_cost_per_1m": 37.5,
          "latency_seconds": 0.73,
          "tokens_per_second": 25.0,
          "context_window": 8192
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 12.754307113238253,
        "coding_score": 13.142408
      }
    },
    "confidence_score": 0.72,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "gpt-41-nano",
    "canonical_name": "GPT-4.1 nano",
    "model_name": "GPT-4.1 nano",
    "aliases": [
      "GPT-4.1 nano",
      "gpt-4.1-nano-2025-04-14"
    ],
    "provider": "OpenAI",
    "context_window": 1000000,
    "open_source": null,
    "arena_votes": 6107,
    "license_type": "Proprietary",
    "creator": null,
    "intelligence_score": 14.8922,
    "coding_score": 11.17,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 1321.71,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": 0.17,
    "latency_seconds": 0.36,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": 88.0,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": 383.7755,
    "terminalbench_hard": 0.038,
    "tau2": 0.173,
    "lcr": 17.0,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": 3.9,
    "gpqa": 51.2,
    "scicode": 25.9,
    "ifbench": 32.0,
    "aime25": 24.0,
    "critpt": 0.0,
    "mmmu_pro": 40.1,
    "livecodebench": 32.6,
    "sources": [
      "Artificial Analysis",
      "LMSYS Chatbot Arena"
    ],
    "source_count": 2,
    "data_tier": 1,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.912578+00:00",
        "confidence": 1.0,
        "raw_name": "GPT-4.1 nano",
        "raw_scores": {
          "intelligence_score": 14.89216078821739,
          "coding_score": 11.17,
          "blended_cost_per_1m": 0.17,
          "latency_seconds": 0.36,
          "tokens_per_second": 88.0,
          "context_window": 1000000,
          "gdpval": 383.77551757486594,
          "terminalbench_hard": 0.038,
          "tau2": 0.173,
          "lcr": 17.0,
          "hle": 3.9,
          "gpqa": 51.2,
          "scicode": 25.9,
          "ifbench": 32.0,
          "aime25": 24.0,
          "critpt": 0.0,
          "mmmu_pro": 40.1,
          "livecodebench": 32.6
        }
      },
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.893584+00:00",
        "confidence": 1.0,
        "raw_name": "gpt-4.1-nano-2025-04-14",
        "raw_scores": {
          "arena_elo": 1321.71,
          "arena_votes": 6107
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 14.89216078821739,
        "coding_score": 11.17,
        "gdpval": 383.77551757486594,
        "terminalbench_hard": 0.038,
        "tau2": 0.173,
        "lcr": 17.0,
        "hle": 3.9,
        "gpqa": 51.2,
        "scicode": 25.9,
        "ifbench": 32.0,
        "aime25": 24.0,
        "critpt": 0.0,
        "mmmu_pro": 40.1,
        "livecodebench": 32.6
      },
      "LMSYS Chatbot Arena": {
        "arena_elo": 1321.71
      }
    },
    "confidence_score": 1.0,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "o4-mini",
    "canonical_name": "o4-mini (high)",
    "model_name": "o4-mini (high)",
    "aliases": [
      "o4-mini (high)",
      "o4-mini-2025-04-16"
    ],
    "provider": "OpenAI",
    "context_window": 200000,
    "open_source": null,
    "arena_votes": 46375,
    "license_type": "Proprietary",
    "creator": null,
    "intelligence_score": 33.06,
    "coding_score": 25.61,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 1390.98,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": 1.93,
    "latency_seconds": 60.42,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": 118.0,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": 1014.3726,
    "terminalbench_hard": 0.152,
    "tau2": 0.556,
    "lcr": 55.0,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": 17.5,
    "gpqa": 78.4,
    "scicode": 46.5,
    "ifbench": 68.7,
    "aime25": 90.7,
    "critpt": 0.6,
    "mmmu_pro": 69.2,
    "livecodebench": 85.9,
    "sources": [
      "Artificial Analysis",
      "LMSYS Chatbot Arena"
    ],
    "source_count": 2,
    "data_tier": 1,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.912644+00:00",
        "confidence": 1.0,
        "raw_name": "o4-mini (high)",
        "raw_scores": {
          "intelligence_score": 33.06,
          "coding_score": 25.61,
          "blended_cost_per_1m": 1.93,
          "latency_seconds": 60.42,
          "tokens_per_second": 118.0,
          "context_window": 200000,
          "gdpval": 1014.3725793079434,
          "terminalbench_hard": 0.152,
          "tau2": 0.556,
          "lcr": 55.0,
          "hle": 17.5,
          "gpqa": 78.4,
          "scicode": 46.5,
          "ifbench": 68.7,
          "aime25": 90.7,
          "critpt": 0.6,
          "mmmu_pro": 69.2,
          "livecodebench": 85.9
        }
      },
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.892398+00:00",
        "confidence": 1.0,
        "raw_name": "o4-mini-2025-04-16",
        "raw_scores": {
          "arena_elo": 1390.98,
          "arena_votes": 46375
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 33.06,
        "coding_score": 25.61,
        "gdpval": 1014.3725793079434,
        "terminalbench_hard": 0.152,
        "tau2": 0.556,
        "lcr": 55.0,
        "hle": 17.5,
        "gpqa": 78.4,
        "scicode": 46.5,
        "ifbench": 68.7,
        "aime25": 90.7,
        "critpt": 0.6,
        "mmmu_pro": 69.2,
        "livecodebench": 85.9
      },
      "LMSYS Chatbot Arena": {
        "arena_elo": 1390.98
      }
    },
    "confidence_score": 1.0,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "o1-pro",
    "canonical_name": "o1-pro",
    "model_name": "o1-pro",
    "aliases": [
      "o1-pro"
    ],
    "provider": "OpenAI",
    "context_window": 200000,
    "open_source": null,
    "arena_votes": null,
    "license_type": null,
    "creator": null,
    "intelligence_score": 25.7608,
    "coding_score": null,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": null,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": 262.5,
    "latency_seconds": 0.0,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": 0.0,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": null,
    "tau2": null,
    "lcr": null,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": null,
    "gpqa": null,
    "scicode": null,
    "ifbench": null,
    "aime25": null,
    "critpt": null,
    "mmmu_pro": null,
    "livecodebench": null,
    "sources": [
      "Artificial Analysis"
    ],
    "source_count": 1,
    "data_tier": 2,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.912684+00:00",
        "confidence": 0.65,
        "raw_name": "o1-pro",
        "raw_scores": {
          "intelligence_score": 25.760825664214558,
          "blended_cost_per_1m": 262.5,
          "latency_seconds": 0.0,
          "tokens_per_second": 0.0,
          "context_window": 200000
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 25.760825664214558
      }
    },
    "confidence_score": 0.65,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "gpt-51-codex",
    "canonical_name": "GPT-5.1 Codex (high)",
    "model_name": "GPT-5.1 Codex (high)",
    "aliases": [
      "GPT-5.1 Codex (high)"
    ],
    "provider": "OpenAI",
    "context_window": 400000,
    "open_source": null,
    "arena_votes": null,
    "license_type": null,
    "creator": null,
    "intelligence_score": 43.11,
    "coding_score": 36.62,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": null,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": 3.44,
    "latency_seconds": 15.33,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": 184.0,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": 1187.2517,
    "terminalbench_hard": 0.348,
    "tau2": 0.83,
    "lcr": 67.3,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": 23.4,
    "gpqa": 86.0,
    "scicode": 40.2,
    "ifbench": 70.0,
    "aime25": 95.7,
    "critpt": 5.7,
    "mmmu_pro": 72.5,
    "livecodebench": 84.9,
    "sources": [
      "Artificial Analysis"
    ],
    "source_count": 1,
    "data_tier": 2,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.912732+00:00",
        "confidence": 1.0,
        "raw_name": "GPT-5.1 Codex (high)",
        "raw_scores": {
          "intelligence_score": 43.11,
          "coding_score": 36.62,
          "blended_cost_per_1m": 3.44,
          "latency_seconds": 15.33,
          "tokens_per_second": 184.0,
          "context_window": 400000,
          "gdpval": 1187.2517410693195,
          "terminalbench_hard": 0.348,
          "tau2": 0.83,
          "lcr": 67.3,
          "hle": 23.4,
          "gpqa": 86.0,
          "scicode": 40.2,
          "ifbench": 70.0,
          "aime25": 95.7,
          "critpt": 5.7,
          "mmmu_pro": 72.5,
          "livecodebench": 84.9
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 43.11,
        "coding_score": 36.62,
        "gdpval": 1187.2517410693195,
        "terminalbench_hard": 0.348,
        "tau2": 0.83,
        "lcr": 67.3,
        "hle": 23.4,
        "gpqa": 86.0,
        "scicode": 40.2,
        "ifbench": 70.0,
        "aime25": 95.7,
        "critpt": 5.7,
        "mmmu_pro": 72.5,
        "livecodebench": 84.9
      }
    },
    "confidence_score": 1.0,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "gpt-45",
    "canonical_name": "GPT-4.5 (Preview)",
    "model_name": "GPT-4.5 (Preview)",
    "aliases": [
      "GPT-4.5 (Preview)"
    ],
    "provider": "OpenAI",
    "context_window": 128000,
    "open_source": null,
    "arena_votes": null,
    "license_type": null,
    "creator": null,
    "intelligence_score": 19.9568,
    "coding_score": null,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": null,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": 0.0,
    "latency_seconds": 0.0,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": 0.0,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": null,
    "tau2": null,
    "lcr": null,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": null,
    "gpqa": null,
    "scicode": null,
    "ifbench": null,
    "aime25": null,
    "critpt": null,
    "mmmu_pro": null,
    "livecodebench": null,
    "sources": [
      "Artificial Analysis"
    ],
    "source_count": 1,
    "data_tier": 2,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.912755+00:00",
        "confidence": 0.65,
        "raw_name": "GPT-4.5 (Preview)",
        "raw_scores": {
          "intelligence_score": 19.956828896300806,
          "blended_cost_per_1m": 0.0,
          "latency_seconds": 0.0,
          "tokens_per_second": 0.0,
          "context_window": 128000
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 19.956828896300806
      }
    },
    "confidence_score": 0.65,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "llama-31-instruct-70b",
    "canonical_name": "Llama 3.1 Instruct 70B",
    "model_name": "Llama 3.1 Instruct 70B",
    "aliases": [
      "Llama 3.1 Instruct 70B"
    ],
    "provider": "Meta",
    "context_window": 128000,
    "open_source": true,
    "arena_votes": null,
    "license_type": "LLAMA 3.1 COMMUNITY LICENSE AGREEMENT",
    "creator": null,
    "intelligence_score": 13.1343,
    "coding_score": 10.93,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": null,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": 328.6272,
    "terminalbench_hard": 0.03,
    "tau2": 0.152,
    "lcr": 6.3,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": 4.6,
    "gpqa": 40.9,
    "scicode": 26.7,
    "ifbench": 34.4,
    "aime25": 4.0,
    "critpt": 0.0,
    "mmmu_pro": null,
    "livecodebench": 23.2,
    "sources": [
      "Artificial Analysis"
    ],
    "source_count": 1,
    "data_tier": 2,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.912836+00:00",
        "confidence": 0.79,
        "raw_name": "Llama 3.1 Instruct 70B",
        "raw_scores": {
          "intelligence_score": 13.134271418700061,
          "coding_score": 10.93,
          "context_window": 128000,
          "gdpval": 328.62721614391035,
          "terminalbench_hard": 0.03,
          "tau2": 0.152,
          "lcr": 6.3,
          "hle": 4.6,
          "gpqa": 40.9,
          "scicode": 26.7,
          "ifbench": 34.4,
          "aime25": 4.0,
          "critpt": 0.0,
          "livecodebench": 23.2
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 13.134271418700061,
        "coding_score": 10.93,
        "gdpval": 328.62721614391035,
        "terminalbench_hard": 0.03,
        "tau2": 0.152,
        "lcr": 6.3,
        "hle": 4.6,
        "gpqa": 40.9,
        "scicode": 26.7,
        "ifbench": 34.4,
        "aime25": 4.0,
        "critpt": 0.0,
        "livecodebench": 23.2
      }
    },
    "confidence_score": 0.79,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "llama-31-instruct-8b",
    "canonical_name": "Llama 3.1 Instruct 8B",
    "model_name": "Llama 3.1 Instruct 8B",
    "aliases": [
      "Llama 3.1 Instruct 8B"
    ],
    "provider": "Meta",
    "context_window": 128000,
    "open_source": true,
    "arena_votes": null,
    "license_type": "LLAMA 3.1 COMMUNITY LICENSE AGREEMENT",
    "creator": null,
    "intelligence_score": 11.3019,
    "coding_score": 4.9,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": null,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": 334.3104,
    "terminalbench_hard": 0.008,
    "tau2": 0.164,
    "lcr": 15.7,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": 5.1,
    "gpqa": 25.9,
    "scicode": 13.2,
    "ifbench": 28.6,
    "aime25": 4.3,
    "critpt": 0.0,
    "mmmu_pro": null,
    "livecodebench": 11.6,
    "sources": [
      "Artificial Analysis"
    ],
    "source_count": 1,
    "data_tier": 2,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.912877+00:00",
        "confidence": 0.79,
        "raw_name": "Llama 3.1 Instruct 8B",
        "raw_scores": {
          "intelligence_score": 11.301889825217446,
          "coding_score": 4.9,
          "context_window": 128000,
          "gdpval": 334.31040902776294,
          "terminalbench_hard": 0.008,
          "tau2": 0.164,
          "lcr": 15.7,
          "hle": 5.1,
          "gpqa": 25.9,
          "scicode": 13.2,
          "ifbench": 28.6,
          "aime25": 4.3,
          "critpt": 0.0,
          "livecodebench": 11.6
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 11.301889825217446,
        "coding_score": 4.9,
        "gdpval": 334.31040902776294,
        "terminalbench_hard": 0.008,
        "tau2": 0.164,
        "lcr": 15.7,
        "hle": 5.1,
        "gpqa": 25.9,
        "scicode": 13.2,
        "ifbench": 28.6,
        "aime25": 4.3,
        "critpt": 0.0,
        "livecodebench": 11.6
      }
    },
    "confidence_score": 0.79,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "llama-32-instruct-3b",
    "canonical_name": "Llama 3.2 Instruct 3B",
    "model_name": "Llama 3.2 Instruct 3B",
    "aliases": [
      "Llama 3.2 Instruct 3B"
    ],
    "provider": "Meta",
    "context_window": 128000,
    "open_source": true,
    "arena_votes": null,
    "license_type": "LLAMA 3.2 COMMUNITY LICENSE AGREEMENT",
    "creator": null,
    "intelligence_score": 9.7022,
    "coding_score": null,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": null,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": null,
    "tau2": 0.211,
    "lcr": 2.0,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": 5.2,
    "gpqa": 25.5,
    "scicode": 5.2,
    "ifbench": 26.2,
    "aime25": 3.3,
    "critpt": null,
    "mmmu_pro": null,
    "livecodebench": 8.3,
    "sources": [
      "Artificial Analysis"
    ],
    "source_count": 1,
    "data_tier": 2,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.912913+00:00",
        "confidence": 0.72,
        "raw_name": "Llama 3.2 Instruct 3B",
        "raw_scores": {
          "intelligence_score": 9.702177369527044,
          "context_window": 128000,
          "tau2": 0.211,
          "lcr": 2.0,
          "hle": 5.2,
          "gpqa": 25.5,
          "scicode": 5.2,
          "ifbench": 26.2,
          "aime25": 3.3,
          "livecodebench": 8.3
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 9.702177369527044,
        "tau2": 0.211,
        "lcr": 2.0,
        "hle": 5.2,
        "gpqa": 25.5,
        "scicode": 5.2,
        "ifbench": 26.2,
        "aime25": 3.3,
        "livecodebench": 8.3
      }
    },
    "confidence_score": 0.72,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "llama-3-instruct-70b",
    "canonical_name": "Llama 3 Instruct 70B",
    "model_name": "Llama 3 Instruct 70B",
    "aliases": [
      "Llama 3 Instruct 70B"
    ],
    "provider": "Meta",
    "context_window": 8192,
    "open_source": true,
    "arena_votes": null,
    "license_type": "META LLAMA 3 COMMUNITY LICENSE AGREEMENT",
    "creator": null,
    "intelligence_score": 10.1922,
    "coding_score": 6.79,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": null,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": 0.008,
    "tau2": 0.0,
    "lcr": 0.0,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": 4.4,
    "gpqa": 37.9,
    "scicode": 18.9,
    "ifbench": 37.1,
    "aime25": null,
    "critpt": 0.0,
    "mmmu_pro": null,
    "livecodebench": 19.8,
    "sources": [
      "Artificial Analysis"
    ],
    "source_count": 1,
    "data_tier": 2,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.912948+00:00",
        "confidence": 0.72,
        "raw_name": "Llama 3 Instruct 70B",
        "raw_scores": {
          "intelligence_score": 10.192203109851526,
          "coding_score": 6.79,
          "context_window": 8192,
          "terminalbench_hard": 0.008,
          "tau2": 0.0,
          "lcr": 0.0,
          "hle": 4.4,
          "gpqa": 37.9,
          "scicode": 18.9,
          "ifbench": 37.1,
          "critpt": 0.0,
          "livecodebench": 19.8
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 10.192203109851526,
        "coding_score": 6.79,
        "terminalbench_hard": 0.008,
        "tau2": 0.0,
        "lcr": 0.0,
        "hle": 4.4,
        "gpqa": 37.9,
        "scicode": 18.9,
        "ifbench": 37.1,
        "critpt": 0.0,
        "livecodebench": 19.8
      }
    },
    "confidence_score": 0.72,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "llama-3-instruct-8b",
    "canonical_name": "Llama 3 Instruct 8B",
    "model_name": "Llama 3 Instruct 8B",
    "aliases": [
      "Llama 3 Instruct 8B"
    ],
    "provider": "Meta",
    "context_window": 8192,
    "open_source": true,
    "arena_votes": null,
    "license_type": "META LLAMA 3 COMMUNITY LICENSE AGREEMENT",
    "creator": null,
    "intelligence_score": 8.701,
    "coding_score": 3.98,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": null,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": 0.0,
    "tau2": 0.0,
    "lcr": 0.0,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": 5.1,
    "gpqa": 29.6,
    "scicode": 11.9,
    "ifbench": 24.6,
    "aime25": null,
    "critpt": 0.0,
    "mmmu_pro": null,
    "livecodebench": 9.6,
    "sources": [
      "Artificial Analysis"
    ],
    "source_count": 1,
    "data_tier": 2,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.912984+00:00",
        "confidence": 0.72,
        "raw_name": "Llama 3 Instruct 8B",
        "raw_scores": {
          "intelligence_score": 8.701018683736894,
          "coding_score": 3.98,
          "context_window": 8192,
          "terminalbench_hard": 0.0,
          "tau2": 0.0,
          "lcr": 0.0,
          "hle": 5.1,
          "gpqa": 29.6,
          "scicode": 11.9,
          "ifbench": 24.6,
          "critpt": 0.0,
          "livecodebench": 9.6
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 8.701018683736894,
        "coding_score": 3.98,
        "terminalbench_hard": 0.0,
        "tau2": 0.0,
        "lcr": 0.0,
        "hle": 5.1,
        "gpqa": 29.6,
        "scicode": 11.9,
        "ifbench": 24.6,
        "critpt": 0.0,
        "livecodebench": 9.6
      }
    },
    "confidence_score": 0.72,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "llama-32-instruct-1b",
    "canonical_name": "Llama 3.2 Instruct 1B",
    "model_name": "Llama 3.2 Instruct 1B",
    "aliases": [
      "Llama 3.2 Instruct 1B"
    ],
    "provider": "Meta",
    "context_window": 128000,
    "open_source": true,
    "arena_votes": null,
    "license_type": "LLAMA 3.2 COMMUNITY LICENSE AGREEMENT",
    "creator": null,
    "intelligence_score": 9.1307,
    "coding_score": 0.58,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": null,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": 0.0,
    "tau2": 0.0,
    "lcr": 5.0,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": 5.3,
    "gpqa": 19.6,
    "scicode": 1.7,
    "ifbench": 22.8,
    "aime25": 0.0,
    "critpt": 0.0,
    "mmmu_pro": null,
    "livecodebench": 1.9,
    "sources": [
      "Artificial Analysis"
    ],
    "source_count": 1,
    "data_tier": 2,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.913025+00:00",
        "confidence": 0.79,
        "raw_name": "Llama 3.2 Instruct 1B",
        "raw_scores": {
          "intelligence_score": 9.13072365693697,
          "coding_score": 0.58,
          "context_window": 128000,
          "terminalbench_hard": 0.0,
          "tau2": 0.0,
          "lcr": 5.0,
          "hle": 5.3,
          "gpqa": 19.6,
          "scicode": 1.7,
          "ifbench": 22.8,
          "aime25": 0.0,
          "critpt": 0.0,
          "livecodebench": 1.9
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 9.13072365693697,
        "coding_score": 0.58,
        "terminalbench_hard": 0.0,
        "tau2": 0.0,
        "lcr": 5.0,
        "hle": 5.3,
        "gpqa": 19.6,
        "scicode": 1.7,
        "ifbench": 22.8,
        "aime25": 0.0,
        "critpt": 0.0,
        "livecodebench": 1.9
      }
    },
    "confidence_score": 0.79,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "llama-2-chat-70b",
    "canonical_name": "Llama 2 Chat 70B",
    "model_name": "Llama 2 Chat 70B",
    "aliases": [
      "Llama 2 Chat 70B"
    ],
    "provider": "Meta",
    "context_window": 4096,
    "open_source": true,
    "arena_votes": null,
    "license_type": "Llama 2 Community License Agreement",
    "creator": null,
    "intelligence_score": 8.3708,
    "coding_score": null,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": null,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": 0.0,
    "latency_seconds": 0.0,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": 0.0,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": null,
    "tau2": null,
    "lcr": null,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": 5.0,
    "gpqa": 32.7,
    "scicode": null,
    "ifbench": null,
    "aime25": null,
    "critpt": null,
    "mmmu_pro": null,
    "livecodebench": 9.8,
    "sources": [
      "Artificial Analysis"
    ],
    "source_count": 1,
    "data_tier": 2,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.913056+00:00",
        "confidence": 0.8600000000000001,
        "raw_name": "Llama 2 Chat 70B",
        "raw_scores": {
          "intelligence_score": 8.370802665737399,
          "blended_cost_per_1m": 0.0,
          "latency_seconds": 0.0,
          "tokens_per_second": 0.0,
          "context_window": 4096,
          "hle": 5.0,
          "gpqa": 32.7,
          "livecodebench": 9.8
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 8.370802665737399,
        "hle": 5.0,
        "gpqa": 32.7,
        "livecodebench": 9.8
      }
    },
    "confidence_score": 0.86,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "llama-2-chat-13b",
    "canonical_name": "Llama 2 Chat 13B",
    "model_name": "Llama 2 Chat 13B",
    "aliases": [
      "Llama 2 Chat 13B"
    ],
    "provider": "Meta",
    "context_window": 4096,
    "open_source": true,
    "arena_votes": null,
    "license_type": "Llama 2 Community License Agreement",
    "creator": null,
    "intelligence_score": 8.3576,
    "coding_score": null,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": null,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": 0.0,
    "latency_seconds": 0.0,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": 0.0,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": null,
    "tau2": null,
    "lcr": null,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": 4.7,
    "gpqa": 32.1,
    "scicode": 11.8,
    "ifbench": null,
    "aime25": null,
    "critpt": null,
    "mmmu_pro": null,
    "livecodebench": 9.8,
    "sources": [
      "Artificial Analysis"
    ],
    "source_count": 1,
    "data_tier": 2,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.913086+00:00",
        "confidence": 0.8600000000000001,
        "raw_name": "Llama 2 Chat 13B",
        "raw_scores": {
          "intelligence_score": 8.35759395005398,
          "blended_cost_per_1m": 0.0,
          "latency_seconds": 0.0,
          "tokens_per_second": 0.0,
          "context_window": 4096,
          "hle": 4.7,
          "gpqa": 32.1,
          "scicode": 11.8,
          "livecodebench": 9.8
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 8.35759395005398,
        "hle": 4.7,
        "gpqa": 32.1,
        "scicode": 11.8,
        "livecodebench": 9.8
      }
    },
    "confidence_score": 0.86,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "llama-2-chat-7b",
    "canonical_name": "Llama 2 Chat 7B",
    "model_name": "Llama 2 Chat 7B",
    "aliases": [
      "Llama 2 Chat 7B"
    ],
    "provider": "Meta",
    "context_window": 4096,
    "open_source": true,
    "arena_votes": null,
    "license_type": "Llama 2 Community License Agreement",
    "creator": null,
    "intelligence_score": 9.7385,
    "coding_score": null,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": null,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": 0.1,
    "latency_seconds": 0.46,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": 117.0,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": null,
    "tau2": null,
    "lcr": null,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": 5.8,
    "gpqa": 22.7,
    "scicode": 0.0,
    "ifbench": null,
    "aime25": null,
    "critpt": null,
    "mmmu_pro": null,
    "livecodebench": 0.2,
    "sources": [
      "Artificial Analysis"
    ],
    "source_count": 1,
    "data_tier": 2,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.913120+00:00",
        "confidence": 0.8600000000000001,
        "raw_name": "Llama 2 Chat 7B",
        "raw_scores": {
          "intelligence_score": 9.738523435238086,
          "blended_cost_per_1m": 0.1,
          "latency_seconds": 0.46,
          "tokens_per_second": 117.0,
          "context_window": 4096,
          "hle": 5.8,
          "gpqa": 22.7,
          "scicode": 0.0,
          "livecodebench": 0.2
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 9.738523435238086,
        "hle": 5.8,
        "gpqa": 22.7,
        "scicode": 0.0,
        "livecodebench": 0.2
      }
    },
    "confidence_score": 0.86,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "gemini-20-pro-experimental",
    "canonical_name": "Gemini 2.0 Pro Experimental (Feb '25)",
    "model_name": "Gemini 2.0 Pro Experimental (Feb '25)",
    "aliases": [
      "Gemini 2.0 Pro Experimental (Feb '25)"
    ],
    "provider": "Google",
    "context_window": 2000000,
    "open_source": null,
    "arena_votes": null,
    "license_type": null,
    "creator": null,
    "intelligence_score": 18.0526,
    "coding_score": 25.5479,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": null,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": null,
    "tau2": null,
    "lcr": null,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": 6.8,
    "gpqa": 62.2,
    "scicode": 31.2,
    "ifbench": null,
    "aime25": null,
    "critpt": null,
    "mmmu_pro": null,
    "livecodebench": 34.7,
    "sources": [
      "Artificial Analysis"
    ],
    "source_count": 1,
    "data_tier": 2,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.913151+00:00",
        "confidence": 0.72,
        "raw_name": "Gemini 2.0 Pro Experimental (Feb '25)",
        "raw_scores": {
          "intelligence_score": 18.052648552336592,
          "coding_score": 25.5478568,
          "context_window": 2000000,
          "hle": 6.8,
          "gpqa": 62.2,
          "scicode": 31.2,
          "livecodebench": 34.7
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 18.052648552336592,
        "coding_score": 25.5478568,
        "hle": 6.8,
        "gpqa": 62.2,
        "scicode": 31.2,
        "livecodebench": 34.7
      }
    },
    "confidence_score": 0.72,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "gemini-20-flash",
    "canonical_name": "Gemini 2.0 Flash (experimental)",
    "model_name": "Gemini 2.0 Flash (experimental)",
    "aliases": [
      "Gemini 2.0 Flash (Feb '25)",
      "Gemini 2.0 Flash (experimental)",
      "gemini-2.0-flash-001"
    ],
    "provider": "Google",
    "context_window": 1000000,
    "open_source": null,
    "arena_votes": 44686,
    "license_type": "Proprietary",
    "creator": null,
    "intelligence_score": 17.2109,
    "coding_score": 13.64,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 1360.78,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": 617.4766,
    "terminalbench_hard": 0.038,
    "tau2": 0.295,
    "lcr": 28.3,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": 5.0292,
    "gpqa": 62.8868,
    "scicode": 33.616,
    "ifbench": 40.2,
    "aime25": 21.7,
    "critpt": 0.0,
    "mmmu_pro": null,
    "livecodebench": 27.8028,
    "sources": [
      "Artificial Analysis",
      "LMSYS Chatbot Arena"
    ],
    "source_count": 2,
    "data_tier": 1,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.913178+00:00",
        "confidence": 0.65,
        "raw_name": "Gemini 2.0 Flash (experimental)",
        "raw_scores": {
          "intelligence_score": 16.7744977006624,
          "context_window": 1000000,
          "hle": 4.7,
          "gpqa": 63.6,
          "scicode": 34.0,
          "livecodebench": 21.0
        }
      },
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.913295+00:00",
        "confidence": 0.79,
        "raw_name": "Gemini 2.0 Flash (Feb '25)",
        "raw_scores": {
          "intelligence_score": 17.570036030936603,
          "coding_score": 13.64,
          "context_window": 1000000,
          "gdpval": 617.4765994714783,
          "terminalbench_hard": 0.038,
          "tau2": 0.295,
          "lcr": 28.3,
          "hle": 5.3,
          "gpqa": 62.3,
          "scicode": 33.3,
          "ifbench": 40.2,
          "aime25": 21.7,
          "critpt": 0.0,
          "livecodebench": 33.4
        }
      },
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.892915+00:00",
        "confidence": 1.0,
        "raw_name": "gemini-2.0-flash-001",
        "raw_scores": {
          "arena_elo": 1360.78,
          "arena_votes": 44686
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 16.7744977006624,
        "hle": 4.7,
        "gpqa": 63.6,
        "scicode": 34.0,
        "livecodebench": 21.0,
        "coding_score": 13.64,
        "gdpval": 617.4765994714783,
        "terminalbench_hard": 0.038,
        "tau2": 0.295,
        "lcr": 28.3,
        "ifbench": 40.2,
        "aime25": 21.7,
        "critpt": 0.0
      },
      "LMSYS Chatbot Arena": {
        "arena_elo": 1360.78
      }
    },
    "confidence_score": 0.8133,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "gemini-15-pro",
    "canonical_name": "Gemini 1.5 Pro (Sep '24)",
    "model_name": "Gemini 1.5 Pro (Sep '24)",
    "aliases": [
      "Gemini 1.5 Pro (May '24)",
      "Gemini 1.5 Pro (Sep '24)",
      "gemini-1.5-pro-001",
      "gemini-1.5-pro-002"
    ],
    "provider": "Google",
    "context_window": 2000000,
    "open_source": null,
    "arena_votes": 79132,
    "license_type": "Proprietary",
    "creator": null,
    "intelligence_score": 13.9949,
    "coding_score": 21.7064,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 1337.125,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": null,
    "tau2": null,
    "lcr": null,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": 4.4,
    "gpqa": 48.0,
    "scicode": 28.45,
    "ifbench": null,
    "aime25": null,
    "critpt": null,
    "mmmu_pro": 55.0,
    "livecodebench": 28.0,
    "sources": [
      "Artificial Analysis",
      "LMSYS Chatbot Arena"
    ],
    "source_count": 2,
    "data_tier": 1,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.913223+00:00",
        "confidence": 0.72,
        "raw_name": "Gemini 1.5 Pro (Sep '24)",
        "raw_scores": {
          "intelligence_score": 15.99411151166208,
          "coding_score": 23.6279234,
          "context_window": 2000000,
          "hle": 4.9,
          "gpqa": 58.9,
          "scicode": 29.5,
          "mmmu_pro": 55.0,
          "livecodebench": 31.6
        }
      },
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.913988+00:00",
        "confidence": 0.72,
        "raw_name": "Gemini 1.5 Pro (May '24)",
        "raw_scores": {
          "intelligence_score": 11.995643433356523,
          "coding_score": 19.7848523,
          "context_window": 2000000,
          "hle": 3.9,
          "gpqa": 37.1,
          "scicode": 27.4,
          "livecodebench": 24.4
        }
      },
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.893028+00:00",
        "confidence": 1.0,
        "raw_name": "gemini-1.5-pro-002",
        "raw_scores": {
          "arena_elo": 1351.24,
          "arena_votes": 55607
        }
      },
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.893534+00:00",
        "confidence": 1.0,
        "raw_name": "gemini-1.5-pro-001",
        "raw_scores": {
          "arena_elo": 1323.01,
          "arena_votes": 79132
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 15.99411151166208,
        "coding_score": 23.6279234,
        "hle": 4.9,
        "gpqa": 58.9,
        "scicode": 29.5,
        "mmmu_pro": 55.0,
        "livecodebench": 31.6
      },
      "LMSYS Chatbot Arena": {
        "arena_elo": 1351.24
      }
    },
    "confidence_score": 0.86,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "gemini-20-flash-lite",
    "canonical_name": "Gemini 2.0 Flash-Lite (Preview)",
    "model_name": "Gemini 2.0 Flash-Lite (Preview)",
    "aliases": [
      "Gemini 2.0 Flash-Lite (Feb '25)",
      "Gemini 2.0 Flash-Lite (Preview)"
    ],
    "provider": "Google",
    "context_window": 1000000,
    "open_source": null,
    "arena_votes": null,
    "license_type": null,
    "creator": null,
    "intelligence_score": 14.5797,
    "coding_score": null,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": null,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": 0.0,
    "latency_seconds": 0.0,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": 0.0,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": null,
    "tau2": null,
    "lcr": null,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": 4.0556,
    "gpqa": 53.8987,
    "scicode": 24.8291,
    "ifbench": null,
    "aime25": null,
    "critpt": null,
    "mmmu_pro": null,
    "livecodebench": 18.1583,
    "sources": [
      "Artificial Analysis"
    ],
    "source_count": 1,
    "data_tier": 2,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.913255+00:00",
        "confidence": 0.8600000000000001,
        "raw_name": "Gemini 2.0 Flash-Lite (Preview)",
        "raw_scores": {
          "intelligence_score": 14.487085601010705,
          "blended_cost_per_1m": 0.0,
          "latency_seconds": 0.0,
          "tokens_per_second": 0.0,
          "context_window": 1000000,
          "hle": 4.4,
          "gpqa": 54.2,
          "scicode": 24.7,
          "livecodebench": 17.9
        }
      },
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.913457+00:00",
        "confidence": 0.65,
        "raw_name": "Gemini 2.0 Flash-Lite (Feb '25)",
        "raw_scores": {
          "intelligence_score": 14.702194584063244,
          "context_window": 1000000,
          "hle": 3.6,
          "gpqa": 53.5,
          "scicode": 25.0,
          "livecodebench": 18.5
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 14.487085601010705,
        "hle": 4.4,
        "gpqa": 54.2,
        "scicode": 24.7,
        "livecodebench": 17.9
      }
    },
    "confidence_score": 0.755,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "gemini-15-flash",
    "canonical_name": "Gemini 1.5 Flash (Sep '24)",
    "model_name": "Gemini 1.5 Flash (Sep '24)",
    "aliases": [
      "Gemini 1.5 Flash (May '24)",
      "Gemini 1.5 Flash (Sep '24)",
      "gemini-1.5-flash-001",
      "gemini-1.5-flash-002"
    ],
    "provider": "Google",
    "context_window": 1000000,
    "open_source": null,
    "arena_votes": 62823,
    "license_type": "Proprietary",
    "creator": null,
    "intelligence_score": 12.1263,
    "coding_score": null,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 1297.625,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": null,
    "tau2": null,
    "lcr": null,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": 3.85,
    "gpqa": 39.35,
    "scicode": 22.4,
    "ifbench": null,
    "aime25": null,
    "critpt": null,
    "mmmu_pro": 48.4,
    "livecodebench": 23.45,
    "sources": [
      "Artificial Analysis",
      "LMSYS Chatbot Arena"
    ],
    "source_count": 2,
    "data_tier": 1,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.913327+00:00",
        "confidence": 0.65,
        "raw_name": "Gemini 1.5 Flash (Sep '24)",
        "raw_scores": {
          "intelligence_score": 13.791318335280737,
          "context_window": 1000000,
          "hle": 3.5,
          "gpqa": 46.3,
          "scicode": 26.7,
          "mmmu_pro": 48.4,
          "livecodebench": 27.3
        }
      },
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.913631+00:00",
        "confidence": 0.65,
        "raw_name": "Gemini 1.5 Flash (May '24)",
        "raw_scores": {
          "intelligence_score": 10.46126910425808,
          "context_window": 1000000,
          "hle": 4.2,
          "gpqa": 32.4,
          "scicode": 18.1,
          "livecodebench": 19.6
        }
      },
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.893773+00:00",
        "confidence": 1.0,
        "raw_name": "gemini-1.5-flash-002",
        "raw_scores": {
          "arena_elo": 1309.7,
          "arena_votes": 34909
        }
      },
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.894048+00:00",
        "confidence": 1.0,
        "raw_name": "gemini-1.5-flash-001",
        "raw_scores": {
          "arena_elo": 1285.55,
          "arena_votes": 62823
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 13.791318335280737,
        "hle": 3.5,
        "gpqa": 46.3,
        "scicode": 26.7,
        "mmmu_pro": 48.4,
        "livecodebench": 27.3
      },
      "LMSYS Chatbot Arena": {
        "arena_elo": 1309.7
      }
    },
    "confidence_score": 0.825,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "gemini-15-flash-8b",
    "canonical_name": "Gemini 1.5 Flash-8B",
    "model_name": "Gemini 1.5 Flash-8B",
    "aliases": [
      "Gemini 1.5 Flash-8B"
    ],
    "provider": "Google",
    "context_window": 1048576,
    "open_source": null,
    "arena_votes": null,
    "license_type": null,
    "creator": null,
    "intelligence_score": 11.1317,
    "coding_score": null,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": null,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": 0.0,
    "latency_seconds": 0.0,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": 0.0,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": null,
    "tau2": null,
    "lcr": null,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": 4.5,
    "gpqa": 35.9,
    "scicode": 22.9,
    "ifbench": null,
    "aime25": null,
    "critpt": null,
    "mmmu_pro": 36.5,
    "livecodebench": 21.7,
    "sources": [
      "Artificial Analysis"
    ],
    "source_count": 1,
    "data_tier": 2,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.913361+00:00",
        "confidence": 0.8600000000000001,
        "raw_name": "Gemini 1.5 Flash-8B",
        "raw_scores": {
          "intelligence_score": 11.131677468292837,
          "blended_cost_per_1m": 0.0,
          "latency_seconds": 0.0,
          "tokens_per_second": 0.0,
          "context_window": 1048576,
          "hle": 4.5,
          "gpqa": 35.9,
          "scicode": 22.9,
          "mmmu_pro": 36.5,
          "livecodebench": 21.7
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 11.131677468292837,
        "hle": 4.5,
        "gpqa": 35.9,
        "scicode": 22.9,
        "mmmu_pro": 36.5,
        "livecodebench": 21.7
      }
    },
    "confidence_score": 0.86,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "palm-2",
    "canonical_name": "PALM-2",
    "model_name": "PALM-2",
    "aliases": [
      "PALM-2",
      "palm-2"
    ],
    "provider": "Google",
    "context_window": 8000,
    "open_source": null,
    "arena_votes": 8554,
    "license_type": "Proprietary",
    "creator": null,
    "intelligence_score": 8.594,
    "coding_score": 4.5591,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 1137.12,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": 0.0,
    "latency_seconds": 0.0,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": 0.0,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": null,
    "tau2": null,
    "lcr": null,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": null,
    "gpqa": null,
    "scicode": null,
    "ifbench": null,
    "aime25": null,
    "critpt": null,
    "mmmu_pro": null,
    "livecodebench": null,
    "sources": [
      "Artificial Analysis",
      "LMSYS Chatbot Arena"
    ],
    "source_count": 2,
    "data_tier": 1,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.913385+00:00",
        "confidence": 0.72,
        "raw_name": "PALM-2",
        "raw_scores": {
          "intelligence_score": 8.594046799469973,
          "coding_score": 4.5591306,
          "blended_cost_per_1m": 0.0,
          "latency_seconds": 0.0,
          "tokens_per_second": 0.0,
          "context_window": 8000
        }
      },
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.895137+00:00",
        "confidence": 1.0,
        "raw_name": "palm-2",
        "raw_scores": {
          "arena_elo": 1137.12,
          "arena_votes": 8554
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 8.594046799469973,
        "coding_score": 4.5591306
      },
      "LMSYS Chatbot Arena": {
        "arena_elo": 1137.12
      }
    },
    "confidence_score": 0.86,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "gemini-25-flash",
    "canonical_name": "Gemini 2.5 Flash (Non-reasoning)",
    "model_name": "Gemini 2.5 Flash (Non-reasoning)",
    "aliases": [
      "Gemini 2.5 Flash (Non-reasoning)",
      "Gemini 2.5 Flash (Reasoning)",
      "gemini-2.5-flash",
      "gemini-2.5-flash-preview-09-2025"
    ],
    "provider": "Google",
    "context_window": 1000000,
    "open_source": null,
    "arena_votes": 96569,
    "license_type": "Proprietary",
    "creator": null,
    "intelligence_score": 24.0592,
    "coding_score": 19.985,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 1407.56,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": 730.9105,
    "terminalbench_hard": 0.1285,
    "tau2": 0.2325,
    "lcr": 53.8,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": 8.1,
    "gpqa": 73.65,
    "scicode": 34.25,
    "ifbench": 44.65,
    "aime25": 66.8,
    "critpt": 1.25,
    "mmmu_pro": 67.3,
    "livecodebench": 59.5,
    "sources": [
      "Artificial Analysis",
      "LMSYS Chatbot Arena"
    ],
    "source_count": 2,
    "data_tier": 1,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.913428+00:00",
        "confidence": 0.79,
        "raw_name": "Gemini 2.5 Flash (Non-reasoning)",
        "raw_scores": {
          "intelligence_score": 21.078423891125734,
          "coding_score": 17.76,
          "context_window": 1000000,
          "gdpval": 753.0797348623781,
          "terminalbench_hard": 0.121,
          "tau2": 0.149,
          "lcr": 45.9,
          "hle": 5.1,
          "gpqa": 68.3,
          "scicode": 29.1,
          "ifbench": 39.0,
          "aime25": 60.3,
          "critpt": 1.4,
          "mmmu_pro": 65.5,
          "livecodebench": 49.5
        }
      },
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.913602+00:00",
        "confidence": 0.79,
        "raw_name": "Gemini 2.5 Flash (Reasoning)",
        "raw_scores": {
          "intelligence_score": 27.04,
          "coding_score": 22.21,
          "context_window": 1000000,
          "gdpval": 708.7412727542157,
          "terminalbench_hard": 0.136,
          "tau2": 0.316,
          "lcr": 61.7,
          "hle": 11.1,
          "gpqa": 79.0,
          "scicode": 39.4,
          "ifbench": 50.3,
          "aime25": 73.3,
          "critpt": 1.1,
          "mmmu_pro": 69.1,
          "livecodebench": 69.5
        }
      },
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.892124+00:00",
        "confidence": 1.0,
        "raw_name": "gemini-2.5-flash",
        "raw_scores": {
          "arena_elo": 1410.86,
          "arena_votes": 96569
        }
      },
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.892180+00:00",
        "confidence": 1.0,
        "raw_name": "gemini-2.5-flash-preview-09-2025",
        "raw_scores": {
          "arena_elo": 1404.26,
          "arena_votes": 32541
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 21.078423891125734,
        "coding_score": 17.76,
        "gdpval": 753.0797348623781,
        "terminalbench_hard": 0.121,
        "tau2": 0.149,
        "lcr": 45.9,
        "hle": 5.1,
        "gpqa": 68.3,
        "scicode": 29.1,
        "ifbench": 39.0,
        "aime25": 60.3,
        "critpt": 1.4,
        "mmmu_pro": 65.5,
        "livecodebench": 49.5
      },
      "LMSYS Chatbot Arena": {
        "arena_elo": 1410.86
      }
    },
    "confidence_score": 0.895,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "gemini-10-ultra",
    "canonical_name": "Gemini 1.0 Ultra",
    "model_name": "Gemini 1.0 Ultra",
    "aliases": [
      "Gemini 1.0 Ultra"
    ],
    "provider": "Google",
    "context_window": 32768,
    "open_source": null,
    "arena_votes": null,
    "license_type": null,
    "creator": null,
    "intelligence_score": 10.1467,
    "coding_score": 17.6498,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": null,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": 0.0,
    "latency_seconds": 0.0,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": 0.0,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": null,
    "tau2": null,
    "lcr": null,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": null,
    "gpqa": null,
    "scicode": null,
    "ifbench": null,
    "aime25": null,
    "critpt": null,
    "mmmu_pro": null,
    "livecodebench": null,
    "sources": [
      "Artificial Analysis"
    ],
    "source_count": 1,
    "data_tier": 2,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.913482+00:00",
        "confidence": 0.72,
        "raw_name": "Gemini 1.0 Ultra",
        "raw_scores": {
          "intelligence_score": 10.146700927796493,
          "coding_score": 17.6497963,
          "blended_cost_per_1m": 0.0,
          "latency_seconds": 0.0,
          "tokens_per_second": 0.0,
          "context_window": 32768
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 10.146700927796493,
        "coding_score": 17.6497963
      }
    },
    "confidence_score": 0.72,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "gemini-25-flash-preview",
    "canonical_name": "Gemini 2.5 Flash Preview (Sep '25) (Non-reasoning)",
    "model_name": "Gemini 2.5 Flash Preview (Sep '25) (Non-reasoning)",
    "aliases": [
      "Gemini 2.5 Flash Preview (Non-reasoning)",
      "Gemini 2.5 Flash Preview (Reasoning)",
      "Gemini 2.5 Flash Preview (Sep '25) (Non-reasoning)",
      "Gemini 2.5 Flash Preview (Sep '25) (Reasoning)"
    ],
    "provider": "Google",
    "context_window": 1000000,
    "open_source": null,
    "arena_votes": null,
    "license_type": null,
    "creator": null,
    "intelligence_score": 25.1018,
    "coding_score": 23.355,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": null,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": 984.4604,
    "terminalbench_hard": 0.1555,
    "tau2": 0.37,
    "lcr": 60.5,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": 9.3698,
    "gpqa": 71.924,
    "scicode": 34.7569,
    "ifbench": 47.9,
    "aime25": 67.5,
    "critpt": 0.15,
    "mmmu_pro": 68.8372,
    "livecodebench": 57.2628,
    "sources": [
      "Artificial Analysis"
    ],
    "source_count": 1,
    "data_tier": 2,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.913524+00:00",
        "confidence": 0.79,
        "raw_name": "Gemini 2.5 Flash Preview (Sep '25) (Non-reasoning)",
        "raw_scores": {
          "intelligence_score": 25.7,
          "coding_score": 22.1,
          "context_window": 1000000,
          "gdpval": 876.0320066030342,
          "terminalbench_hard": 0.144,
          "tau2": 0.284,
          "lcr": 56.7,
          "hle": 7.8,
          "gpqa": 76.6,
          "scicode": 37.5,
          "ifbench": 43.5,
          "aime25": 56.7,
          "critpt": 0.0,
          "mmmu_pro": 70.2,
          "livecodebench": 62.5
        }
      },
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.913898+00:00",
        "confidence": 0.65,
        "raw_name": "Gemini 2.5 Flash Preview (Non-reasoning)",
        "raw_scores": {
          "intelligence_score": 17.844790713497186,
          "context_window": 1000000,
          "hle": 5.0,
          "gpqa": 59.4,
          "scicode": 23.3,
          "mmmu_pro": 62.0,
          "livecodebench": 40.6
        }
      },
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.913926+00:00",
        "confidence": 0.65,
        "raw_name": "Gemini 2.5 Flash Preview (Reasoning)",
        "raw_scores": {
          "intelligence_score": 24.29283568571511,
          "context_window": 1000000,
          "hle": 11.6,
          "gpqa": 69.8,
          "scicode": 35.9,
          "livecodebench": 50.5
        }
      },
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.914030+00:00",
        "confidence": 0.79,
        "raw_name": "Gemini 2.5 Flash Preview (Sep '25) (Reasoning)",
        "raw_scores": {
          "intelligence_score": 31.14,
          "coding_score": 24.61,
          "context_window": 1000000,
          "gdpval": 1092.888769989659,
          "terminalbench_hard": 0.167,
          "tau2": 0.456,
          "lcr": 64.3,
          "hle": 12.7,
          "gpqa": 79.3,
          "scicode": 40.5,
          "ifbench": 52.3,
          "aime25": 78.3,
          "critpt": 0.3,
          "mmmu_pro": 73.1,
          "livecodebench": 71.3
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 25.7,
        "coding_score": 22.1,
        "gdpval": 876.0320066030342,
        "terminalbench_hard": 0.144,
        "tau2": 0.284,
        "lcr": 56.7,
        "hle": 7.8,
        "gpqa": 76.6,
        "scicode": 37.5,
        "ifbench": 43.5,
        "aime25": 56.7,
        "critpt": 0.0,
        "mmmu_pro": 70.2,
        "livecodebench": 62.5
      }
    },
    "confidence_score": 0.72,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "gemini-10-pro",
    "canonical_name": "Gemini 1.0 Pro",
    "model_name": "Gemini 1.0 Pro",
    "aliases": [
      "Gemini 1.0 Pro"
    ],
    "provider": "Google",
    "context_window": 32768,
    "open_source": null,
    "arena_votes": null,
    "license_type": null,
    "creator": null,
    "intelligence_score": 8.5018,
    "coding_score": null,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": null,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": 0.0,
    "latency_seconds": 0.0,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": 0.0,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": null,
    "tau2": null,
    "lcr": null,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": 4.6,
    "gpqa": 27.7,
    "scicode": 11.7,
    "ifbench": null,
    "aime25": null,
    "critpt": null,
    "mmmu_pro": null,
    "livecodebench": 11.6,
    "sources": [
      "Artificial Analysis"
    ],
    "source_count": 1,
    "data_tier": 2,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.913559+00:00",
        "confidence": 0.8600000000000001,
        "raw_name": "Gemini 1.0 Pro",
        "raw_scores": {
          "intelligence_score": 8.501805478425053,
          "blended_cost_per_1m": 0.0,
          "latency_seconds": 0.0,
          "tokens_per_second": 0.0,
          "context_window": 32768,
          "hle": 4.6,
          "gpqa": 27.7,
          "scicode": 11.7,
          "livecodebench": 11.6
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 8.501805478425053,
        "hle": 4.6,
        "gpqa": 27.7,
        "scicode": 11.7,
        "livecodebench": 11.6
      }
    },
    "confidence_score": 0.86,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "gemma-3n-e4b-instruct-preview",
    "canonical_name": "Gemma 3n E4B Instruct Preview (May '25)",
    "model_name": "Gemma 3n E4B Instruct Preview (May '25)",
    "aliases": [
      "Gemma 3n E4B Instruct Preview (May '25)"
    ],
    "provider": "Google",
    "context_window": 32000,
    "open_source": null,
    "arena_votes": null,
    "license_type": null,
    "creator": null,
    "intelligence_score": 10.059,
    "coding_score": null,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": null,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": null,
    "tau2": null,
    "lcr": null,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": 4.9,
    "gpqa": 27.8,
    "scicode": 8.6,
    "ifbench": null,
    "aime25": null,
    "critpt": null,
    "mmmu_pro": null,
    "livecodebench": 13.8,
    "sources": [
      "Artificial Analysis"
    ],
    "source_count": 1,
    "data_tier": 2,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.913658+00:00",
        "confidence": 0.65,
        "raw_name": "Gemma 3n E4B Instruct Preview (May '25)",
        "raw_scores": {
          "intelligence_score": 10.058952665396129,
          "context_window": 32000,
          "hle": 4.9,
          "gpqa": 27.8,
          "scicode": 8.6,
          "livecodebench": 13.8
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 10.058952665396129,
        "hle": 4.9,
        "gpqa": 27.8,
        "scicode": 8.6,
        "livecodebench": 13.8
      }
    },
    "confidence_score": 0.65,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "gemini-25-flash-lite",
    "canonical_name": "Gemini 2.5 Flash-Lite (Non-reasoning)",
    "model_name": "Gemini 2.5 Flash-Lite (Non-reasoning)",
    "aliases": [
      "Gemini 2.5 Flash-Lite (Non-reasoning)",
      "Gemini 2.5 Flash-Lite (Reasoning)"
    ],
    "provider": "Google",
    "context_window": 1000000,
    "open_source": null,
    "arena_votes": null,
    "license_type": null,
    "creator": null,
    "intelligence_score": 16.7897,
    "coding_score": 8.445,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": null,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": 360.7193,
    "terminalbench_hard": 0.034,
    "tau2": 0.187,
    "lcr": 41.3,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": 5.05,
    "gpqa": 54.95,
    "scicode": 18.5,
    "ifbench": 40.7,
    "aime25": 44.3,
    "critpt": 0.0,
    "mmmu_pro": 56.1,
    "livecodebench": 49.65,
    "sources": [
      "Artificial Analysis"
    ],
    "source_count": 1,
    "data_tier": 2,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.913700+00:00",
        "confidence": 0.79,
        "raw_name": "Gemini 2.5 Flash-Lite (Non-reasoning)",
        "raw_scores": {
          "intelligence_score": 16.009462487620045,
          "coding_score": 7.42,
          "context_window": 1000000,
          "gdpval": 357.78307098777884,
          "terminalbench_hard": 0.023,
          "tau2": 0.19,
          "lcr": 31.3,
          "hle": 3.7,
          "gpqa": 47.4,
          "scicode": 17.7,
          "ifbench": 31.5,
          "aime25": 35.3,
          "critpt": 0.0,
          "mmmu_pro": 54.0,
          "livecodebench": 40.0
        }
      },
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.913809+00:00",
        "confidence": 0.79,
        "raw_name": "Gemini 2.5 Flash-Lite (Reasoning)",
        "raw_scores": {
          "intelligence_score": 17.57,
          "coding_score": 9.47,
          "context_window": 1000000,
          "gdpval": 363.6555554225481,
          "terminalbench_hard": 0.045,
          "tau2": 0.184,
          "lcr": 51.3,
          "hle": 6.4,
          "gpqa": 62.5,
          "scicode": 19.3,
          "ifbench": 49.9,
          "aime25": 53.3,
          "critpt": 0.0,
          "mmmu_pro": 58.2,
          "livecodebench": 59.3
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 16.009462487620045,
        "coding_score": 7.42,
        "gdpval": 357.78307098777884,
        "terminalbench_hard": 0.023,
        "tau2": 0.19,
        "lcr": 31.3,
        "hle": 3.7,
        "gpqa": 47.4,
        "scicode": 17.7,
        "ifbench": 31.5,
        "aime25": 35.3,
        "critpt": 0.0,
        "mmmu_pro": 54.0,
        "livecodebench": 40.0
      }
    },
    "confidence_score": 0.79,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "gemini-20-flash-thinking-experimental",
    "canonical_name": "Gemini 2.0 Flash Thinking Experimental (Dec '24)",
    "model_name": "Gemini 2.0 Flash Thinking Experimental (Dec '24)",
    "aliases": [
      "Gemini 2.0 Flash Thinking Experimental (Dec '24)",
      "Gemini 2.0 Flash Thinking Experimental (Jan '25)"
    ],
    "provider": "Google",
    "context_window": 2000000,
    "open_source": null,
    "arena_votes": null,
    "license_type": null,
    "creator": null,
    "intelligence_score": 16.8448,
    "coding_score": 24.1093,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": null,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": null,
    "tau2": null,
    "lcr": null,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": 7.1,
    "gpqa": 70.1,
    "scicode": 32.9,
    "ifbench": null,
    "aime25": null,
    "critpt": null,
    "mmmu_pro": null,
    "livecodebench": 32.1,
    "sources": [
      "Artificial Analysis"
    ],
    "source_count": 1,
    "data_tier": 2,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.913767+00:00",
        "confidence": 0.44,
        "raw_name": "Gemini 2.0 Flash Thinking Experimental (Dec '24)",
        "raw_scores": {
          "intelligence_score": 12.331778143209375,
          "context_window": 2000000
        }
      },
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.913867+00:00",
        "confidence": 0.72,
        "raw_name": "Gemini 2.0 Flash Thinking Experimental (Jan '25)",
        "raw_scores": {
          "intelligence_score": 19.60282838932838,
          "coding_score": 24.1092929,
          "context_window": 1000000,
          "hle": 7.1,
          "gpqa": 70.1,
          "scicode": 32.9,
          "livecodebench": 32.1
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 12.331778143209375,
        "coding_score": 24.1092929,
        "hle": 7.1,
        "gpqa": 70.1,
        "scicode": 32.9,
        "livecodebench": 32.1
      }
    },
    "confidence_score": 0.58,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "gemini-25-pro-preview",
    "canonical_name": "Gemini 2.5 Pro Preview (May' 25)",
    "model_name": "Gemini 2.5 Pro Preview (May' 25)",
    "aliases": [
      "Gemini 2.5 Pro Preview (Mar' 25)",
      "Gemini 2.5 Pro Preview (May' 25)"
    ],
    "provider": "Google",
    "context_window": 1000000,
    "open_source": null,
    "arena_votes": null,
    "license_type": null,
    "creator": null,
    "intelligence_score": 29.9408,
    "coding_score": 46.7293,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": null,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": null,
    "tau2": null,
    "lcr": null,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": 16.2934,
    "gpqa": 82.9358,
    "scicode": 40.4964,
    "ifbench": null,
    "aime25": null,
    "critpt": null,
    "mmmu_pro": null,
    "livecodebench": 77.4204,
    "sources": [
      "Artificial Analysis"
    ],
    "source_count": 1,
    "data_tier": 2,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.913837+00:00",
        "confidence": 0.65,
        "raw_name": "Gemini 2.5 Pro Preview (May' 25)",
        "raw_scores": {
          "intelligence_score": 29.54758250414228,
          "context_window": 1000000,
          "hle": 15.4,
          "gpqa": 82.2,
          "scicode": 41.6,
          "livecodebench": 77.0
        }
      },
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.913958+00:00",
        "confidence": 0.72,
        "raw_name": "Gemini 2.5 Pro Preview (Mar' 25)",
        "raw_scores": {
          "intelligence_score": 30.295702949046902,
          "coding_score": 46.7292835,
          "context_window": 1000000,
          "hle": 17.1,
          "gpqa": 83.6,
          "scicode": 39.5,
          "livecodebench": 77.8
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 29.54758250414228,
        "hle": 15.4,
        "gpqa": 82.2,
        "scicode": 41.6,
        "livecodebench": 77.0,
        "coding_score": 46.7292835
      }
    },
    "confidence_score": 0.685,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "claude-35-sonnet",
    "canonical_name": "Claude 3.5 Sonnet (Oct '24)",
    "model_name": "Claude 3.5 Sonnet (Oct '24)",
    "aliases": [
      "Claude 3.5 Sonnet (June '24)",
      "Claude 3.5 Sonnet (Oct '24)",
      "claude-3.5-sonnet-20241022"
    ],
    "provider": "Anthropic",
    "context_window": 200000,
    "open_source": null,
    "arena_votes": 89293,
    "license_type": "Proprietary",
    "creator": null,
    "intelligence_score": 15.0935,
    "coding_score": 28.2088,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 1372.29,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": null,
    "tau2": null,
    "lcr": null,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": 3.8051,
    "gpqa": 58.0496,
    "scicode": 34.2277,
    "ifbench": null,
    "aime25": null,
    "critpt": null,
    "mmmu_pro": null,
    "livecodebench": 38.1,
    "sources": [
      "Artificial Analysis",
      "LMSYS Chatbot Arena"
    ],
    "source_count": 2,
    "data_tier": 1,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.914060+00:00",
        "confidence": 0.72,
        "raw_name": "Claude 3.5 Sonnet (Oct '24)",
        "raw_scores": {
          "intelligence_score": 15.926898851351913,
          "coding_score": 30.1638764,
          "context_window": 200000,
          "hle": 3.9,
          "gpqa": 59.9,
          "scicode": 36.6,
          "livecodebench": 38.1
        }
      },
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.914089+00:00",
        "confidence": 0.65,
        "raw_name": "Claude 3.5 Sonnet (June '24)",
        "raw_scores": {
          "intelligence_score": 14.170308711536471,
          "coding_score": 26.0431282,
          "context_window": 200000,
          "hle": 3.7,
          "gpqa": 56.0,
          "scicode": 31.6
        }
      },
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.892784+00:00",
        "confidence": 1.0,
        "raw_name": "claude-3.5-sonnet-20241022",
        "raw_scores": {
          "arena_elo": 1372.29,
          "arena_votes": 89293
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 15.926898851351913,
        "coding_score": 30.1638764,
        "hle": 3.9,
        "gpqa": 59.9,
        "scicode": 36.6,
        "livecodebench": 38.1
      },
      "LMSYS Chatbot Arena": {
        "arena_elo": 1372.29
      }
    },
    "confidence_score": 0.79,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "claude-3-opus",
    "canonical_name": "Claude 3 Opus",
    "model_name": "Claude 3 Opus",
    "aliases": [
      "Claude 3 Opus",
      "claude-3-opus-20240229"
    ],
    "provider": "Anthropic",
    "context_window": 200000,
    "open_source": null,
    "arena_votes": 194904,
    "license_type": "Proprietary",
    "creator": null,
    "intelligence_score": 12.4525,
    "coding_score": 19.5267,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 1321.93,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": 30.0,
    "latency_seconds": 0.0,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": 0.0,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": null,
    "tau2": null,
    "lcr": null,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": 3.1,
    "gpqa": 48.9,
    "scicode": 23.3,
    "ifbench": null,
    "aime25": null,
    "critpt": null,
    "mmmu_pro": null,
    "livecodebench": 27.9,
    "sources": [
      "Artificial Analysis",
      "LMSYS Chatbot Arena"
    ],
    "source_count": 2,
    "data_tier": 1,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.914121+00:00",
        "confidence": 0.9300000000000002,
        "raw_name": "Claude 3 Opus",
        "raw_scores": {
          "intelligence_score": 12.452455969031023,
          "coding_score": 19.5266898,
          "blended_cost_per_1m": 30.0,
          "latency_seconds": 0.0,
          "tokens_per_second": 0.0,
          "context_window": 200000,
          "hle": 3.1,
          "gpqa": 48.9,
          "scicode": 23.3,
          "livecodebench": 27.9
        }
      },
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.893572+00:00",
        "confidence": 1.0,
        "raw_name": "claude-3-opus-20240229",
        "raw_scores": {
          "arena_elo": 1321.93,
          "arena_votes": 194904
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 12.452455969031023,
        "coding_score": 19.5266898,
        "hle": 3.1,
        "gpqa": 48.9,
        "scicode": 23.3,
        "livecodebench": 27.9
      },
      "LMSYS Chatbot Arena": {
        "arena_elo": 1321.93
      }
    },
    "confidence_score": 0.965,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "claude-35-haiku",
    "canonical_name": "Claude 3.5 Haiku",
    "model_name": "Claude 3.5 Haiku",
    "aliases": [
      "Claude 3.5 Haiku",
      "claude-3.5-haiku-20241022"
    ],
    "provider": "Anthropic",
    "context_window": 200000,
    "open_source": null,
    "arena_votes": 70972,
    "license_type": "Proprietary",
    "creator": null,
    "intelligence_score": 12.3383,
    "coding_score": 10.66,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 1323.39,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": 1.6,
    "latency_seconds": 0.0,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": 0.0,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": 757.0845,
    "terminalbench_hard": 0.023,
    "tau2": 0.246,
    "lcr": 23.3,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": 3.5,
    "gpqa": 40.8,
    "scicode": 27.4,
    "ifbench": 42.8,
    "aime25": null,
    "critpt": 0.0,
    "mmmu_pro": 45.6,
    "livecodebench": 31.4,
    "sources": [
      "Artificial Analysis",
      "LMSYS Chatbot Arena"
    ],
    "source_count": 2,
    "data_tier": 1,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.914164+00:00",
        "confidence": 0.9300000000000002,
        "raw_name": "Claude 3.5 Haiku",
        "raw_scores": {
          "intelligence_score": 12.338270892083939,
          "coding_score": 10.66,
          "blended_cost_per_1m": 1.6,
          "latency_seconds": 0.0,
          "tokens_per_second": 0.0,
          "context_window": 200000,
          "gdpval": 757.0845341491752,
          "terminalbench_hard": 0.023,
          "tau2": 0.246,
          "lcr": 23.3,
          "hle": 3.5,
          "gpqa": 40.8,
          "scicode": 27.4,
          "ifbench": 42.8,
          "critpt": 0.0,
          "mmmu_pro": 45.6,
          "livecodebench": 31.4
        }
      },
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.893508+00:00",
        "confidence": 1.0,
        "raw_name": "claude-3.5-haiku-20241022",
        "raw_scores": {
          "arena_elo": 1323.39,
          "arena_votes": 70972
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 12.338270892083939,
        "coding_score": 10.66,
        "gdpval": 757.0845341491752,
        "terminalbench_hard": 0.023,
        "tau2": 0.246,
        "lcr": 23.3,
        "hle": 3.5,
        "gpqa": 40.8,
        "scicode": 27.4,
        "ifbench": 42.8,
        "critpt": 0.0,
        "mmmu_pro": 45.6,
        "livecodebench": 31.4
      },
      "LMSYS Chatbot Arena": {
        "arena_elo": 1323.39
      }
    },
    "confidence_score": 0.965,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "claude-3-sonnet",
    "canonical_name": "Claude 3 Sonnet",
    "model_name": "Claude 3 Sonnet",
    "aliases": [
      "Claude 3 Sonnet"
    ],
    "provider": "Anthropic",
    "context_window": 200000,
    "open_source": null,
    "arena_votes": null,
    "license_type": null,
    "creator": null,
    "intelligence_score": 10.2703,
    "coding_score": null,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": null,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": 6.0,
    "latency_seconds": 0.0,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": 0.0,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": null,
    "tau2": null,
    "lcr": null,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": 3.8,
    "gpqa": 40.0,
    "scicode": 22.9,
    "ifbench": null,
    "aime25": null,
    "critpt": null,
    "mmmu_pro": null,
    "livecodebench": 17.5,
    "sources": [
      "Artificial Analysis"
    ],
    "source_count": 1,
    "data_tier": 2,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.914210+00:00",
        "confidence": 0.8600000000000001,
        "raw_name": "Claude 3 Sonnet",
        "raw_scores": {
          "intelligence_score": 10.270295671896926,
          "blended_cost_per_1m": 6.0,
          "latency_seconds": 0.0,
          "tokens_per_second": 0.0,
          "context_window": 200000,
          "hle": 3.8,
          "gpqa": 40.0,
          "scicode": 22.9,
          "livecodebench": 17.5
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 10.270295671896926,
        "hle": 3.8,
        "gpqa": 40.0,
        "scicode": 22.9,
        "livecodebench": 17.5
      }
    },
    "confidence_score": 0.86,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "claude-3-haiku",
    "canonical_name": "Claude 3 Haiku",
    "model_name": "Claude 3 Haiku",
    "aliases": [
      "Claude 3 Haiku"
    ],
    "provider": "Anthropic",
    "context_window": 200000,
    "open_source": null,
    "arena_votes": null,
    "license_type": null,
    "creator": null,
    "intelligence_score": 9.3001,
    "coding_score": 6.72,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": null,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": 0.5,
    "latency_seconds": 0.51,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": 117.0,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": 424.3785,
    "terminalbench_hard": 0.008,
    "tau2": 0.211,
    "lcr": 21.0,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": 3.9,
    "gpqa": 37.4,
    "scicode": 18.6,
    "ifbench": 36.1,
    "aime25": null,
    "critpt": 0.0,
    "mmmu_pro": 30.8,
    "livecodebench": 15.4,
    "sources": [
      "Artificial Analysis"
    ],
    "source_count": 1,
    "data_tier": 2,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.914255+00:00",
        "confidence": 0.9300000000000002,
        "raw_name": "Claude 3 Haiku",
        "raw_scores": {
          "intelligence_score": 9.300141268649064,
          "coding_score": 6.72,
          "blended_cost_per_1m": 0.5,
          "latency_seconds": 0.51,
          "tokens_per_second": 117.0,
          "context_window": 200000,
          "gdpval": 424.3784954691123,
          "terminalbench_hard": 0.008,
          "tau2": 0.211,
          "lcr": 21.0,
          "hle": 3.9,
          "gpqa": 37.4,
          "scicode": 18.6,
          "ifbench": 36.1,
          "critpt": 0.0,
          "mmmu_pro": 30.8,
          "livecodebench": 15.4
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 9.300141268649064,
        "coding_score": 6.72,
        "gdpval": 424.3784954691123,
        "terminalbench_hard": 0.008,
        "tau2": 0.211,
        "lcr": 21.0,
        "hle": 3.9,
        "gpqa": 37.4,
        "scicode": 18.6,
        "ifbench": 36.1,
        "critpt": 0.0,
        "mmmu_pro": 30.8,
        "livecodebench": 15.4
      }
    },
    "confidence_score": 0.93,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "claude-instant",
    "canonical_name": "Claude Instant",
    "model_name": "Claude Instant",
    "aliases": [
      "Claude Instant"
    ],
    "provider": "Anthropic",
    "context_window": 100000,
    "open_source": null,
    "arena_votes": null,
    "license_type": null,
    "creator": null,
    "intelligence_score": 7.4139,
    "coding_score": 7.7554,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": null,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": 0.0,
    "latency_seconds": 0.0,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": 0.0,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": null,
    "tau2": null,
    "lcr": null,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": 3.8,
    "gpqa": 33.0,
    "scicode": null,
    "ifbench": null,
    "aime25": null,
    "critpt": null,
    "mmmu_pro": null,
    "livecodebench": 10.9,
    "sources": [
      "Artificial Analysis"
    ],
    "source_count": 1,
    "data_tier": 2,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.914286+00:00",
        "confidence": 0.9300000000000002,
        "raw_name": "Claude Instant",
        "raw_scores": {
          "intelligence_score": 7.413887665229345,
          "coding_score": 7.7553606,
          "blended_cost_per_1m": 0.0,
          "latency_seconds": 0.0,
          "tokens_per_second": 0.0,
          "context_window": 100000,
          "hle": 3.8,
          "gpqa": 33.0,
          "livecodebench": 10.9
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 7.413887665229345,
        "coding_score": 7.7553606,
        "hle": 3.8,
        "gpqa": 33.0,
        "livecodebench": 10.9
      }
    },
    "confidence_score": 0.93,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "claude-45-sonnet",
    "canonical_name": "Claude 4.5 Sonnet (Reasoning)",
    "model_name": "Claude 4.5 Sonnet (Reasoning)",
    "aliases": [
      "Claude 4.5 Sonnet (Non-reasoning)",
      "Claude 4.5 Sonnet (Reasoning)"
    ],
    "provider": "Anthropic",
    "context_window": 1000000,
    "open_source": null,
    "arena_votes": null,
    "license_type": null,
    "creator": null,
    "intelligence_score": 40.085,
    "coding_score": 36.05,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": null,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": 1297.4521,
    "terminalbench_hard": 0.322,
    "tau2": 0.743,
    "lcr": 58.5,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": 12.2,
    "gpqa": 78.05,
    "scicode": 43.75,
    "ifbench": 50.0,
    "aime25": 62.5,
    "critpt": 0.55,
    "mmmu_pro": 66.95,
    "livecodebench": 65.2,
    "sources": [
      "Artificial Analysis"
    ],
    "source_count": 1,
    "data_tier": 2,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.914330+00:00",
        "confidence": 0.79,
        "raw_name": "Claude 4.5 Sonnet (Reasoning)",
        "raw_scores": {
          "intelligence_score": 43.03,
          "coding_score": 38.63,
          "context_window": 1000000,
          "gdpval": 1275.6086138003407,
          "terminalbench_hard": 0.356,
          "tau2": 0.781,
          "lcr": 65.7,
          "hle": 17.3,
          "gpqa": 83.4,
          "scicode": 44.7,
          "ifbench": 57.3,
          "aime25": 88.0,
          "critpt": 1.1,
          "mmmu_pro": 68.7,
          "livecodebench": 71.4
        }
      },
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.914606+00:00",
        "confidence": 0.79,
        "raw_name": "Claude 4.5 Sonnet (Non-reasoning)",
        "raw_scores": {
          "intelligence_score": 37.14,
          "coding_score": 33.47,
          "context_window": 1000000,
          "gdpval": 1319.2956855638495,
          "terminalbench_hard": 0.288,
          "tau2": 0.705,
          "lcr": 51.3,
          "hle": 7.1,
          "gpqa": 72.7,
          "scicode": 42.8,
          "ifbench": 42.7,
          "aime25": 37.0,
          "critpt": 0.0,
          "mmmu_pro": 65.2,
          "livecodebench": 59.0
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 43.03,
        "coding_score": 38.63,
        "gdpval": 1275.6086138003407,
        "terminalbench_hard": 0.356,
        "tau2": 0.781,
        "lcr": 65.7,
        "hle": 17.3,
        "gpqa": 83.4,
        "scicode": 44.7,
        "ifbench": 57.3,
        "aime25": 88.0,
        "critpt": 1.1,
        "mmmu_pro": 68.7,
        "livecodebench": 71.4
      }
    },
    "confidence_score": 0.79,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "claude-opus-41",
    "canonical_name": "Claude 4.1 Opus (Non-reasoning)",
    "model_name": "Claude 4.1 Opus (Non-reasoning)",
    "aliases": [
      "Claude 4.1 Opus (Non-reasoning)",
      "Claude 4.1 Opus (Reasoning)",
      "claude-opus-4.1-20250805",
      "claude-opus-4.1-20250805-thinking-16k"
    ],
    "provider": "Anthropic",
    "context_window": 200000,
    "open_source": null,
    "arena_votes": 77218,
    "license_type": "Proprietary",
    "creator": null,
    "intelligence_score": 28.9111,
    "coding_score": 36.52,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 1447.44,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": 0.343,
    "tau2": 0.714,
    "lcr": 66.3,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": 11.9,
    "gpqa": 80.9,
    "scicode": 40.9,
    "ifbench": 55.4,
    "aime25": 80.3,
    "critpt": 0.0,
    "mmmu_pro": 67.9,
    "livecodebench": 65.4,
    "sources": [
      "Artificial Analysis",
      "LMSYS Chatbot Arena"
    ],
    "source_count": 2,
    "data_tier": 1,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.914471+00:00",
        "confidence": 0.44,
        "raw_name": "Claude 4.1 Opus (Non-reasoning)",
        "raw_scores": {
          "intelligence_score": 23.565610165346172,
          "context_window": 200000
        }
      },
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.914522+00:00",
        "confidence": 0.79,
        "raw_name": "Claude 4.1 Opus (Reasoning)",
        "raw_scores": {
          "intelligence_score": 31.888270712229925,
          "coding_score": 36.52,
          "context_window": 200000,
          "terminalbench_hard": 0.343,
          "tau2": 0.714,
          "lcr": 66.3,
          "hle": 11.9,
          "gpqa": 80.9,
          "scicode": 40.9,
          "ifbench": 55.4,
          "aime25": 80.3,
          "critpt": 0.0,
          "mmmu_pro": 67.9,
          "livecodebench": 65.4
        }
      },
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.891607+00:00",
        "confidence": 1.0,
        "raw_name": "claude-opus-4.1-20250805-thinking-16k",
        "raw_scores": {
          "arena_elo": 1448.65,
          "arena_votes": 49597
        }
      },
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.891622+00:00",
        "confidence": 1.0,
        "raw_name": "claude-opus-4.1-20250805",
        "raw_scores": {
          "arena_elo": 1446.23,
          "arena_votes": 77218
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 23.565610165346172,
        "coding_score": 36.52,
        "terminalbench_hard": 0.343,
        "tau2": 0.714,
        "lcr": 66.3,
        "hle": 11.9,
        "gpqa": 80.9,
        "scicode": 40.9,
        "ifbench": 55.4,
        "aime25": 80.3,
        "critpt": 0.0,
        "mmmu_pro": 67.9,
        "livecodebench": 65.4
      },
      "LMSYS Chatbot Arena": {
        "arena_elo": 1448.65
      }
    },
    "confidence_score": 0.8075,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "claude-37-sonnet",
    "canonical_name": "Claude 3.7 Sonnet (Reasoning)",
    "model_name": "Claude 3.7 Sonnet (Reasoning)",
    "aliases": [
      "Claude 3.7 Sonnet (Non-reasoning)",
      "Claude 3.7 Sonnet (Reasoning)",
      "claude-3.7-sonnet-20250219"
    ],
    "provider": "Anthropic",
    "context_window": 200000,
    "open_source": null,
    "arena_votes": 44275,
    "license_type": "Proprietary",
    "creator": null,
    "intelligence_score": 32.76,
    "coding_score": 27.125,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 1371.24,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": 1066.9443,
    "terminalbench_hard": 0.212,
    "tau2": 0.5235,
    "lcr": 54.5,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": 7.55,
    "gpqa": 71.4,
    "scicode": 38.95,
    "ifbench": 46.15,
    "aime25": 38.65,
    "critpt": 0.45,
    "mmmu_pro": 60.1,
    "livecodebench": 43.35,
    "sources": [
      "Artificial Analysis",
      "LMSYS Chatbot Arena"
    ],
    "source_count": 2,
    "data_tier": 1,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.914563+00:00",
        "confidence": 0.79,
        "raw_name": "Claude 3.7 Sonnet (Reasoning)",
        "raw_scores": {
          "intelligence_score": 34.71,
          "coding_score": 27.57,
          "context_window": 200000,
          "gdpval": 1067.4928088867234,
          "terminalbench_hard": 0.212,
          "tau2": 0.547,
          "lcr": 60.7,
          "hle": 10.3,
          "gpqa": 77.2,
          "scicode": 40.3,
          "ifbench": 48.3,
          "aime25": 56.3,
          "critpt": 0.9,
          "livecodebench": 47.3
        }
      },
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.914882+00:00",
        "confidence": 0.79,
        "raw_name": "Claude 3.7 Sonnet (Non-reasoning)",
        "raw_scores": {
          "intelligence_score": 30.81,
          "coding_score": 26.68,
          "context_window": 200000,
          "gdpval": 1066.395866415472,
          "terminalbench_hard": 0.212,
          "tau2": 0.5,
          "lcr": 48.3,
          "hle": 4.8,
          "gpqa": 65.6,
          "scicode": 37.6,
          "ifbench": 44.0,
          "aime25": 21.0,
          "critpt": 0.0,
          "mmmu_pro": 60.1,
          "livecodebench": 39.4
        }
      },
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.892814+00:00",
        "confidence": 1.0,
        "raw_name": "claude-3.7-sonnet-20250219",
        "raw_scores": {
          "arena_elo": 1371.24,
          "arena_votes": 44275
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 34.71,
        "coding_score": 27.57,
        "gdpval": 1067.4928088867234,
        "terminalbench_hard": 0.212,
        "tau2": 0.547,
        "lcr": 60.7,
        "hle": 10.3,
        "gpqa": 77.2,
        "scicode": 40.3,
        "ifbench": 48.3,
        "aime25": 56.3,
        "critpt": 0.9,
        "livecodebench": 47.3,
        "mmmu_pro": 60.1
      },
      "LMSYS Chatbot Arena": {
        "arena_elo": 1371.24
      }
    },
    "confidence_score": 0.86,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "claude-opus-45",
    "canonical_name": "Claude Opus 4.5 (Reasoning)",
    "model_name": "Claude Opus 4.5 (Reasoning)",
    "aliases": [
      "Claude Opus 4.5 (Non-reasoning)",
      "Claude Opus 4.5 (Reasoning)",
      "claude-opus-4.5-20251101",
      "claude-opus-4.5-20251101-thinking-32k"
    ],
    "provider": "Anthropic",
    "context_window": 200000,
    "open_source": null,
    "arena_votes": 35476,
    "license_type": "Proprietary",
    "creator": null,
    "intelligence_score": 46.41,
    "coding_score": 45.385,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 1469.045,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": 1408.0567,
    "terminalbench_hard": 0.4395,
    "tau2": 0.879,
    "lcr": 69.65,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": 20.65,
    "gpqa": 83.8,
    "scicode": 48.25,
    "ifbench": 50.5,
    "aime25": 77.0,
    "critpt": 2.45,
    "mmmu_pro": 72.6,
    "livecodebench": 80.45,
    "sources": [
      "Artificial Analysis",
      "LMSYS Chatbot Arena"
    ],
    "source_count": 2,
    "data_tier": 1,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.914647+00:00",
        "confidence": 0.79,
        "raw_name": "Claude Opus 4.5 (Reasoning)",
        "raw_scores": {
          "intelligence_score": 49.73,
          "coding_score": 47.83,
          "context_window": 200000,
          "gdpval": 1400.015736858286,
          "terminalbench_hard": 0.47,
          "tau2": 0.895,
          "lcr": 74.0,
          "hle": 28.4,
          "gpqa": 86.6,
          "scicode": 49.5,
          "ifbench": 58.0,
          "aime25": 91.3,
          "critpt": 4.6,
          "mmmu_pro": 74.0,
          "livecodebench": 87.1
        }
      },
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.914689+00:00",
        "confidence": 0.79,
        "raw_name": "Claude Opus 4.5 (Non-reasoning)",
        "raw_scores": {
          "intelligence_score": 43.09,
          "coding_score": 42.94,
          "context_window": 200000,
          "gdpval": 1416.0977341083603,
          "terminalbench_hard": 0.409,
          "tau2": 0.863,
          "lcr": 65.3,
          "hle": 12.9,
          "gpqa": 81.0,
          "scicode": 47.0,
          "ifbench": 43.0,
          "aime25": 62.7,
          "critpt": 0.3,
          "mmmu_pro": 71.2,
          "livecodebench": 73.8
        }
      },
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.891394+00:00",
        "confidence": 1.0,
        "raw_name": "claude-opus-4.5-20251101-thinking-32k",
        "raw_scores": {
          "arena_elo": 1470.96,
          "arena_votes": 30541
        }
      },
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.891425+00:00",
        "confidence": 1.0,
        "raw_name": "claude-opus-4.5-20251101",
        "raw_scores": {
          "arena_elo": 1467.13,
          "arena_votes": 35476
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 49.73,
        "coding_score": 47.83,
        "gdpval": 1400.015736858286,
        "terminalbench_hard": 0.47,
        "tau2": 0.895,
        "lcr": 74.0,
        "hle": 28.4,
        "gpqa": 86.6,
        "scicode": 49.5,
        "ifbench": 58.0,
        "aime25": 91.3,
        "critpt": 4.6,
        "mmmu_pro": 74.0,
        "livecodebench": 87.1
      },
      "LMSYS Chatbot Arena": {
        "arena_elo": 1470.96
      }
    },
    "confidence_score": 0.895,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "claude-opus-4",
    "canonical_name": "Claude 4 Opus (Non-reasoning)",
    "model_name": "Claude 4 Opus (Non-reasoning)",
    "aliases": [
      "Claude 4 Opus (Non-reasoning)",
      "Claude 4 Opus (Reasoning)",
      "claude-opus-4-20250514",
      "claude-opus-4-20250514-thinking-16k"
    ],
    "provider": "Anthropic",
    "context_window": 200000,
    "open_source": null,
    "arena_votes": 45304,
    "license_type": "Proprietary",
    "creator": null,
    "intelligence_score": 24.8868,
    "coding_score": 33.98,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 1418.335,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": 0.311,
    "tau2": 0.705,
    "lcr": 34.7967,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": 8.9344,
    "gpqa": 75.0702,
    "scicode": 40.3245,
    "ifbench": 48.7411,
    "aime25": 55.6576,
    "critpt": null,
    "mmmu_pro": null,
    "livecodebench": 59.1179,
    "sources": [
      "Artificial Analysis",
      "LMSYS Chatbot Arena"
    ],
    "source_count": 2,
    "data_tier": 1,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.914723+00:00",
        "confidence": 0.72,
        "raw_name": "Claude 4 Opus (Non-reasoning)",
        "raw_scores": {
          "intelligence_score": 22.171205776538883,
          "context_window": 200000,
          "lcr": 36.0,
          "hle": 5.9,
          "gpqa": 70.1,
          "scicode": 40.9,
          "ifbench": 43.3,
          "aime25": 36.3,
          "livecodebench": 54.2
        }
      },
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.914759+00:00",
        "confidence": 0.79,
        "raw_name": "Claude 4 Opus (Reasoning)",
        "raw_scores": {
          "intelligence_score": 27.361794173399694,
          "coding_score": 33.98,
          "context_window": 200000,
          "terminalbench_hard": 0.311,
          "tau2": 0.705,
          "lcr": 33.7,
          "hle": 11.7,
          "gpqa": 79.6,
          "scicode": 39.8,
          "ifbench": 53.7,
          "aime25": 73.3,
          "livecodebench": 63.6
        }
      },
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.891846+00:00",
        "confidence": 1.0,
        "raw_name": "claude-opus-4-20250514-thinking-16k",
        "raw_scores": {
          "arena_elo": 1423.71,
          "arena_votes": 37722
        }
      },
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.892087+00:00",
        "confidence": 1.0,
        "raw_name": "claude-opus-4-20250514",
        "raw_scores": {
          "arena_elo": 1412.96,
          "arena_votes": 45304
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 22.171205776538883,
        "lcr": 36.0,
        "hle": 5.9,
        "gpqa": 70.1,
        "scicode": 40.9,
        "ifbench": 43.3,
        "aime25": 36.3,
        "livecodebench": 54.2,
        "coding_score": 33.98,
        "terminalbench_hard": 0.311,
        "tau2": 0.705
      },
      "LMSYS Chatbot Arena": {
        "arena_elo": 1423.71
      }
    },
    "confidence_score": 0.8775,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "claude-21",
    "canonical_name": "Claude 2.1",
    "model_name": "Claude 2.1",
    "aliases": [
      "Claude 2.1"
    ],
    "provider": "Anthropic",
    "context_window": 200000,
    "open_source": null,
    "arena_votes": null,
    "license_type": null,
    "creator": null,
    "intelligence_score": 9.3247,
    "coding_score": 14.0194,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": null,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": 0.0,
    "latency_seconds": 0.0,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": 0.0,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": null,
    "tau2": null,
    "lcr": null,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": 4.2,
    "gpqa": 31.9,
    "scicode": 18.4,
    "ifbench": null,
    "aime25": null,
    "critpt": null,
    "mmmu_pro": null,
    "livecodebench": 19.5,
    "sources": [
      "Artificial Analysis"
    ],
    "source_count": 1,
    "data_tier": 2,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.914794+00:00",
        "confidence": 0.9300000000000002,
        "raw_name": "Claude 2.1",
        "raw_scores": {
          "intelligence_score": 9.324651438021968,
          "coding_score": 14.019371,
          "blended_cost_per_1m": 0.0,
          "latency_seconds": 0.0,
          "tokens_per_second": 0.0,
          "context_window": 200000,
          "hle": 4.2,
          "gpqa": 31.9,
          "scicode": 18.4,
          "livecodebench": 19.5
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 9.324651438021968,
        "coding_score": 14.019371,
        "hle": 4.2,
        "gpqa": 31.9,
        "scicode": 18.4,
        "livecodebench": 19.5
      }
    },
    "confidence_score": 0.93,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "claude-4-sonnet",
    "canonical_name": "Claude 4 Sonnet (Reasoning)",
    "model_name": "Claude 4 Sonnet (Reasoning)",
    "aliases": [
      "Claude 4 Sonnet (Non-reasoning)",
      "Claude 4 Sonnet (Reasoning)"
    ],
    "provider": "Anthropic",
    "context_window": 1000000,
    "open_source": null,
    "arena_votes": null,
    "license_type": null,
    "creator": null,
    "intelligence_score": 35.83,
    "coding_score": 32.33,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": null,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": 1159.3387,
    "terminalbench_hard": 0.292,
    "tau2": 0.5845,
    "lcr": 54.5,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": 6.8,
    "gpqa": 73.0,
    "scicode": 38.65,
    "ifbench": 50.05,
    "aime25": 56.15,
    "critpt": 0.7,
    "mmmu_pro": 62.1,
    "livecodebench": 55.2,
    "sources": [
      "Artificial Analysis"
    ],
    "source_count": 1,
    "data_tier": 2,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.914837+00:00",
        "confidence": 0.79,
        "raw_name": "Claude 4 Sonnet (Reasoning)",
        "raw_scores": {
          "intelligence_score": 38.66,
          "coding_score": 34.06,
          "context_window": 1000000,
          "gdpval": 1155.1115790770912,
          "terminalbench_hard": 0.311,
          "tau2": 0.646,
          "lcr": 64.7,
          "hle": 9.6,
          "gpqa": 77.7,
          "scicode": 40.0,
          "ifbench": 54.7,
          "aime25": 74.3,
          "critpt": 0.3,
          "mmmu_pro": 61.8,
          "livecodebench": 65.5
        }
      },
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.914924+00:00",
        "confidence": 0.79,
        "raw_name": "Claude 4 Sonnet (Non-reasoning)",
        "raw_scores": {
          "intelligence_score": 33.0,
          "coding_score": 30.6,
          "context_window": 1000000,
          "gdpval": 1163.5658699476603,
          "terminalbench_hard": 0.273,
          "tau2": 0.523,
          "lcr": 44.3,
          "hle": 4.0,
          "gpqa": 68.3,
          "scicode": 37.3,
          "ifbench": 45.4,
          "aime25": 38.0,
          "critpt": 1.1,
          "mmmu_pro": 62.4,
          "livecodebench": 44.9
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 38.66,
        "coding_score": 34.06,
        "gdpval": 1155.1115790770912,
        "terminalbench_hard": 0.311,
        "tau2": 0.646,
        "lcr": 64.7,
        "hle": 9.6,
        "gpqa": 77.7,
        "scicode": 40.0,
        "ifbench": 54.7,
        "aime25": 74.3,
        "critpt": 0.3,
        "mmmu_pro": 61.8,
        "livecodebench": 65.5
      }
    },
    "confidence_score": 0.79,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "claude-20",
    "canonical_name": "Claude 2.0",
    "model_name": "Claude 2.0",
    "aliases": [
      "Claude 2.0"
    ],
    "provider": "Anthropic",
    "context_window": 100000,
    "open_source": null,
    "arena_votes": null,
    "license_type": null,
    "creator": null,
    "intelligence_score": 9.0586,
    "coding_score": 12.8597,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": null,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": 0.0,
    "latency_seconds": 0.0,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": 0.0,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": null,
    "tau2": null,
    "lcr": null,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": null,
    "gpqa": 34.4,
    "scicode": 19.4,
    "ifbench": null,
    "aime25": null,
    "critpt": null,
    "mmmu_pro": null,
    "livecodebench": 17.1,
    "sources": [
      "Artificial Analysis"
    ],
    "source_count": 1,
    "data_tier": 2,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.914956+00:00",
        "confidence": 0.8600000000000001,
        "raw_name": "Claude 2.0",
        "raw_scores": {
          "intelligence_score": 9.058555199878974,
          "coding_score": 12.8596855,
          "blended_cost_per_1m": 0.0,
          "latency_seconds": 0.0,
          "tokens_per_second": 0.0,
          "context_window": 100000,
          "gpqa": 34.4,
          "scicode": 19.4,
          "livecodebench": 17.1
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 9.058555199878974,
        "coding_score": 12.8596855,
        "gpqa": 34.4,
        "scicode": 19.4,
        "livecodebench": 17.1
      }
    },
    "confidence_score": 0.86,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "mistral-large-2",
    "canonical_name": "Mistral Large 2 (Nov '24)",
    "model_name": "Mistral Large 2 (Nov '24)",
    "aliases": [
      "Mistral Large 2 (Jul '24)",
      "Mistral Large 2 (Nov '24)"
    ],
    "provider": "Mistral AI",
    "context_window": 128000,
    "open_source": false,
    "arena_votes": null,
    "license_type": "Mistral Research License",
    "creator": null,
    "intelligence_score": 13.893,
    "coding_score": 13.76,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": null,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": 376.2323,
    "terminalbench_hard": 0.061,
    "tau2": 0.318,
    "lcr": 3.5834,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": 3.6185,
    "gpqa": 47.9325,
    "scicode": 28.1987,
    "ifbench": 31.3907,
    "aime25": 7.3245,
    "critpt": 0.0,
    "mmmu_pro": null,
    "livecodebench": 28.0603,
    "sources": [
      "Artificial Analysis"
    ],
    "source_count": 1,
    "data_tier": 2,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.914996+00:00",
        "confidence": 0.79,
        "raw_name": "Mistral Large 2 (Nov '24)",
        "raw_scores": {
          "intelligence_score": 14.676681663368385,
          "coding_score": 13.76,
          "context_window": 128000,
          "gdpval": 376.2323329743872,
          "terminalbench_hard": 0.061,
          "tau2": 0.307,
          "lcr": 5.3,
          "hle": 4.0,
          "gpqa": 48.6,
          "scicode": 29.2,
          "ifbench": 31.2,
          "aime25": 14.0,
          "critpt": 0.0,
          "livecodebench": 29.3
        }
      },
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.915031+00:00",
        "confidence": 0.72,
        "raw_name": "Mistral Large 2 (Jul '24)",
        "raw_scores": {
          "intelligence_score": 13.033082438627458,
          "context_window": 128000,
          "tau2": 0.33,
          "lcr": 1.7,
          "hle": 3.2,
          "gpqa": 47.2,
          "scicode": 27.1,
          "ifbench": 31.6,
          "aime25": 0.0,
          "livecodebench": 26.7
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 14.676681663368385,
        "coding_score": 13.76,
        "gdpval": 376.2323329743872,
        "terminalbench_hard": 0.061,
        "tau2": 0.307,
        "lcr": 5.3,
        "hle": 4.0,
        "gpqa": 48.6,
        "scicode": 29.2,
        "ifbench": 31.2,
        "aime25": 14.0,
        "critpt": 0.0,
        "livecodebench": 29.3
      }
    },
    "confidence_score": 0.755,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "pixtral-large",
    "canonical_name": "Pixtral Large",
    "model_name": "Pixtral Large",
    "aliases": [
      "Pixtral Large"
    ],
    "provider": null,
    "context_window": 128000,
    "open_source": false,
    "arena_votes": null,
    "license_type": "Mistral Research License",
    "creator": null,
    "intelligence_score": 14.0002,
    "coding_score": null,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": null,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": 3.0,
    "latency_seconds": 0.43,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": 50.0,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": null,
    "tau2": 0.365,
    "lcr": 10.3,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": 3.6,
    "gpqa": 50.5,
    "scicode": 29.2,
    "ifbench": 34.5,
    "aime25": 2.3,
    "critpt": null,
    "mmmu_pro": 50.6,
    "livecodebench": 26.1,
    "sources": [
      "Artificial Analysis"
    ],
    "source_count": 1,
    "data_tier": 2,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.915071+00:00",
        "confidence": 0.9300000000000002,
        "raw_name": "Pixtral Large",
        "raw_scores": {
          "intelligence_score": 14.000187381306647,
          "blended_cost_per_1m": 3.0,
          "latency_seconds": 0.43,
          "tokens_per_second": 50.0,
          "context_window": 128000,
          "tau2": 0.365,
          "lcr": 10.3,
          "hle": 3.6,
          "gpqa": 50.5,
          "scicode": 29.2,
          "ifbench": 34.5,
          "aime25": 2.3,
          "mmmu_pro": 50.6,
          "livecodebench": 26.1
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 14.000187381306647,
        "tau2": 0.365,
        "lcr": 10.3,
        "hle": 3.6,
        "gpqa": 50.5,
        "scicode": 29.2,
        "ifbench": 34.5,
        "aime25": 2.3,
        "mmmu_pro": 50.6,
        "livecodebench": 26.1
      }
    },
    "confidence_score": 0.93,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "mistral-small-3",
    "canonical_name": "Mistral Small 3",
    "model_name": "Mistral Small 3",
    "aliases": [
      "Mistral Small 3"
    ],
    "provider": "Mistral AI",
    "context_window": 32000,
    "open_source": true,
    "arena_votes": null,
    "license_type": "Apache 2.0",
    "creator": null,
    "intelligence_score": 12.6671,
    "coding_score": null,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": null,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": 0.15,
    "latency_seconds": 0.35,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": 237.0,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": null,
    "tau2": 0.196,
    "lcr": 0.0,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": 4.1,
    "gpqa": 46.2,
    "scicode": 23.6,
    "ifbench": 26.4,
    "aime25": 4.3,
    "critpt": null,
    "mmmu_pro": null,
    "livecodebench": 25.2,
    "sources": [
      "Artificial Analysis"
    ],
    "source_count": 1,
    "data_tier": 2,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.915109+00:00",
        "confidence": 0.9300000000000002,
        "raw_name": "Mistral Small 3",
        "raw_scores": {
          "intelligence_score": 12.667127328098278,
          "blended_cost_per_1m": 0.15,
          "latency_seconds": 0.35,
          "tokens_per_second": 237.0,
          "context_window": 32000,
          "tau2": 0.196,
          "lcr": 0.0,
          "hle": 4.1,
          "gpqa": 46.2,
          "scicode": 23.6,
          "ifbench": 26.4,
          "aime25": 4.3,
          "livecodebench": 25.2
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 12.667127328098278,
        "tau2": 0.196,
        "lcr": 0.0,
        "hle": 4.1,
        "gpqa": 46.2,
        "scicode": 23.6,
        "ifbench": 26.4,
        "aime25": 4.3,
        "livecodebench": 25.2
      }
    },
    "confidence_score": 0.93,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "mistral-small",
    "canonical_name": "Mistral Small (Sep '24)",
    "model_name": "Mistral Small (Sep '24)",
    "aliases": [
      "Mistral Small (Feb '24)",
      "Mistral Small (Sep '24)"
    ],
    "provider": "Mistral AI",
    "context_window": 32768,
    "open_source": false,
    "arena_votes": null,
    "license_type": "Mistral Research License",
    "creator": null,
    "intelligence_score": 9.6092,
    "coding_score": null,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": null,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": null,
    "tau2": null,
    "lcr": null,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": 4.35,
    "gpqa": 34.15,
    "scicode": 14.5,
    "ifbench": null,
    "aime25": null,
    "critpt": null,
    "mmmu_pro": null,
    "livecodebench": 12.6,
    "sources": [
      "Artificial Analysis"
    ],
    "source_count": 1,
    "data_tier": 2,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.915142+00:00",
        "confidence": 0.65,
        "raw_name": "Mistral Small (Sep '24)",
        "raw_scores": {
          "intelligence_score": 10.178799016274557,
          "context_window": 32768,
          "hle": 4.3,
          "gpqa": 38.1,
          "scicode": 15.6,
          "livecodebench": 14.1
        }
      },
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.915214+00:00",
        "confidence": 0.65,
        "raw_name": "Mistral Small (Feb '24)",
        "raw_scores": {
          "intelligence_score": 9.039501606668157,
          "context_window": 32768,
          "hle": 4.4,
          "gpqa": 30.2,
          "scicode": 13.4,
          "livecodebench": 11.1
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 10.178799016274557,
        "hle": 4.3,
        "gpqa": 38.1,
        "scicode": 15.6,
        "livecodebench": 14.1
      }
    },
    "confidence_score": 0.65,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "mixtral-8x22b-instruct",
    "canonical_name": "Mixtral 8x22B Instruct",
    "model_name": "Mixtral 8x22B Instruct",
    "aliases": [
      "Mixtral 8x22B Instruct"
    ],
    "provider": "Mistral AI",
    "context_window": 65384,
    "open_source": true,
    "arena_votes": null,
    "license_type": "Apache 2.0",
    "creator": null,
    "intelligence_score": 9.8442,
    "coding_score": null,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": null,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": null,
    "tau2": null,
    "lcr": null,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": 4.1,
    "gpqa": 33.2,
    "scicode": 18.8,
    "ifbench": null,
    "aime25": null,
    "critpt": null,
    "mmmu_pro": null,
    "livecodebench": 14.8,
    "sources": [
      "Artificial Analysis"
    ],
    "source_count": 1,
    "data_tier": 2,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.915171+00:00",
        "confidence": 0.65,
        "raw_name": "Mixtral 8x22B Instruct",
        "raw_scores": {
          "intelligence_score": 9.844182670676119,
          "context_window": 65384,
          "hle": 4.1,
          "gpqa": 33.2,
          "scicode": 18.8,
          "livecodebench": 14.8
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 9.844182670676119,
        "hle": 4.1,
        "gpqa": 33.2,
        "scicode": 18.8,
        "livecodebench": 14.8
      }
    },
    "confidence_score": 0.65,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "mistral-large",
    "canonical_name": "Mistral Large (Feb '24)",
    "model_name": "Mistral Large (Feb '24)",
    "aliases": [
      "Mistral Large (Feb '24)"
    ],
    "provider": "Mistral AI",
    "context_window": 32768,
    "open_source": null,
    "arena_votes": null,
    "license_type": null,
    "creator": null,
    "intelligence_score": 9.9092,
    "coding_score": null,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": null,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": null,
    "tau2": null,
    "lcr": null,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": 3.4,
    "gpqa": 35.1,
    "scicode": 20.8,
    "ifbench": null,
    "aime25": null,
    "critpt": null,
    "mmmu_pro": null,
    "livecodebench": 17.8,
    "sources": [
      "Artificial Analysis"
    ],
    "source_count": 1,
    "data_tier": 2,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.915244+00:00",
        "confidence": 0.65,
        "raw_name": "Mistral Large (Feb '24)",
        "raw_scores": {
          "intelligence_score": 9.90917085347575,
          "context_window": 32768,
          "hle": 3.4,
          "gpqa": 35.1,
          "scicode": 20.8,
          "livecodebench": 17.8
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 9.90917085347575,
        "hle": 3.4,
        "gpqa": 35.1,
        "scicode": 20.8,
        "livecodebench": 17.8
      }
    },
    "confidence_score": 0.65,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "mixtral-8x7b-instruct",
    "canonical_name": "Mixtral 8x7B Instruct",
    "model_name": "Mixtral 8x7B Instruct",
    "aliases": [
      "Mixtral 8x7B Instruct"
    ],
    "provider": "Mistral AI",
    "context_window": 32768,
    "open_source": true,
    "arena_votes": null,
    "license_type": "Apache 2.0",
    "creator": null,
    "intelligence_score": 7.7312,
    "coding_score": null,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": null,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": null,
    "tau2": null,
    "lcr": null,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": 4.5,
    "gpqa": 29.2,
    "scicode": 2.8,
    "ifbench": null,
    "aime25": null,
    "critpt": null,
    "mmmu_pro": null,
    "livecodebench": 6.6,
    "sources": [
      "Artificial Analysis"
    ],
    "source_count": 1,
    "data_tier": 2,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.915276+00:00",
        "confidence": 0.65,
        "raw_name": "Mixtral 8x7B Instruct",
        "raw_scores": {
          "intelligence_score": 7.731195590246845,
          "context_window": 32768,
          "hle": 4.5,
          "gpqa": 29.2,
          "scicode": 2.8,
          "livecodebench": 6.6
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 7.731195590246845,
        "hle": 4.5,
        "gpqa": 29.2,
        "scicode": 2.8,
        "livecodebench": 6.6
      }
    },
    "confidence_score": 0.65,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "mistral-7b-instruct",
    "canonical_name": "Mistral 7B Instruct",
    "model_name": "Mistral 7B Instruct",
    "aliases": [
      "Mistral 7B Instruct",
      "mistral-7b-instruct"
    ],
    "provider": "Mistral",
    "context_window": 8192,
    "open_source": true,
    "arena_votes": 8977,
    "license_type": "Apache 2.0",
    "creator": null,
    "intelligence_score": 7.4139,
    "coding_score": null,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 1109.58,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": null,
    "tau2": 0.0,
    "lcr": 0.0,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": 4.3,
    "gpqa": 17.7,
    "scicode": 2.4,
    "ifbench": 19.9,
    "aime25": null,
    "critpt": 0.0,
    "mmmu_pro": null,
    "livecodebench": 4.6,
    "sources": [
      "Artificial Analysis",
      "LMSYS Chatbot Arena"
    ],
    "source_count": 2,
    "data_tier": 1,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.915315+00:00",
        "confidence": 0.65,
        "raw_name": "Mistral 7B Instruct",
        "raw_scores": {
          "intelligence_score": 7.413887665229345,
          "context_window": 8192,
          "tau2": 0.0,
          "lcr": 0.0,
          "hle": 4.3,
          "gpqa": 17.7,
          "scicode": 2.4,
          "ifbench": 19.9,
          "critpt": 0.0,
          "livecodebench": 4.6
        }
      },
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.895321+00:00",
        "confidence": 1.0,
        "raw_name": "mistral-7b-instruct",
        "raw_scores": {
          "arena_elo": 1109.58,
          "arena_votes": 8977
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 7.413887665229345,
        "tau2": 0.0,
        "lcr": 0.0,
        "hle": 4.3,
        "gpqa": 17.7,
        "scicode": 2.4,
        "ifbench": 19.9,
        "critpt": 0.0,
        "livecodebench": 4.6
      },
      "LMSYS Chatbot Arena": {
        "arena_elo": 1109.58
      }
    },
    "confidence_score": 0.825,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "mistral-saba",
    "canonical_name": "Mistral Saba",
    "model_name": "Mistral Saba",
    "aliases": [
      "Mistral Saba"
    ],
    "provider": "Mistral AI",
    "context_window": 32000,
    "open_source": null,
    "arena_votes": null,
    "license_type": null,
    "creator": null,
    "intelligence_score": 12.129,
    "coding_score": null,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": null,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": 0.0,
    "latency_seconds": 0.0,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": 0.0,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": null,
    "tau2": null,
    "lcr": null,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": 4.1,
    "gpqa": 42.4,
    "scicode": 24.1,
    "ifbench": null,
    "aime25": null,
    "critpt": null,
    "mmmu_pro": null,
    "livecodebench": null,
    "sources": [
      "Artificial Analysis"
    ],
    "source_count": 1,
    "data_tier": 2,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.915350+00:00",
        "confidence": 0.79,
        "raw_name": "Mistral Saba",
        "raw_scores": {
          "intelligence_score": 12.128983524455348,
          "blended_cost_per_1m": 0.0,
          "latency_seconds": 0.0,
          "tokens_per_second": 0.0,
          "context_window": 32000,
          "hle": 4.1,
          "gpqa": 42.4,
          "scicode": 24.1
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 12.128983524455348,
        "hle": 4.1,
        "gpqa": 42.4,
        "scicode": 24.1
      }
    },
    "confidence_score": 0.79,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "devstral-small",
    "canonical_name": "Devstral Small (Jul '25)",
    "model_name": "Devstral Small (Jul '25)",
    "aliases": [
      "Devstral Small (Jul '25)",
      "Devstral Small (May '25)"
    ],
    "provider": null,
    "context_window": 256000,
    "open_source": true,
    "arena_votes": null,
    "license_type": "Apache 2.0",
    "creator": null,
    "intelligence_score": 13.5509,
    "coding_score": 12.1781,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": null,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": 725.6071,
    "terminalbench_hard": 0.061,
    "tau2": 0.3298,
    "lcr": 21.6252,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": 3.843,
    "gpqa": 42.3536,
    "scicode": 24.3954,
    "ifbench": 33.1695,
    "aime25": 29.3,
    "critpt": 0.0,
    "mmmu_pro": null,
    "livecodebench": 25.5907,
    "sources": [
      "Artificial Analysis"
    ],
    "source_count": 1,
    "data_tier": 2,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.915391+00:00",
        "confidence": 0.79,
        "raw_name": "Devstral Small (Jul '25)",
        "raw_scores": {
          "intelligence_score": 14.840945563179064,
          "coding_score": 12.14,
          "context_window": 256000,
          "gdpval": 604.9877846983588,
          "terminalbench_hard": 0.061,
          "tau2": 0.284,
          "lcr": 17.0,
          "hle": 3.7,
          "gpqa": 41.4,
          "scicode": 24.3,
          "ifbench": 34.6,
          "aime25": 29.3,
          "critpt": 0.0,
          "livecodebench": 25.4
        }
      },
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.915517+00:00",
        "confidence": 0.72,
        "raw_name": "Devstral Small (May '25)",
        "raw_scores": {
          "intelligence_score": 12.13536984200864,
          "coding_score": 12.22,
          "context_window": 256000,
          "gdpval": 857.9532052664244,
          "terminalbench_hard": 0.061,
          "tau2": 0.38,
          "lcr": 26.7,
          "hle": 4.0,
          "gpqa": 43.4,
          "scicode": 24.5,
          "ifbench": 31.6,
          "critpt": 0.0,
          "livecodebench": 25.8
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 14.840945563179064,
        "coding_score": 12.14,
        "gdpval": 604.9877846983588,
        "terminalbench_hard": 0.061,
        "tau2": 0.284,
        "lcr": 17.0,
        "hle": 3.7,
        "gpqa": 41.4,
        "scicode": 24.3,
        "ifbench": 34.6,
        "aime25": 29.3,
        "critpt": 0.0,
        "livecodebench": 25.4
      }
    },
    "confidence_score": 0.755,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "magistral-medium-1",
    "canonical_name": "Magistral Medium 1",
    "model_name": "Magistral Medium 1",
    "aliases": [
      "Magistral Medium 1"
    ],
    "provider": null,
    "context_window": 40000,
    "open_source": null,
    "arena_votes": null,
    "license_type": null,
    "creator": null,
    "intelligence_score": 17.4124,
    "coding_score": 15.96,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": null,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": 0.0,
    "latency_seconds": 0.0,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": 0.0,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": 704.8141,
    "terminalbench_hard": 0.091,
    "tau2": 0.231,
    "lcr": 0.0,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": 9.5,
    "gpqa": 67.9,
    "scicode": 29.7,
    "ifbench": 25.1,
    "aime25": 40.3,
    "critpt": 0.3,
    "mmmu_pro": null,
    "livecodebench": 52.7,
    "sources": [
      "Artificial Analysis"
    ],
    "source_count": 1,
    "data_tier": 2,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.915434+00:00",
        "confidence": 1.0,
        "raw_name": "Magistral Medium 1",
        "raw_scores": {
          "intelligence_score": 17.41244103787162,
          "coding_score": 15.96,
          "blended_cost_per_1m": 0.0,
          "latency_seconds": 0.0,
          "tokens_per_second": 0.0,
          "context_window": 40000,
          "gdpval": 704.8140599525326,
          "terminalbench_hard": 0.091,
          "tau2": 0.231,
          "lcr": 0.0,
          "hle": 9.5,
          "gpqa": 67.9,
          "scicode": 29.7,
          "ifbench": 25.1,
          "aime25": 40.3,
          "critpt": 0.3,
          "livecodebench": 52.7
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 17.41244103787162,
        "coding_score": 15.96,
        "gdpval": 704.8140599525326,
        "terminalbench_hard": 0.091,
        "tau2": 0.231,
        "lcr": 0.0,
        "hle": 9.5,
        "gpqa": 67.9,
        "scicode": 29.7,
        "ifbench": 25.1,
        "aime25": 40.3,
        "critpt": 0.3,
        "livecodebench": 52.7
      }
    },
    "confidence_score": 1.0,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "devstral-medium",
    "canonical_name": "Devstral Medium",
    "model_name": "Devstral Medium",
    "aliases": [
      "Devstral Medium"
    ],
    "provider": null,
    "context_window": 256000,
    "open_source": null,
    "arena_votes": null,
    "license_type": null,
    "creator": null,
    "intelligence_score": 15.1028,
    "coding_score": 15.86,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": null,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": 0.8,
    "latency_seconds": 0.44,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": 109.0,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": 734.7735,
    "terminalbench_hard": 0.091,
    "tau2": 0.199,
    "lcr": 28.7,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": 3.8,
    "gpqa": 49.2,
    "scicode": 29.4,
    "ifbench": 29.9,
    "aime25": 4.7,
    "critpt": 0.0,
    "mmmu_pro": null,
    "livecodebench": 33.7,
    "sources": [
      "Artificial Analysis"
    ],
    "source_count": 1,
    "data_tier": 2,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.915477+00:00",
        "confidence": 1.0,
        "raw_name": "Devstral Medium",
        "raw_scores": {
          "intelligence_score": 15.102796916165374,
          "coding_score": 15.86,
          "blended_cost_per_1m": 0.8,
          "latency_seconds": 0.44,
          "tokens_per_second": 109.0,
          "context_window": 256000,
          "gdpval": 734.7735448842205,
          "terminalbench_hard": 0.091,
          "tau2": 0.199,
          "lcr": 28.7,
          "hle": 3.8,
          "gpqa": 49.2,
          "scicode": 29.4,
          "ifbench": 29.9,
          "aime25": 4.7,
          "critpt": 0.0,
          "livecodebench": 33.7
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 15.102796916165374,
        "coding_score": 15.86,
        "gdpval": 734.7735448842205,
        "terminalbench_hard": 0.091,
        "tau2": 0.199,
        "lcr": 28.7,
        "hle": 3.8,
        "gpqa": 49.2,
        "scicode": 29.4,
        "ifbench": 29.9,
        "aime25": 4.7,
        "critpt": 0.0,
        "livecodebench": 33.7
      }
    },
    "confidence_score": 1.0,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "mistral-small-31",
    "canonical_name": "Mistral Small 3.1",
    "model_name": "Mistral Small 3.1",
    "aliases": [
      "Mistral Small 3.1"
    ],
    "provider": "Mistral AI",
    "context_window": 128000,
    "open_source": true,
    "arena_votes": null,
    "license_type": "Apache 2.0",
    "creator": null,
    "intelligence_score": 13.9745,
    "coding_score": 13.89,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": null,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": 0.15,
    "latency_seconds": 0.37,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": 143.0,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": 383.2095,
    "terminalbench_hard": 0.076,
    "tau2": 0.251,
    "lcr": 19.7,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": 4.8,
    "gpqa": 45.4,
    "scicode": 26.5,
    "ifbench": 29.9,
    "aime25": 3.7,
    "critpt": 0.0,
    "mmmu_pro": null,
    "livecodebench": 21.2,
    "sources": [
      "Artificial Analysis"
    ],
    "source_count": 1,
    "data_tier": 2,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.915560+00:00",
        "confidence": 1.0,
        "raw_name": "Mistral Small 3.1",
        "raw_scores": {
          "intelligence_score": 13.974509959335386,
          "coding_score": 13.89,
          "blended_cost_per_1m": 0.15,
          "latency_seconds": 0.37,
          "tokens_per_second": 143.0,
          "context_window": 128000,
          "gdpval": 383.20952908756135,
          "terminalbench_hard": 0.076,
          "tau2": 0.251,
          "lcr": 19.7,
          "hle": 4.8,
          "gpqa": 45.4,
          "scicode": 26.5,
          "ifbench": 29.9,
          "aime25": 3.7,
          "critpt": 0.0,
          "livecodebench": 21.2
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 13.974509959335386,
        "coding_score": 13.89,
        "gdpval": 383.20952908756135,
        "terminalbench_hard": 0.076,
        "tau2": 0.251,
        "lcr": 19.7,
        "hle": 4.8,
        "gpqa": 45.4,
        "scicode": 26.5,
        "ifbench": 29.9,
        "aime25": 3.7,
        "critpt": 0.0,
        "livecodebench": 21.2
      }
    },
    "confidence_score": 1.0,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "magistral-small-1",
    "canonical_name": "Magistral Small 1",
    "model_name": "Magistral Small 1",
    "aliases": [
      "Magistral Small 1"
    ],
    "provider": null,
    "context_window": 40000,
    "open_source": true,
    "arena_votes": null,
    "license_type": "Apache 2.0",
    "creator": null,
    "intelligence_score": 16.7916,
    "coding_score": 11.05,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": null,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": 0.0,
    "latency_seconds": 0.0,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": 0.0,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": 0.045,
    "tau2": 0.266,
    "lcr": 0.0,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": 7.2,
    "gpqa": 64.1,
    "scicode": 24.1,
    "ifbench": 24.8,
    "aime25": 41.3,
    "critpt": 0.0,
    "mmmu_pro": null,
    "livecodebench": 51.4,
    "sources": [
      "Artificial Analysis"
    ],
    "source_count": 1,
    "data_tier": 2,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.915602+00:00",
        "confidence": 1.0,
        "raw_name": "Magistral Small 1",
        "raw_scores": {
          "intelligence_score": 16.791620007828033,
          "coding_score": 11.05,
          "blended_cost_per_1m": 0.0,
          "latency_seconds": 0.0,
          "tokens_per_second": 0.0,
          "context_window": 40000,
          "terminalbench_hard": 0.045,
          "tau2": 0.266,
          "lcr": 0.0,
          "hle": 7.2,
          "gpqa": 64.1,
          "scicode": 24.1,
          "ifbench": 24.8,
          "aime25": 41.3,
          "critpt": 0.0,
          "livecodebench": 51.4
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 16.791620007828033,
        "coding_score": 11.05,
        "terminalbench_hard": 0.045,
        "tau2": 0.266,
        "lcr": 0.0,
        "hle": 7.2,
        "gpqa": 64.1,
        "scicode": 24.1,
        "ifbench": 24.8,
        "aime25": 41.3,
        "critpt": 0.0,
        "livecodebench": 51.4
      }
    },
    "confidence_score": 1.0,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "mistral-medium-3",
    "canonical_name": "Mistral Medium 3",
    "model_name": "Mistral Medium 3",
    "aliases": [
      "Mistral Medium 3"
    ],
    "provider": "Mistral AI",
    "context_window": 128000,
    "open_source": null,
    "arena_votes": null,
    "license_type": null,
    "creator": null,
    "intelligence_score": 17.612,
    "coding_score": 13.56,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": null,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": 0.8,
    "latency_seconds": 0.41,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": 50.0,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": 639.1114,
    "terminalbench_hard": 0.038,
    "tau2": 0.243,
    "lcr": 28.0,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": 4.3,
    "gpqa": 57.8,
    "scicode": 33.1,
    "ifbench": 39.3,
    "aime25": 30.3,
    "critpt": 0.0,
    "mmmu_pro": 53.0,
    "livecodebench": 40.0,
    "sources": [
      "Artificial Analysis"
    ],
    "source_count": 1,
    "data_tier": 2,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.915651+00:00",
        "confidence": 1.0,
        "raw_name": "Mistral Medium 3",
        "raw_scores": {
          "intelligence_score": 17.611990577780258,
          "coding_score": 13.56,
          "blended_cost_per_1m": 0.8,
          "latency_seconds": 0.41,
          "tokens_per_second": 50.0,
          "context_window": 128000,
          "gdpval": 639.1114228589472,
          "terminalbench_hard": 0.038,
          "tau2": 0.243,
          "lcr": 28.0,
          "hle": 4.3,
          "gpqa": 57.8,
          "scicode": 33.1,
          "ifbench": 39.3,
          "aime25": 30.3,
          "critpt": 0.0,
          "mmmu_pro": 53.0,
          "livecodebench": 40.0
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 17.611990577780258,
        "coding_score": 13.56,
        "gdpval": 639.1114228589472,
        "terminalbench_hard": 0.038,
        "tau2": 0.243,
        "lcr": 28.0,
        "hle": 4.3,
        "gpqa": 57.8,
        "scicode": 33.1,
        "ifbench": 39.3,
        "aime25": 30.3,
        "critpt": 0.0,
        "mmmu_pro": 53.0,
        "livecodebench": 40.0
      }
    },
    "confidence_score": 1.0,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "mistral-medium",
    "canonical_name": "Mistral Medium",
    "model_name": "Mistral Medium",
    "aliases": [
      "Mistral Medium",
      "mistral-medium",
      "mistral-medium-2508"
    ],
    "provider": "Mistral",
    "context_window": 32768,
    "open_source": null,
    "arena_votes": 65627,
    "license_type": "Proprietary",
    "creator": null,
    "intelligence_score": 9.011,
    "coding_score": null,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 1317.175,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": 4.09,
    "latency_seconds": 0.43,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": 85.0,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": null,
    "tau2": null,
    "lcr": null,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": 3.4,
    "gpqa": 34.9,
    "scicode": 11.8,
    "ifbench": null,
    "aime25": null,
    "critpt": null,
    "mmmu_pro": null,
    "livecodebench": 9.9,
    "sources": [
      "Artificial Analysis",
      "LMSYS Chatbot Arena"
    ],
    "source_count": 2,
    "data_tier": 1,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.915684+00:00",
        "confidence": 0.8600000000000001,
        "raw_name": "Mistral Medium",
        "raw_scores": {
          "intelligence_score": 9.010996334814534,
          "blended_cost_per_1m": 4.09,
          "latency_seconds": 0.43,
          "tokens_per_second": 85.0,
          "context_window": 32768,
          "hle": 3.4,
          "gpqa": 34.9,
          "scicode": 11.8,
          "livecodebench": 9.9
        }
      },
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.892099+00:00",
        "confidence": 1.0,
        "raw_name": "mistral-medium-2508",
        "raw_scores": {
          "arena_elo": 1411.39,
          "arena_votes": 65627
        }
      },
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.894553+00:00",
        "confidence": 1.0,
        "raw_name": "mistral-medium",
        "raw_scores": {
          "arena_elo": 1222.96,
          "arena_votes": 34552
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 9.010996334814534,
        "hle": 3.4,
        "gpqa": 34.9,
        "scicode": 11.8,
        "livecodebench": 9.9
      },
      "LMSYS Chatbot Arena": {
        "arena_elo": 1411.39
      }
    },
    "confidence_score": 0.9533,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "deepseek-r1-distill-qwen-32b",
    "canonical_name": "DeepSeek R1 Distill Qwen 32B",
    "model_name": "DeepSeek R1 Distill Qwen 32B",
    "aliases": [
      "DeepSeek R1 Distill Qwen 32B"
    ],
    "provider": "Alibaba",
    "context_window": 128000,
    "open_source": true,
    "arena_votes": null,
    "license_type": "Apache 2.0",
    "creator": null,
    "intelligence_score": 17.1667,
    "coding_score": null,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": null,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": 0.27,
    "latency_seconds": 0.24,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": 56.0,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": null,
    "tau2": null,
    "lcr": 9.7,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": 5.5,
    "gpqa": 61.5,
    "scicode": 37.6,
    "ifbench": 22.9,
    "aime25": 63.0,
    "critpt": null,
    "mmmu_pro": null,
    "livecodebench": 27.0,
    "sources": [
      "Artificial Analysis"
    ],
    "source_count": 1,
    "data_tier": 2,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.915721+00:00",
        "confidence": 0.9300000000000002,
        "raw_name": "DeepSeek R1 Distill Qwen 32B",
        "raw_scores": {
          "intelligence_score": 17.166714190986596,
          "blended_cost_per_1m": 0.27,
          "latency_seconds": 0.24,
          "tokens_per_second": 56.0,
          "context_window": 128000,
          "lcr": 9.7,
          "hle": 5.5,
          "gpqa": 61.5,
          "scicode": 37.6,
          "ifbench": 22.9,
          "aime25": 63.0,
          "livecodebench": 27.0
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 17.166714190986596,
        "lcr": 9.7,
        "hle": 5.5,
        "gpqa": 61.5,
        "scicode": 37.6,
        "ifbench": 22.9,
        "aime25": 63.0,
        "livecodebench": 27.0
      }
    },
    "confidence_score": 0.93,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "deepseek-v3",
    "canonical_name": "DeepSeek V3 (Dec '24)",
    "model_name": "DeepSeek V3 (Dec '24)",
    "aliases": [
      "DeepSeek V3 (Dec '24)",
      "deepseek-v3"
    ],
    "provider": "DeepSeek",
    "context_window": 128000,
    "open_source": true,
    "arena_votes": 21788,
    "license_type": "DEEPSEEK LICENSE AGREEMENT",
    "creator": null,
    "intelligence_score": 17.0698,
    "coding_score": 16.35,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 1358.47,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": 469.1425,
    "terminalbench_hard": 0.068,
    "tau2": 0.228,
    "lcr": 29.0,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": 3.6,
    "gpqa": 55.7,
    "scicode": 35.4,
    "ifbench": 34.8,
    "aime25": 26.0,
    "critpt": 0.0,
    "mmmu_pro": null,
    "livecodebench": 35.9,
    "sources": [
      "Artificial Analysis",
      "LMSYS Chatbot Arena"
    ],
    "source_count": 2,
    "data_tier": 1,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.915762+00:00",
        "confidence": 0.79,
        "raw_name": "DeepSeek V3 (Dec '24)",
        "raw_scores": {
          "intelligence_score": 17.06978519415099,
          "coding_score": 16.35,
          "context_window": 128000,
          "gdpval": 469.1425037786339,
          "terminalbench_hard": 0.068,
          "tau2": 0.228,
          "lcr": 29.0,
          "hle": 3.6,
          "gpqa": 55.7,
          "scicode": 35.4,
          "ifbench": 34.8,
          "aime25": 26.0,
          "critpt": 0.0,
          "livecodebench": 35.9
        }
      },
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.892928+00:00",
        "confidence": 1.0,
        "raw_name": "deepseek-v3",
        "raw_scores": {
          "arena_elo": 1358.47,
          "arena_votes": 21788
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 17.06978519415099,
        "coding_score": 16.35,
        "gdpval": 469.1425037786339,
        "terminalbench_hard": 0.068,
        "tau2": 0.228,
        "lcr": 29.0,
        "hle": 3.6,
        "gpqa": 55.7,
        "scicode": 35.4,
        "ifbench": 34.8,
        "aime25": 26.0,
        "critpt": 0.0,
        "livecodebench": 35.9
      },
      "LMSYS Chatbot Arena": {
        "arena_elo": 1358.47
      }
    },
    "confidence_score": 0.895,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "deepseek-r1-distill-qwen-14b",
    "canonical_name": "DeepSeek R1 Distill Qwen 14B",
    "model_name": "DeepSeek R1 Distill Qwen 14B",
    "aliases": [
      "DeepSeek R1 Distill Qwen 14B"
    ],
    "provider": "Alibaba",
    "context_window": 128000,
    "open_source": true,
    "arena_votes": null,
    "license_type": "Apache 2.0",
    "creator": null,
    "intelligence_score": 15.8445,
    "coding_score": null,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": null,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": 0.0,
    "latency_seconds": 0.0,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": 0.0,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": null,
    "tau2": null,
    "lcr": 7.0,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": 4.4,
    "gpqa": 48.4,
    "scicode": 23.9,
    "ifbench": 22.1,
    "aime25": 55.7,
    "critpt": null,
    "mmmu_pro": null,
    "livecodebench": 37.6,
    "sources": [
      "Artificial Analysis"
    ],
    "source_count": 1,
    "data_tier": 2,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.915800+00:00",
        "confidence": 0.9300000000000002,
        "raw_name": "DeepSeek R1 Distill Qwen 14B",
        "raw_scores": {
          "intelligence_score": 15.844510353679771,
          "blended_cost_per_1m": 0.0,
          "latency_seconds": 0.0,
          "tokens_per_second": 0.0,
          "context_window": 128000,
          "lcr": 7.0,
          "hle": 4.4,
          "gpqa": 48.4,
          "scicode": 23.9,
          "ifbench": 22.1,
          "aime25": 55.7,
          "livecodebench": 37.6
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 15.844510353679771,
        "lcr": 7.0,
        "hle": 4.4,
        "gpqa": 48.4,
        "scicode": 23.9,
        "ifbench": 22.1,
        "aime25": 55.7,
        "livecodebench": 37.6
      }
    },
    "confidence_score": 0.93,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "deepseek-v25",
    "canonical_name": "DeepSeek-V2.5 (Dec '24)",
    "model_name": "DeepSeek-V2.5 (Dec '24)",
    "aliases": [
      "DeepSeek-V2.5",
      "DeepSeek-V2.5 (Dec '24)",
      "deepseek-v2.5"
    ],
    "provider": "DeepSeek",
    "context_window": 128000,
    "open_source": true,
    "arena_votes": 24574,
    "license_type": "DEEPSEEK LICENSE AGREEMENT",
    "creator": null,
    "intelligence_score": 12.4005,
    "coding_score": null,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 1306.99,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": 0.0,
    "latency_seconds": 0.0,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": 0.0,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": null,
    "tau2": null,
    "lcr": null,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": null,
    "gpqa": null,
    "scicode": null,
    "ifbench": null,
    "aime25": null,
    "critpt": null,
    "mmmu_pro": null,
    "livecodebench": null,
    "sources": [
      "Artificial Analysis",
      "LMSYS Chatbot Arena"
    ],
    "source_count": 2,
    "data_tier": 1,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.915819+00:00",
        "confidence": 0.44,
        "raw_name": "DeepSeek-V2.5 (Dec '24)",
        "raw_scores": {
          "intelligence_score": 12.511590737265823,
          "context_window": 128000
        }
      },
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.916332+00:00",
        "confidence": 0.65,
        "raw_name": "DeepSeek-V2.5",
        "raw_scores": {
          "intelligence_score": 12.3252882665075,
          "blended_cost_per_1m": 0.0,
          "latency_seconds": 0.0,
          "tokens_per_second": 0.0,
          "context_window": 128000
        }
      },
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.893811+00:00",
        "confidence": 1.0,
        "raw_name": "deepseek-v2.5",
        "raw_scores": {
          "arena_elo": 1306.99,
          "arena_votes": 24574
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 12.511590737265823
      },
      "LMSYS Chatbot Arena": {
        "arena_elo": 1306.99
      }
    },
    "confidence_score": 0.6967,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "deepseek-coder-v2",
    "canonical_name": "DeepSeek-Coder-V2",
    "model_name": "DeepSeek-Coder-V2",
    "aliases": [
      "DeepSeek-Coder-V2",
      "deepseek-coder-v2"
    ],
    "provider": "DeepSeek",
    "context_window": 128000,
    "open_source": true,
    "arena_votes": 15147,
    "license_type": "DEEPSEEK LICENSE AGREEMENT",
    "creator": null,
    "intelligence_score": 10.6082,
    "coding_score": null,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 1264.19,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": 0.0,
    "latency_seconds": 0.0,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": 0.0,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": null,
    "tau2": null,
    "lcr": null,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": null,
    "gpqa": null,
    "scicode": null,
    "ifbench": null,
    "aime25": null,
    "critpt": null,
    "mmmu_pro": null,
    "livecodebench": null,
    "sources": [
      "Artificial Analysis",
      "LMSYS Chatbot Arena"
    ],
    "source_count": 2,
    "data_tier": 1,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.915843+00:00",
        "confidence": 0.65,
        "raw_name": "DeepSeek-Coder-V2",
        "raw_scores": {
          "intelligence_score": 10.608221945579603,
          "blended_cost_per_1m": 0.0,
          "latency_seconds": 0.0,
          "tokens_per_second": 0.0,
          "context_window": 128000
        }
      },
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.894250+00:00",
        "confidence": 1.0,
        "raw_name": "deepseek-coder-v2",
        "raw_scores": {
          "arena_elo": 1264.19,
          "arena_votes": 15147
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 10.608221945579603
      },
      "LMSYS Chatbot Arena": {
        "arena_elo": 1264.19
      }
    },
    "confidence_score": 0.825,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "deepseek-r1-distill-llama-8b",
    "canonical_name": "DeepSeek R1 Distill Llama 8B",
    "model_name": "DeepSeek R1 Distill Llama 8B",
    "aliases": [
      "DeepSeek R1 Distill Llama 8B"
    ],
    "provider": "Meta",
    "context_window": 128000,
    "open_source": true,
    "arena_votes": null,
    "license_type": "LLAMA 3.1 COMMUNITY LICENSE AGREEMENT",
    "creator": null,
    "intelligence_score": 12.1003,
    "coding_score": null,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": null,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": 0.0,
    "latency_seconds": 0.0,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": 0.0,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": null,
    "tau2": null,
    "lcr": 0.0,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": 4.2,
    "gpqa": 30.2,
    "scicode": 11.9,
    "ifbench": 17.6,
    "aime25": 41.3,
    "critpt": null,
    "mmmu_pro": null,
    "livecodebench": 23.3,
    "sources": [
      "Artificial Analysis"
    ],
    "source_count": 1,
    "data_tier": 2,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.915878+00:00",
        "confidence": 0.9300000000000002,
        "raw_name": "DeepSeek R1 Distill Llama 8B",
        "raw_scores": {
          "intelligence_score": 12.10028639307147,
          "blended_cost_per_1m": 0.0,
          "latency_seconds": 0.0,
          "tokens_per_second": 0.0,
          "context_window": 128000,
          "lcr": 0.0,
          "hle": 4.2,
          "gpqa": 30.2,
          "scicode": 11.9,
          "ifbench": 17.6,
          "aime25": 41.3,
          "livecodebench": 23.3
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 12.10028639307147,
        "lcr": 0.0,
        "hle": 4.2,
        "gpqa": 30.2,
        "scicode": 11.9,
        "ifbench": 17.6,
        "aime25": 41.3,
        "livecodebench": 23.3
      }
    },
    "confidence_score": 0.93,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "deepseek-llm-67b-chat",
    "canonical_name": "DeepSeek LLM 67B Chat (V1)",
    "model_name": "DeepSeek LLM 67B Chat (V1)",
    "aliases": [
      "DeepSeek LLM 67B Chat (V1)",
      "deepseek-llm-67b-chat"
    ],
    "provider": "DeepSeek",
    "context_window": 4096,
    "open_source": true,
    "arena_votes": 4933,
    "license_type": "DEEPSEEK LICENSE AGREEMENT",
    "creator": null,
    "intelligence_score": 8.3708,
    "coding_score": null,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 1184.35,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": null,
    "tau2": null,
    "lcr": null,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": null,
    "gpqa": null,
    "scicode": null,
    "ifbench": null,
    "aime25": null,
    "critpt": null,
    "mmmu_pro": null,
    "livecodebench": null,
    "sources": [
      "Artificial Analysis",
      "LMSYS Chatbot Arena"
    ],
    "source_count": 2,
    "data_tier": 1,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.915899+00:00",
        "confidence": 0.44,
        "raw_name": "DeepSeek LLM 67B Chat (V1)",
        "raw_scores": {
          "intelligence_score": 8.370802665737399,
          "context_window": 4096
        }
      },
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.894751+00:00",
        "confidence": 1.0,
        "raw_name": "deepseek-llm-67b-chat",
        "raw_scores": {
          "arena_elo": 1184.35,
          "arena_votes": 4933
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 8.370802665737399
      },
      "LMSYS Chatbot Arena": {
        "arena_elo": 1184.35
      }
    },
    "confidence_score": 0.72,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "deepseek-r1-distill-qwen-15b",
    "canonical_name": "DeepSeek R1 Distill Qwen 1.5B",
    "model_name": "DeepSeek R1 Distill Qwen 1.5B",
    "aliases": [
      "DeepSeek R1 Distill Qwen 1.5B"
    ],
    "provider": "Alibaba",
    "context_window": 128000,
    "open_source": true,
    "arena_votes": null,
    "license_type": "Apache 2.0",
    "creator": null,
    "intelligence_score": 9.0753,
    "coding_score": null,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": null,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": 0.0,
    "latency_seconds": 0.0,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": 0.0,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": null,
    "tau2": null,
    "lcr": 0.3,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": 3.3,
    "gpqa": 9.8,
    "scicode": 6.6,
    "ifbench": 13.2,
    "aime25": 22.0,
    "critpt": null,
    "mmmu_pro": null,
    "livecodebench": 7.0,
    "sources": [
      "Artificial Analysis"
    ],
    "source_count": 1,
    "data_tier": 2,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.915936+00:00",
        "confidence": 0.9300000000000002,
        "raw_name": "DeepSeek R1 Distill Qwen 1.5B",
        "raw_scores": {
          "intelligence_score": 9.075326286181491,
          "blended_cost_per_1m": 0.0,
          "latency_seconds": 0.0,
          "tokens_per_second": 0.0,
          "context_window": 128000,
          "lcr": 0.3,
          "hle": 3.3,
          "gpqa": 9.8,
          "scicode": 6.6,
          "ifbench": 13.2,
          "aime25": 22.0,
          "livecodebench": 7.0
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 9.075326286181491,
        "lcr": 0.3,
        "hle": 3.3,
        "gpqa": 9.8,
        "scicode": 6.6,
        "ifbench": 13.2,
        "aime25": 22.0,
        "livecodebench": 7.0
      }
    },
    "confidence_score": 0.93,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "deepseek-v3-0324",
    "canonical_name": "DeepSeek V3 0324",
    "model_name": "DeepSeek V3 0324",
    "aliases": [
      "DeepSeek V3 0324",
      "deepseek-v3-0324"
    ],
    "provider": "DeepSeek",
    "context_window": 128000,
    "open_source": true,
    "arena_votes": 46431,
    "license_type": "MIT",
    "creator": null,
    "intelligence_score": 22.28,
    "coding_score": 22.02,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 1394.16,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": 1.25,
    "latency_seconds": 0.0,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": 0.0,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": 458.0688,
    "terminalbench_hard": 0.152,
    "tau2": 0.471,
    "lcr": 41.0,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": 5.2,
    "gpqa": 65.5,
    "scicode": 35.8,
    "ifbench": 41.0,
    "aime25": 41.0,
    "critpt": 0.0,
    "mmmu_pro": null,
    "livecodebench": 40.5,
    "sources": [
      "Artificial Analysis",
      "LMSYS Chatbot Arena"
    ],
    "source_count": 2,
    "data_tier": 1,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.915980+00:00",
        "confidence": 1.0,
        "raw_name": "DeepSeek V3 0324",
        "raw_scores": {
          "intelligence_score": 22.28,
          "coding_score": 22.02,
          "blended_cost_per_1m": 1.25,
          "latency_seconds": 0.0,
          "tokens_per_second": 0.0,
          "context_window": 128000,
          "gdpval": 458.06880157653995,
          "terminalbench_hard": 0.152,
          "tau2": 0.471,
          "lcr": 41.0,
          "hle": 5.2,
          "gpqa": 65.5,
          "scicode": 35.8,
          "ifbench": 41.0,
          "aime25": 41.0,
          "critpt": 0.0,
          "livecodebench": 40.5
        }
      },
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.892361+00:00",
        "confidence": 1.0,
        "raw_name": "deepseek-v3-0324",
        "raw_scores": {
          "arena_elo": 1394.16,
          "arena_votes": 46431
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 22.28,
        "coding_score": 22.02,
        "gdpval": 458.06880157653995,
        "terminalbench_hard": 0.152,
        "tau2": 0.471,
        "lcr": 41.0,
        "hle": 5.2,
        "gpqa": 65.5,
        "scicode": 35.8,
        "ifbench": 41.0,
        "aime25": 41.0,
        "critpt": 0.0,
        "livecodebench": 40.5
      },
      "LMSYS Chatbot Arena": {
        "arena_elo": 1394.16
      }
    },
    "confidence_score": 1.0,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "deepseek-v31",
    "canonical_name": "DeepSeek V3.1 (Non-reasoning)",
    "model_name": "DeepSeek V3.1 (Non-reasoning)",
    "aliases": [
      "DeepSeek V3.1 (Non-reasoning)",
      "DeepSeek V3.1 (Reasoning)",
      "DeepSeek V3.1 Terminus (Non-reasoning)",
      "DeepSeek V3.1 Terminus (Reasoning)",
      "deepseek-v3.1",
      "deepseek-v3.1-terminus",
      "deepseek-v3.1-terminus-thinking",
      "deepseek-v3.1-thinking"
    ],
    "provider": "DeepSeek",
    "context_window": 128000,
    "open_source": true,
    "arena_votes": 15194,
    "license_type": "MIT",
    "creator": null,
    "intelligence_score": 29.5725,
    "coding_score": 30.935,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 1416.835,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": 940.4847,
    "terminalbench_hard": 0.2782,
    "tau2": 0.366,
    "lcr": 51.65,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": 10.725,
    "gpqa": 76.425,
    "scicode": 37.125,
    "ifbench": 44.375,
    "aime25": 70.7,
    "critpt": 0.925,
    "mmmu_pro": null,
    "livecodebench": 67.2,
    "sources": [
      "Artificial Analysis",
      "LMSYS Chatbot Arena"
    ],
    "source_count": 2,
    "data_tier": 1,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.916021+00:00",
        "confidence": 0.79,
        "raw_name": "DeepSeek V3.1 (Non-reasoning)",
        "raw_scores": {
          "intelligence_score": 28.13,
          "coding_score": 28.39,
          "context_window": 128000,
          "gdpval": 1110.0016643313531,
          "terminalbench_hard": 0.242,
          "tau2": 0.348,
          "lcr": 45.0,
          "hle": 6.3,
          "gpqa": 73.5,
          "scicode": 36.7,
          "ifbench": 37.8,
          "aime25": 49.7,
          "critpt": 0.0,
          "livecodebench": 57.7
        }
      },
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.916061+00:00",
        "confidence": 0.79,
        "raw_name": "DeepSeek V3.1 Terminus (Non-reasoning)",
        "raw_scores": {
          "intelligence_score": 28.52,
          "coding_score": 31.9,
          "context_window": 128000,
          "gdpval": 996.2058081292439,
          "terminalbench_hard": 0.318,
          "tau2": 0.371,
          "lcr": 43.3,
          "hle": 8.4,
          "gpqa": 75.1,
          "scicode": 32.1,
          "ifbench": 41.2,
          "aime25": 53.7,
          "critpt": 0.0,
          "livecodebench": 52.9
        }
      },
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.916206+00:00",
        "confidence": 0.79,
        "raw_name": "DeepSeek V3.1 (Reasoning)",
        "raw_scores": {
          "intelligence_score": 27.71,
          "coding_score": 29.71,
          "context_window": 128000,
          "gdpval": 632.1303362175086,
          "terminalbench_hard": 0.25,
          "tau2": 0.374,
          "lcr": 53.3,
          "hle": 13.0,
          "gpqa": 77.9,
          "scicode": 39.1,
          "ifbench": 41.5,
          "aime25": 89.7,
          "critpt": 2.0,
          "livecodebench": 78.4
        }
      },
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.916251+00:00",
        "confidence": 0.79,
        "raw_name": "DeepSeek V3.1 Terminus (Reasoning)",
        "raw_scores": {
          "intelligence_score": 33.93,
          "coding_score": 33.74,
          "context_window": 128000,
          "gdpval": 1023.6011580763393,
          "terminalbench_hard": 0.303,
          "tau2": 0.371,
          "lcr": 65.0,
          "hle": 15.2,
          "gpqa": 79.2,
          "scicode": 40.6,
          "ifbench": 57.0,
          "aime25": 89.7,
          "critpt": 1.7,
          "livecodebench": 79.8
        }
      },
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.891961+00:00",
        "confidence": 1.0,
        "raw_name": "deepseek-v3.1",
        "raw_scores": {
          "arena_elo": 1418.24,
          "arena_votes": 15194
        }
      },
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.891974+00:00",
        "confidence": 1.0,
        "raw_name": "deepseek-v3.1-thinking",
        "raw_scores": {
          "arena_elo": 1417.23,
          "arena_votes": 11918
        }
      },
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.892012+00:00",
        "confidence": 1.0,
        "raw_name": "deepseek-v3.1-terminus",
        "raw_scores": {
          "arena_elo": 1416.1,
          "arena_votes": 3743
        }
      },
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.892025+00:00",
        "confidence": 1.0,
        "raw_name": "deepseek-v3.1-terminus-thinking",
        "raw_scores": {
          "arena_elo": 1415.77,
          "arena_votes": 3536
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 28.13,
        "coding_score": 28.39,
        "gdpval": 1110.0016643313531,
        "terminalbench_hard": 0.242,
        "tau2": 0.348,
        "lcr": 45.0,
        "hle": 6.3,
        "gpqa": 73.5,
        "scicode": 36.7,
        "ifbench": 37.8,
        "aime25": 49.7,
        "critpt": 0.0,
        "livecodebench": 57.7
      },
      "LMSYS Chatbot Arena": {
        "arena_elo": 1418.24
      }
    },
    "confidence_score": 0.895,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "deepseek-v2-chat",
    "canonical_name": "DeepSeek-V2-Chat",
    "model_name": "DeepSeek-V2-Chat",
    "aliases": [
      "DeepSeek-V2-Chat"
    ],
    "provider": "DeepSeek",
    "context_window": 128000,
    "open_source": true,
    "arena_votes": null,
    "license_type": "DEEPSEEK LICENSE AGREEMENT",
    "creator": null,
    "intelligence_score": 9.0586,
    "coding_score": null,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": null,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": null,
    "tau2": null,
    "lcr": null,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": null,
    "gpqa": null,
    "scicode": null,
    "ifbench": null,
    "aime25": null,
    "critpt": null,
    "mmmu_pro": null,
    "livecodebench": null,
    "sources": [
      "Artificial Analysis"
    ],
    "source_count": 1,
    "data_tier": 2,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.916311+00:00",
        "confidence": 0.44,
        "raw_name": "DeepSeek-V2-Chat",
        "raw_scores": {
          "intelligence_score": 9.058555199878974,
          "context_window": 128000
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 9.058555199878974
      }
    },
    "confidence_score": 0.44,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "deepseek-coder-v2-lite-instruct",
    "canonical_name": "DeepSeek Coder V2 Lite Instruct",
    "model_name": "DeepSeek Coder V2 Lite Instruct",
    "aliases": [
      "DeepSeek Coder V2 Lite Instruct"
    ],
    "provider": "DeepSeek",
    "context_window": 128000,
    "open_source": true,
    "arena_votes": null,
    "license_type": "DEEPSEEK LICENSE AGREEMENT",
    "creator": null,
    "intelligence_score": 8.4795,
    "coding_score": null,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": null,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": null,
    "tau2": null,
    "lcr": null,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": 5.3,
    "gpqa": 31.9,
    "scicode": 13.9,
    "ifbench": null,
    "aime25": null,
    "critpt": null,
    "mmmu_pro": null,
    "livecodebench": 15.8,
    "sources": [
      "Artificial Analysis"
    ],
    "source_count": 1,
    "data_tier": 2,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.916361+00:00",
        "confidence": 0.65,
        "raw_name": "DeepSeek Coder V2 Lite Instruct",
        "raw_scores": {
          "intelligence_score": 8.479458137084212,
          "context_window": 128000,
          "hle": 5.3,
          "gpqa": 31.9,
          "scicode": 13.9,
          "livecodebench": 15.8
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 8.479458137084212,
        "hle": 5.3,
        "gpqa": 31.9,
        "scicode": 13.9,
        "livecodebench": 15.8
      }
    },
    "confidence_score": 0.65,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "sonar-pro",
    "canonical_name": "Sonar Pro",
    "model_name": "Sonar Pro",
    "aliases": [
      "Sonar Pro"
    ],
    "provider": null,
    "context_window": 200000,
    "open_source": null,
    "arena_votes": null,
    "license_type": null,
    "creator": null,
    "intelligence_score": 15.226,
    "coding_score": null,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": null,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": 6.0,
    "latency_seconds": 1.55,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": 99.0,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": null,
    "tau2": null,
    "lcr": null,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": 7.9,
    "gpqa": 57.8,
    "scicode": 22.6,
    "ifbench": null,
    "aime25": null,
    "critpt": null,
    "mmmu_pro": null,
    "livecodebench": 27.5,
    "sources": [
      "Artificial Analysis"
    ],
    "source_count": 1,
    "data_tier": 2,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.916393+00:00",
        "confidence": 0.8600000000000001,
        "raw_name": "Sonar Pro",
        "raw_scores": {
          "intelligence_score": 15.225966789192796,
          "blended_cost_per_1m": 6.0,
          "latency_seconds": 1.55,
          "tokens_per_second": 99.0,
          "context_window": 200000,
          "hle": 7.9,
          "gpqa": 57.8,
          "scicode": 22.6,
          "livecodebench": 27.5
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 15.225966789192796,
        "hle": 7.9,
        "gpqa": 57.8,
        "scicode": 22.6,
        "livecodebench": 27.5
      }
    },
    "confidence_score": 0.86,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "sonar",
    "canonical_name": "Sonar",
    "model_name": "Sonar",
    "aliases": [
      "Sonar"
    ],
    "provider": null,
    "context_window": 127000,
    "open_source": null,
    "arena_votes": null,
    "license_type": null,
    "creator": null,
    "intelligence_score": 15.4928,
    "coding_score": null,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": null,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": 1.0,
    "latency_seconds": 1.38,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": 122.0,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": null,
    "tau2": null,
    "lcr": null,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": 7.3,
    "gpqa": 47.1,
    "scicode": 22.9,
    "ifbench": null,
    "aime25": null,
    "critpt": null,
    "mmmu_pro": null,
    "livecodebench": 29.5,
    "sources": [
      "Artificial Analysis"
    ],
    "source_count": 1,
    "data_tier": 2,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.916428+00:00",
        "confidence": 0.8600000000000001,
        "raw_name": "Sonar",
        "raw_scores": {
          "intelligence_score": 15.492770267156052,
          "blended_cost_per_1m": 1.0,
          "latency_seconds": 1.38,
          "tokens_per_second": 122.0,
          "context_window": 127000,
          "hle": 7.3,
          "gpqa": 47.1,
          "scicode": 22.9,
          "livecodebench": 29.5
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 15.492770267156052,
        "hle": 7.3,
        "gpqa": 47.1,
        "scicode": 22.9,
        "livecodebench": 29.5
      }
    },
    "confidence_score": 0.86,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "sonar-reasoning",
    "canonical_name": "Sonar Reasoning",
    "model_name": "Sonar Reasoning",
    "aliases": [
      "Sonar Reasoning"
    ],
    "provider": null,
    "context_window": 127000,
    "open_source": null,
    "arena_votes": null,
    "license_type": null,
    "creator": null,
    "intelligence_score": 17.8777,
    "coding_score": null,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": null,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": 0.0,
    "latency_seconds": 0.0,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": 0.0,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": null,
    "tau2": null,
    "lcr": null,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": null,
    "gpqa": 62.3,
    "scicode": null,
    "ifbench": null,
    "aime25": null,
    "critpt": null,
    "mmmu_pro": null,
    "livecodebench": null,
    "sources": [
      "Artificial Analysis"
    ],
    "source_count": 1,
    "data_tier": 2,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.916452+00:00",
        "confidence": 0.72,
        "raw_name": "Sonar Reasoning",
        "raw_scores": {
          "intelligence_score": 17.87769906391809,
          "blended_cost_per_1m": 0.0,
          "latency_seconds": 0.0,
          "tokens_per_second": 0.0,
          "context_window": 127000,
          "gpqa": 62.3
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 17.87769906391809,
        "gpqa": 62.3
      }
    },
    "confidence_score": 0.72,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "sonar-reasoning-pro",
    "canonical_name": "Sonar Reasoning Pro",
    "model_name": "Sonar Reasoning Pro",
    "aliases": [
      "Sonar Reasoning Pro"
    ],
    "provider": null,
    "context_window": 127000,
    "open_source": null,
    "arena_votes": null,
    "license_type": null,
    "creator": null,
    "intelligence_score": 24.6211,
    "coding_score": null,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": null,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": 0.0,
    "latency_seconds": 0.0,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": 0.0,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": null,
    "tau2": null,
    "lcr": null,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": null,
    "gpqa": null,
    "scicode": null,
    "ifbench": null,
    "aime25": null,
    "critpt": null,
    "mmmu_pro": null,
    "livecodebench": null,
    "sources": [
      "Artificial Analysis"
    ],
    "source_count": 1,
    "data_tier": 2,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.916474+00:00",
        "confidence": 0.65,
        "raw_name": "Sonar Reasoning Pro",
        "raw_scores": {
          "intelligence_score": 24.62112108368442,
          "blended_cost_per_1m": 0.0,
          "latency_seconds": 0.0,
          "tokens_per_second": 0.0,
          "context_window": 127000
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 24.62112108368442
      }
    },
    "confidence_score": 0.65,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "grok-beta",
    "canonical_name": "Grok Beta",
    "model_name": "Grok Beta",
    "aliases": [
      "Grok Beta"
    ],
    "provider": "xAI",
    "context_window": 128000,
    "open_source": null,
    "arena_votes": null,
    "license_type": null,
    "creator": null,
    "intelligence_score": 13.2819,
    "coding_score": null,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": null,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": 0.0,
    "latency_seconds": 0.0,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": 0.0,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": null,
    "tau2": null,
    "lcr": null,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": 4.7,
    "gpqa": 47.1,
    "scicode": 29.5,
    "ifbench": null,
    "aime25": null,
    "critpt": null,
    "mmmu_pro": null,
    "livecodebench": 24.1,
    "sources": [
      "Artificial Analysis"
    ],
    "source_count": 1,
    "data_tier": 2,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.916506+00:00",
        "confidence": 0.8600000000000001,
        "raw_name": "Grok Beta",
        "raw_scores": {
          "intelligence_score": 13.281894010187271,
          "blended_cost_per_1m": 0.0,
          "latency_seconds": 0.0,
          "tokens_per_second": 0.0,
          "context_window": 128000,
          "hle": 4.7,
          "gpqa": 47.1,
          "scicode": 29.5,
          "livecodebench": 24.1
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 13.281894010187271,
        "hle": 4.7,
        "gpqa": 47.1,
        "scicode": 29.5,
        "livecodebench": 24.1
      }
    },
    "confidence_score": 0.86,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "grok-4-fast",
    "canonical_name": "Grok 4 Fast (Reasoning)",
    "model_name": "Grok 4 Fast (Reasoning)",
    "aliases": [
      "Grok 4 Fast (Non-reasoning)",
      "Grok 4 Fast (Reasoning)",
      "grok-4-fast-chat",
      "grok-4-fast-reasoning"
    ],
    "provider": "xAI",
    "context_window": 2000000,
    "open_source": null,
    "arena_votes": 18440,
    "license_type": "Proprietary",
    "creator": null,
    "intelligence_score": 29.09,
    "coding_score": 23.2,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 1412.7,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": 913.5733,
    "terminalbench_hard": 0.155,
    "tau2": 0.6475,
    "lcr": 42.35,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": 11.0,
    "gpqa": 72.65,
    "scicode": 38.55,
    "ifbench": 44.1,
    "aime25": 65.5,
    "critpt": 1.45,
    "mmmu_pro": 54.95,
    "livecodebench": 61.65,
    "sources": [
      "Artificial Analysis",
      "LMSYS Chatbot Arena"
    ],
    "source_count": 2,
    "data_tier": 1,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.916548+00:00",
        "confidence": 0.79,
        "raw_name": "Grok 4 Fast (Reasoning)",
        "raw_scores": {
          "intelligence_score": 35.06,
          "coding_score": 27.36,
          "context_window": 2000000,
          "gdpval": 1027.5690775263674,
          "terminalbench_hard": 0.189,
          "tau2": 0.658,
          "lcr": 64.7,
          "hle": 17.0,
          "gpqa": 84.7,
          "scicode": 44.2,
          "ifbench": 50.5,
          "aime25": 89.7,
          "critpt": 2.9,
          "mmmu_pro": 61.8,
          "livecodebench": 83.2
        }
      },
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.916633+00:00",
        "confidence": 0.79,
        "raw_name": "Grok 4 Fast (Non-reasoning)",
        "raw_scores": {
          "intelligence_score": 23.12,
          "coding_score": 19.04,
          "context_window": 2000000,
          "gdpval": 799.5774575882319,
          "terminalbench_hard": 0.121,
          "tau2": 0.637,
          "lcr": 20.0,
          "hle": 5.0,
          "gpqa": 60.6,
          "scicode": 32.9,
          "ifbench": 37.7,
          "aime25": 41.3,
          "critpt": 0.0,
          "mmmu_pro": 48.1,
          "livecodebench": 40.1
        }
      },
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.891898+00:00",
        "confidence": 1.0,
        "raw_name": "grok-4-fast-chat",
        "raw_scores": {
          "arena_elo": 1421.81,
          "arena_votes": 6962
        }
      },
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.892216+00:00",
        "confidence": 1.0,
        "raw_name": "grok-4-fast-reasoning",
        "raw_scores": {
          "arena_elo": 1403.59,
          "arena_votes": 18440
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 35.06,
        "coding_score": 27.36,
        "gdpval": 1027.5690775263674,
        "terminalbench_hard": 0.189,
        "tau2": 0.658,
        "lcr": 64.7,
        "hle": 17.0,
        "gpqa": 84.7,
        "scicode": 44.2,
        "ifbench": 50.5,
        "aime25": 89.7,
        "critpt": 2.9,
        "mmmu_pro": 61.8,
        "livecodebench": 83.2
      },
      "LMSYS Chatbot Arena": {
        "arena_elo": 1421.81
      }
    },
    "confidence_score": 0.895,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "grok-3",
    "canonical_name": "Grok 3",
    "model_name": "Grok 3",
    "aliases": [
      "Grok 3",
      "grok-3-preview-02-24"
    ],
    "provider": "xAI",
    "context_window": 1000000,
    "open_source": null,
    "arena_votes": 33843,
    "license_type": "Proprietary",
    "creator": null,
    "intelligence_score": 25.17,
    "coding_score": 19.84,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 1411.33,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": 6.0,
    "latency_seconds": 0.74,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": 71.0,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": 700.2874,
    "terminalbench_hard": 0.114,
    "tau2": 0.488,
    "lcr": 54.7,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": 5.1,
    "gpqa": 69.3,
    "scicode": 36.8,
    "ifbench": 46.9,
    "aime25": 58.0,
    "critpt": 0.0,
    "mmmu_pro": null,
    "livecodebench": 42.5,
    "sources": [
      "Artificial Analysis",
      "LMSYS Chatbot Arena"
    ],
    "source_count": 2,
    "data_tier": 1,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.916592+00:00",
        "confidence": 1.0,
        "raw_name": "Grok 3",
        "raw_scores": {
          "intelligence_score": 25.17,
          "coding_score": 19.84,
          "blended_cost_per_1m": 6.0,
          "latency_seconds": 0.74,
          "tokens_per_second": 71.0,
          "context_window": 1000000,
          "gdpval": 700.2873537921228,
          "terminalbench_hard": 0.114,
          "tau2": 0.488,
          "lcr": 54.7,
          "hle": 5.1,
          "gpqa": 69.3,
          "scicode": 36.8,
          "ifbench": 46.9,
          "aime25": 58.0,
          "critpt": 0.0,
          "livecodebench": 42.5
        }
      },
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.892112+00:00",
        "confidence": 1.0,
        "raw_name": "grok-3-preview-02-24",
        "raw_scores": {
          "arena_elo": 1411.33,
          "arena_votes": 33843
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 25.17,
        "coding_score": 19.84,
        "gdpval": 700.2873537921228,
        "terminalbench_hard": 0.114,
        "tau2": 0.488,
        "lcr": 54.7,
        "hle": 5.1,
        "gpqa": 69.3,
        "scicode": 36.8,
        "ifbench": 46.9,
        "aime25": 58.0,
        "critpt": 0.0,
        "livecodebench": 42.5
      },
      "LMSYS Chatbot Arena": {
        "arena_elo": 1411.33
      }
    },
    "confidence_score": 1.0,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "grok-3-reasoning-beta",
    "canonical_name": "Grok 3 Reasoning Beta",
    "model_name": "Grok 3 Reasoning Beta",
    "aliases": [
      "Grok 3 Reasoning Beta"
    ],
    "provider": "xAI",
    "context_window": 1000000,
    "open_source": null,
    "arena_votes": null,
    "license_type": null,
    "creator": null,
    "intelligence_score": 21.6477,
    "coding_score": null,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": null,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": 0.0,
    "latency_seconds": 0.0,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": 0.0,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": null,
    "tau2": null,
    "lcr": null,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": null,
    "gpqa": null,
    "scicode": null,
    "ifbench": null,
    "aime25": null,
    "critpt": null,
    "mmmu_pro": null,
    "livecodebench": null,
    "sources": [
      "Artificial Analysis"
    ],
    "source_count": 1,
    "data_tier": 2,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.916660+00:00",
        "confidence": 0.65,
        "raw_name": "Grok 3 Reasoning Beta",
        "raw_scores": {
          "intelligence_score": 21.647717992955027,
          "blended_cost_per_1m": 0.0,
          "latency_seconds": 0.0,
          "tokens_per_second": 0.0,
          "context_window": 1000000
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 21.647717992955027
      }
    },
    "confidence_score": 0.65,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "grok-2",
    "canonical_name": "Grok 2 (Dec '24)",
    "model_name": "Grok 2 (Dec '24)",
    "aliases": [
      "Grok 2 (Dec '24)"
    ],
    "provider": "xAI",
    "context_window": 131072,
    "open_source": true,
    "arena_votes": null,
    "license_type": "Grok 2 Community License Agreement",
    "creator": null,
    "intelligence_score": 13.886,
    "coding_score": null,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": null,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": null,
    "tau2": null,
    "lcr": null,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": 3.8,
    "gpqa": 51.0,
    "scicode": 28.5,
    "ifbench": null,
    "aime25": null,
    "critpt": null,
    "mmmu_pro": null,
    "livecodebench": 26.7,
    "sources": [
      "Artificial Analysis"
    ],
    "source_count": 1,
    "data_tier": 2,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.916688+00:00",
        "confidence": 0.65,
        "raw_name": "Grok 2 (Dec '24)",
        "raw_scores": {
          "intelligence_score": 13.886018572918001,
          "context_window": 131072,
          "hle": 3.8,
          "gpqa": 51.0,
          "scicode": 28.5,
          "livecodebench": 26.7
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 13.886018572918001,
        "hle": 3.8,
        "gpqa": 51.0,
        "scicode": 28.5,
        "livecodebench": 26.7
      }
    },
    "confidence_score": 0.65,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "openchat-35",
    "canonical_name": "OpenChat 3.5 (1210)",
    "model_name": "OpenChat 3.5 (1210)",
    "aliases": [
      "OpenChat 3.5 (1210)",
      "openchat-3.5"
    ],
    "provider": "OpenChat",
    "context_window": 8192,
    "open_source": true,
    "arena_votes": 7967,
    "license_type": "Apache 2.0",
    "creator": null,
    "intelligence_score": 8.3203,
    "coding_score": null,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 1182.2,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": null,
    "tau2": null,
    "lcr": null,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": 4.8,
    "gpqa": 23.0,
    "scicode": null,
    "ifbench": null,
    "aime25": null,
    "critpt": null,
    "mmmu_pro": null,
    "livecodebench": 11.5,
    "sources": [
      "Artificial Analysis",
      "LMSYS Chatbot Arena"
    ],
    "source_count": 2,
    "data_tier": 1,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.916714+00:00",
        "confidence": 0.65,
        "raw_name": "OpenChat 3.5 (1210)",
        "raw_scores": {
          "intelligence_score": 8.320282434218926,
          "context_window": 8192,
          "hle": 4.8,
          "gpqa": 23.0,
          "livecodebench": 11.5
        }
      },
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.894789+00:00",
        "confidence": 1.0,
        "raw_name": "openchat-3.5",
        "raw_scores": {
          "arena_elo": 1182.2,
          "arena_votes": 7967
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 8.320282434218926,
        "hle": 4.8,
        "gpqa": 23.0,
        "livecodebench": 11.5
      },
      "LMSYS Chatbot Arena": {
        "arena_elo": 1182.2
      }
    },
    "confidence_score": 0.825,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "nova-pro",
    "canonical_name": "Nova Pro",
    "model_name": "Nova Pro",
    "aliases": [
      "Nova Pro",
      "amazon-nova-pro-v1.0"
    ],
    "provider": "Amazon",
    "context_window": 300000,
    "open_source": null,
    "arena_votes": 24753,
    "license_type": "Proprietary",
    "creator": null,
    "intelligence_score": 14.0153,
    "coding_score": 10.98,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 1290.18,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": 1.4,
    "latency_seconds": 0.0,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": 0.0,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": 432.0639,
    "terminalbench_hard": 0.061,
    "tau2": 0.14,
    "lcr": 19.0,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": 3.4,
    "gpqa": 49.9,
    "scicode": 20.8,
    "ifbench": 38.1,
    "aime25": 7.0,
    "critpt": 0.0,
    "mmmu_pro": 44.3,
    "livecodebench": 23.3,
    "sources": [
      "Artificial Analysis",
      "LMSYS Chatbot Arena"
    ],
    "source_count": 2,
    "data_tier": 1,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.916760+00:00",
        "confidence": 1.0,
        "raw_name": "Nova Pro",
        "raw_scores": {
          "intelligence_score": 14.015305085876673,
          "coding_score": 10.98,
          "blended_cost_per_1m": 1.4,
          "latency_seconds": 0.0,
          "tokens_per_second": 0.0,
          "context_window": 300000,
          "gdpval": 432.0638982067353,
          "terminalbench_hard": 0.061,
          "tau2": 0.14,
          "lcr": 19.0,
          "hle": 3.4,
          "gpqa": 49.9,
          "scicode": 20.8,
          "ifbench": 38.1,
          "aime25": 7.0,
          "critpt": 0.0,
          "mmmu_pro": 44.3,
          "livecodebench": 23.3
        }
      },
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.893949+00:00",
        "confidence": 1.0,
        "raw_name": "amazon-nova-pro-v1.0",
        "raw_scores": {
          "arena_elo": 1290.18,
          "arena_votes": 24753
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 14.015305085876673,
        "coding_score": 10.98,
        "gdpval": 432.0638982067353,
        "terminalbench_hard": 0.061,
        "tau2": 0.14,
        "lcr": 19.0,
        "hle": 3.4,
        "gpqa": 49.9,
        "scicode": 20.8,
        "ifbench": 38.1,
        "aime25": 7.0,
        "critpt": 0.0,
        "mmmu_pro": 44.3,
        "livecodebench": 23.3
      },
      "LMSYS Chatbot Arena": {
        "arena_elo": 1290.18
      }
    },
    "confidence_score": 1.0,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "nova-lite",
    "canonical_name": "Nova Lite",
    "model_name": "Nova Lite",
    "aliases": [
      "Nova Lite",
      "amazon-nova-lite-v1.0"
    ],
    "provider": "Amazon",
    "context_window": 300000,
    "open_source": null,
    "arena_votes": 19376,
    "license_type": "Proprietary",
    "creator": null,
    "intelligence_score": 12.7502,
    "coding_score": 5.13,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 1260.66,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": 0.1,
    "latency_seconds": 0.4,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": 222.0,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": 391.308,
    "terminalbench_hard": 0.008,
    "tau2": 0.175,
    "lcr": 17.7,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": 4.6,
    "gpqa": 43.3,
    "scicode": 13.9,
    "ifbench": 34.1,
    "aime25": 7.0,
    "critpt": 0.0,
    "mmmu_pro": 37.8,
    "livecodebench": 16.7,
    "sources": [
      "Artificial Analysis",
      "LMSYS Chatbot Arena"
    ],
    "source_count": 2,
    "data_tier": 1,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.916808+00:00",
        "confidence": 1.0,
        "raw_name": "Nova Lite",
        "raw_scores": {
          "intelligence_score": 12.750174437772086,
          "coding_score": 5.13,
          "blended_cost_per_1m": 0.1,
          "latency_seconds": 0.4,
          "tokens_per_second": 222.0,
          "context_window": 300000,
          "gdpval": 391.30799404917116,
          "terminalbench_hard": 0.008,
          "tau2": 0.175,
          "lcr": 17.7,
          "hle": 4.6,
          "gpqa": 43.3,
          "scicode": 13.9,
          "ifbench": 34.1,
          "aime25": 7.0,
          "critpt": 0.0,
          "mmmu_pro": 37.8,
          "livecodebench": 16.7
        }
      },
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.894304+00:00",
        "confidence": 1.0,
        "raw_name": "amazon-nova-lite-v1.0",
        "raw_scores": {
          "arena_elo": 1260.66,
          "arena_votes": 19376
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 12.750174437772086,
        "coding_score": 5.13,
        "gdpval": 391.30799404917116,
        "terminalbench_hard": 0.008,
        "tau2": 0.175,
        "lcr": 17.7,
        "hle": 4.6,
        "gpqa": 43.3,
        "scicode": 13.9,
        "ifbench": 34.1,
        "aime25": 7.0,
        "critpt": 0.0,
        "mmmu_pro": 37.8,
        "livecodebench": 16.7
      },
      "LMSYS Chatbot Arena": {
        "arena_elo": 1260.66
      }
    },
    "confidence_score": 1.0,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "phi-3-mini-instruct-38b",
    "canonical_name": "Phi-3 Mini Instruct 3.8B",
    "model_name": "Phi-3 Mini Instruct 3.8B",
    "aliases": [
      "Phi-3 Mini Instruct 3.8B"
    ],
    "provider": "Microsoft",
    "context_window": 4096,
    "open_source": true,
    "arena_votes": null,
    "license_type": "MIT",
    "creator": null,
    "intelligence_score": 10.1008,
    "coding_score": 3.01,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": null,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": 0.0,
    "tau2": 0.0,
    "lcr": 2.0,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": 4.4,
    "gpqa": 31.9,
    "scicode": 9.0,
    "ifbench": 23.9,
    "aime25": 0.3,
    "critpt": null,
    "mmmu_pro": null,
    "livecodebench": 11.6,
    "sources": [
      "Artificial Analysis"
    ],
    "source_count": 1,
    "data_tier": 2,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.916845+00:00",
        "confidence": 0.79,
        "raw_name": "Phi-3 Mini Instruct 3.8B",
        "raw_scores": {
          "intelligence_score": 10.10075277153229,
          "coding_score": 3.01,
          "context_window": 4096,
          "terminalbench_hard": 0.0,
          "tau2": 0.0,
          "lcr": 2.0,
          "hle": 4.4,
          "gpqa": 31.9,
          "scicode": 9.0,
          "ifbench": 23.9,
          "aime25": 0.3,
          "livecodebench": 11.6
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 10.10075277153229,
        "coding_score": 3.01,
        "terminalbench_hard": 0.0,
        "tau2": 0.0,
        "lcr": 2.0,
        "hle": 4.4,
        "gpqa": 31.9,
        "scicode": 9.0,
        "ifbench": 23.9,
        "aime25": 0.3,
        "livecodebench": 11.6
      }
    },
    "confidence_score": 0.79,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "lfm-40b",
    "canonical_name": "LFM 40B",
    "model_name": "LFM 40B",
    "aliases": [
      "LFM 40B"
    ],
    "provider": null,
    "context_window": 32000,
    "open_source": null,
    "arena_votes": null,
    "license_type": null,
    "creator": null,
    "intelligence_score": 8.7608,
    "coding_score": null,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": null,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": 0.0,
    "latency_seconds": 0.0,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": 0.0,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": null,
    "tau2": null,
    "lcr": null,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": 4.9,
    "gpqa": 32.7,
    "scicode": 7.1,
    "ifbench": null,
    "aime25": null,
    "critpt": null,
    "mmmu_pro": null,
    "livecodebench": 9.6,
    "sources": [
      "Artificial Analysis"
    ],
    "source_count": 1,
    "data_tier": 2,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.916880+00:00",
        "confidence": 0.8600000000000001,
        "raw_name": "LFM 40B",
        "raw_scores": {
          "intelligence_score": 8.760765585157351,
          "blended_cost_per_1m": 0.0,
          "latency_seconds": 0.0,
          "tokens_per_second": 0.0,
          "context_window": 32000,
          "hle": 4.9,
          "gpqa": 32.7,
          "scicode": 7.1,
          "livecodebench": 9.6
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 8.760765585157351,
        "hle": 4.9,
        "gpqa": 32.7,
        "scicode": 7.1,
        "livecodebench": 9.6
      }
    },
    "confidence_score": 0.86,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "lfm2-12b",
    "canonical_name": "LFM2 1.2B",
    "model_name": "LFM2 1.2B",
    "aliases": [
      "LFM2 1.2B"
    ],
    "provider": null,
    "context_window": 32768,
    "open_source": true,
    "arena_votes": null,
    "license_type": "lfm 1.0",
    "creator": null,
    "intelligence_score": 9.3288,
    "coding_score": 0.85,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": null,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": 0.0,
    "latency_seconds": 0.0,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": 0.0,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": 336.1011,
    "terminalbench_hard": 0.0,
    "tau2": 0.126,
    "lcr": 0.0,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": 5.7,
    "gpqa": 22.8,
    "scicode": 2.5,
    "ifbench": 22.0,
    "aime25": 3.3,
    "critpt": 0.0,
    "mmmu_pro": null,
    "livecodebench": 2.0,
    "sources": [
      "Artificial Analysis"
    ],
    "source_count": 1,
    "data_tier": 2,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.916922+00:00",
        "confidence": 1.0,
        "raw_name": "LFM2 1.2B",
        "raw_scores": {
          "intelligence_score": 9.328833244924207,
          "coding_score": 0.85,
          "blended_cost_per_1m": 0.0,
          "latency_seconds": 0.0,
          "tokens_per_second": 0.0,
          "context_window": 32768,
          "gdpval": 336.10112809641623,
          "terminalbench_hard": 0.0,
          "tau2": 0.126,
          "lcr": 0.0,
          "hle": 5.7,
          "gpqa": 22.8,
          "scicode": 2.5,
          "ifbench": 22.0,
          "aime25": 3.3,
          "critpt": 0.0,
          "livecodebench": 2.0
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 9.328833244924207,
        "coding_score": 0.85,
        "gdpval": 336.10112809641623,
        "terminalbench_hard": 0.0,
        "tau2": 0.126,
        "lcr": 0.0,
        "hle": 5.7,
        "gpqa": 22.8,
        "scicode": 2.5,
        "ifbench": 22.0,
        "aime25": 3.3,
        "critpt": 0.0,
        "livecodebench": 2.0
      }
    },
    "confidence_score": 1.0,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "solar-mini",
    "canonical_name": "Solar Mini",
    "model_name": "Solar Mini",
    "aliases": [
      "Solar Mini"
    ],
    "provider": null,
    "context_window": 4096,
    "open_source": true,
    "arena_votes": null,
    "license_type": "Apache 2.0",
    "creator": null,
    "intelligence_score": 11.9013,
    "coding_score": null,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": null,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": 0.15,
    "latency_seconds": 1.19,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": 81.0,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": null,
    "tau2": 0.202,
    "lcr": null,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": null,
    "gpqa": null,
    "scicode": null,
    "ifbench": null,
    "aime25": null,
    "critpt": null,
    "mmmu_pro": null,
    "livecodebench": null,
    "sources": [
      "Artificial Analysis"
    ],
    "source_count": 1,
    "data_tier": 2,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.916948+00:00",
        "confidence": 0.65,
        "raw_name": "Solar Mini",
        "raw_scores": {
          "intelligence_score": 11.901298628500898,
          "blended_cost_per_1m": 0.15,
          "latency_seconds": 1.19,
          "tokens_per_second": 81.0,
          "context_window": 4096,
          "tau2": 0.202
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 11.901298628500898,
        "tau2": 0.202
      }
    },
    "confidence_score": 0.65,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "dbrx-instruct",
    "canonical_name": "DBRX Instruct",
    "model_name": "DBRX Instruct",
    "aliases": [
      "DBRX Instruct"
    ],
    "provider": "Databricks",
    "context_window": 32768,
    "open_source": true,
    "arena_votes": null,
    "license_type": "Databricks Open Model License",
    "creator": null,
    "intelligence_score": 8.3159,
    "coding_score": null,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": null,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": null,
    "tau2": null,
    "lcr": null,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": 6.6,
    "gpqa": 33.1,
    "scicode": 11.8,
    "ifbench": null,
    "aime25": null,
    "critpt": null,
    "mmmu_pro": null,
    "livecodebench": 9.3,
    "sources": [
      "Artificial Analysis"
    ],
    "source_count": 1,
    "data_tier": 2,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.917033+00:00",
        "confidence": 0.65,
        "raw_name": "DBRX Instruct",
        "raw_scores": {
          "intelligence_score": 8.315903697714282,
          "context_window": 32768,
          "hle": 6.6,
          "gpqa": 33.1,
          "scicode": 11.8,
          "livecodebench": 9.3
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 8.315903697714282,
        "hle": 6.6,
        "gpqa": 33.1,
        "scicode": 11.8,
        "livecodebench": 9.3
      }
    },
    "confidence_score": 0.65,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "minimax-m21",
    "canonical_name": "MiniMax-M2.1",
    "model_name": "MiniMax-M2.1",
    "aliases": [
      "MiniMax-M2.1"
    ],
    "provider": "MiniMax",
    "context_window": 204800,
    "open_source": true,
    "arena_votes": null,
    "license_type": "MIT",
    "creator": null,
    "intelligence_score": 39.42,
    "coding_score": 32.77,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": null,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": 0.53,
    "latency_seconds": 3.25,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": 54.0,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": 1087.5111,
    "terminalbench_hard": 0.288,
    "tau2": 0.854,
    "lcr": 59.0,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": 22.2,
    "gpqa": 83.0,
    "scicode": 40.7,
    "ifbench": 69.9,
    "aime25": 82.7,
    "critpt": 0.3,
    "mmmu_pro": null,
    "livecodebench": 81.0,
    "sources": [
      "Artificial Analysis"
    ],
    "source_count": 1,
    "data_tier": 2,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.917077+00:00",
        "confidence": 1.0,
        "raw_name": "MiniMax-M2.1",
        "raw_scores": {
          "intelligence_score": 39.42,
          "coding_score": 32.77,
          "blended_cost_per_1m": 0.53,
          "latency_seconds": 3.25,
          "tokens_per_second": 54.0,
          "context_window": 204800,
          "gdpval": 1087.5111305011965,
          "terminalbench_hard": 0.288,
          "tau2": 0.854,
          "lcr": 59.0,
          "hle": 22.2,
          "gpqa": 83.0,
          "scicode": 40.7,
          "ifbench": 69.9,
          "aime25": 82.7,
          "critpt": 0.3,
          "livecodebench": 81.0
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 39.42,
        "coding_score": 32.77,
        "gdpval": 1087.5111305011965,
        "terminalbench_hard": 0.288,
        "tau2": 0.854,
        "lcr": 59.0,
        "hle": 22.2,
        "gpqa": 83.0,
        "scicode": 40.7,
        "ifbench": 69.9,
        "aime25": 82.7,
        "critpt": 0.3,
        "livecodebench": 81.0
      }
    },
    "confidence_score": 1.0,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "minimax-m2",
    "canonical_name": "MiniMax-M2",
    "model_name": "MiniMax-M2",
    "aliases": [
      "MiniMax-M2",
      "minimax-m2"
    ],
    "provider": "MiniMax",
    "context_window": 204800,
    "open_source": true,
    "arena_votes": 6688,
    "license_type": "MIT",
    "creator": null,
    "intelligence_score": 36.09,
    "coding_score": 29.21,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 1346.97,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": 0.53,
    "latency_seconds": 3.55,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": 52.0,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": 1052.4054,
    "terminalbench_hard": 0.258,
    "tau2": 0.868,
    "lcr": 61.0,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": 12.5,
    "gpqa": 77.7,
    "scicode": 36.1,
    "ifbench": 72.3,
    "aime25": 78.3,
    "critpt": 0.9,
    "mmmu_pro": null,
    "livecodebench": 82.6,
    "sources": [
      "Artificial Analysis",
      "LMSYS Chatbot Arena"
    ],
    "source_count": 2,
    "data_tier": 1,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.917120+00:00",
        "confidence": 1.0,
        "raw_name": "MiniMax-M2",
        "raw_scores": {
          "intelligence_score": 36.09,
          "coding_score": 29.21,
          "blended_cost_per_1m": 0.53,
          "latency_seconds": 3.55,
          "tokens_per_second": 52.0,
          "context_window": 204800,
          "gdpval": 1052.4053907915022,
          "terminalbench_hard": 0.258,
          "tau2": 0.868,
          "lcr": 61.0,
          "hle": 12.5,
          "gpqa": 77.7,
          "scicode": 36.1,
          "ifbench": 72.3,
          "aime25": 78.3,
          "critpt": 0.9,
          "livecodebench": 82.6
        }
      },
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.893121+00:00",
        "confidence": 1.0,
        "raw_name": "minimax-m2",
        "raw_scores": {
          "arena_elo": 1346.97,
          "arena_votes": 6688
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 36.09,
        "coding_score": 29.21,
        "gdpval": 1052.4053907915022,
        "terminalbench_hard": 0.258,
        "tau2": 0.868,
        "lcr": 61.0,
        "hle": 12.5,
        "gpqa": 77.7,
        "scicode": 36.1,
        "ifbench": 72.3,
        "aime25": 78.3,
        "critpt": 0.9,
        "livecodebench": 82.6
      },
      "LMSYS Chatbot Arena": {
        "arena_elo": 1346.97
      }
    },
    "confidence_score": 1.0,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "minimax-m1-80k",
    "canonical_name": "MiniMax M1 80k",
    "model_name": "MiniMax M1 80k",
    "aliases": [
      "MiniMax M1 80k"
    ],
    "provider": "MiniMax",
    "context_window": 1000000,
    "open_source": true,
    "arena_votes": null,
    "license_type": "Apache 2.0",
    "creator": null,
    "intelligence_score": 24.43,
    "coding_score": 14.48,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": null,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": 0.96,
    "latency_seconds": 0.0,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": 0.0,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": 1026.0807,
    "terminalbench_hard": 0.03,
    "tau2": 0.342,
    "lcr": 54.3,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": 8.2,
    "gpqa": 69.7,
    "scicode": 37.4,
    "ifbench": 41.8,
    "aime25": 61.0,
    "critpt": 0.0,
    "mmmu_pro": null,
    "livecodebench": 71.1,
    "sources": [
      "Artificial Analysis"
    ],
    "source_count": 1,
    "data_tier": 2,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.917164+00:00",
        "confidence": 1.0,
        "raw_name": "MiniMax M1 80k",
        "raw_scores": {
          "intelligence_score": 24.43,
          "coding_score": 14.48,
          "blended_cost_per_1m": 0.96,
          "latency_seconds": 0.0,
          "tokens_per_second": 0.0,
          "context_window": 1000000,
          "gdpval": 1026.0806795440035,
          "terminalbench_hard": 0.03,
          "tau2": 0.342,
          "lcr": 54.3,
          "hle": 8.2,
          "gpqa": 69.7,
          "scicode": 37.4,
          "ifbench": 41.8,
          "aime25": 61.0,
          "critpt": 0.0,
          "livecodebench": 71.1
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 24.43,
        "coding_score": 14.48,
        "gdpval": 1026.0806795440035,
        "terminalbench_hard": 0.03,
        "tau2": 0.342,
        "lcr": 54.3,
        "hle": 8.2,
        "gpqa": 69.7,
        "scicode": 37.4,
        "ifbench": 41.8,
        "aime25": 61.0,
        "critpt": 0.0,
        "livecodebench": 71.1
      }
    },
    "confidence_score": 1.0,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "minimax-m1-40k",
    "canonical_name": "MiniMax M1 40k",
    "model_name": "MiniMax M1 40k",
    "aliases": [
      "MiniMax M1 40k"
    ],
    "provider": "MiniMax",
    "context_window": 1000000,
    "open_source": true,
    "arena_votes": null,
    "license_type": "Apache 2.0",
    "creator": null,
    "intelligence_score": 20.8562,
    "coding_score": 14.13,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": null,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": 0.0,
    "latency_seconds": 0.0,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": 0.0,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": 0.023,
    "tau2": 0.316,
    "lcr": 51.7,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": 7.5,
    "gpqa": 68.2,
    "scicode": 37.8,
    "ifbench": 41.2,
    "aime25": 13.7,
    "critpt": null,
    "mmmu_pro": null,
    "livecodebench": 65.7,
    "sources": [
      "Artificial Analysis"
    ],
    "source_count": 1,
    "data_tier": 2,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.917222+00:00",
        "confidence": 1.0,
        "raw_name": "MiniMax M1 40k",
        "raw_scores": {
          "intelligence_score": 20.856172610675117,
          "coding_score": 14.13,
          "blended_cost_per_1m": 0.0,
          "latency_seconds": 0.0,
          "tokens_per_second": 0.0,
          "context_window": 1000000,
          "terminalbench_hard": 0.023,
          "tau2": 0.316,
          "lcr": 51.7,
          "hle": 7.5,
          "gpqa": 68.2,
          "scicode": 37.8,
          "ifbench": 41.2,
          "aime25": 13.7,
          "livecodebench": 65.7
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 20.856172610675117,
        "coding_score": 14.13,
        "terminalbench_hard": 0.023,
        "tau2": 0.316,
        "lcr": 51.7,
        "hle": 7.5,
        "gpqa": 68.2,
        "scicode": 37.8,
        "ifbench": 41.2,
        "aime25": 13.7,
        "livecodebench": 65.7
      }
    },
    "confidence_score": 1.0,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "kimi-k2",
    "canonical_name": "Kimi K2",
    "model_name": "Kimi K2",
    "aliases": [
      "Kimi K2",
      "kimi-k2-0711-preview",
      "kimi-k2-0905-preview"
    ],
    "provider": "Moonshot",
    "context_window": 128000,
    "open_source": true,
    "arena_votes": 28440,
    "license_type": "Modified MIT License",
    "creator": null,
    "intelligence_score": 26.32,
    "coding_score": 22.1,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 1417.0,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": 1.07,
    "latency_seconds": 0.59,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": 42.0,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": 564.1967,
    "terminalbench_hard": 0.159,
    "tau2": 0.611,
    "lcr": 51.0,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": 7.0,
    "gpqa": 76.6,
    "scicode": 34.5,
    "ifbench": 41.5,
    "aime25": 57.0,
    "critpt": 0.0,
    "mmmu_pro": null,
    "livecodebench": 55.6,
    "sources": [
      "Artificial Analysis",
      "LMSYS Chatbot Arena"
    ],
    "source_count": 2,
    "data_tier": 1,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.917266+00:00",
        "confidence": 1.0,
        "raw_name": "Kimi K2",
        "raw_scores": {
          "intelligence_score": 26.32,
          "coding_score": 22.1,
          "blended_cost_per_1m": 1.07,
          "latency_seconds": 0.59,
          "tokens_per_second": 42.0,
          "context_window": 128000,
          "gdpval": 564.196694679031,
          "terminalbench_hard": 0.159,
          "tau2": 0.611,
          "lcr": 51.0,
          "hle": 7.0,
          "gpqa": 76.6,
          "scicode": 34.5,
          "ifbench": 41.5,
          "aime25": 57.0,
          "critpt": 0.0,
          "livecodebench": 55.6
        }
      },
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.891987+00:00",
        "confidence": 1.0,
        "raw_name": "kimi-k2-0905-preview",
        "raw_scores": {
          "arena_elo": 1417.2,
          "arena_votes": 11912
        }
      },
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.891999+00:00",
        "confidence": 1.0,
        "raw_name": "kimi-k2-0711-preview",
        "raw_scores": {
          "arena_elo": 1416.8,
          "arena_votes": 28440
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 26.32,
        "coding_score": 22.1,
        "gdpval": 564.196694679031,
        "terminalbench_hard": 0.159,
        "tau2": 0.611,
        "lcr": 51.0,
        "hle": 7.0,
        "gpqa": 76.6,
        "scicode": 34.5,
        "ifbench": 41.5,
        "aime25": 57.0,
        "critpt": 0.0,
        "livecodebench": 55.6
      },
      "LMSYS Chatbot Arena": {
        "arena_elo": 1417.2
      }
    },
    "confidence_score": 1.0,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "kimi-k2-0905",
    "canonical_name": "Kimi K2 0905",
    "model_name": "Kimi K2 0905",
    "aliases": [
      "Kimi K2 0905"
    ],
    "provider": "Moonshot",
    "context_window": 256000,
    "open_source": true,
    "arena_votes": null,
    "license_type": "Modified MIT License",
    "creator": null,
    "intelligence_score": 30.85,
    "coding_score": 25.88,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": null,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": 1.2,
    "latency_seconds": 0.44,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": 64.0,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": 886.7278,
    "terminalbench_hard": 0.235,
    "tau2": 0.734,
    "lcr": 52.3,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": 6.3,
    "gpqa": 76.7,
    "scicode": 30.7,
    "ifbench": 41.7,
    "aime25": 57.3,
    "critpt": 0.0,
    "mmmu_pro": null,
    "livecodebench": 61.0,
    "sources": [
      "Artificial Analysis"
    ],
    "source_count": 1,
    "data_tier": 2,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.917310+00:00",
        "confidence": 1.0,
        "raw_name": "Kimi K2 0905",
        "raw_scores": {
          "intelligence_score": 30.85,
          "coding_score": 25.88,
          "blended_cost_per_1m": 1.2,
          "latency_seconds": 0.44,
          "tokens_per_second": 64.0,
          "context_window": 256000,
          "gdpval": 886.7277942857045,
          "terminalbench_hard": 0.235,
          "tau2": 0.734,
          "lcr": 52.3,
          "hle": 6.3,
          "gpqa": 76.7,
          "scicode": 30.7,
          "ifbench": 41.7,
          "aime25": 57.3,
          "critpt": 0.0,
          "livecodebench": 61.0
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 30.85,
        "coding_score": 25.88,
        "gdpval": 886.7277942857045,
        "terminalbench_hard": 0.235,
        "tau2": 0.734,
        "lcr": 52.3,
        "hle": 6.3,
        "gpqa": 76.7,
        "scicode": 30.7,
        "ifbench": 41.7,
        "aime25": 57.3,
        "critpt": 0.0,
        "livecodebench": 61.0
      }
    },
    "confidence_score": 1.0,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "kimi-k2-thinking",
    "canonical_name": "Kimi K2 Thinking",
    "model_name": "Kimi K2 Thinking",
    "aliases": [
      "Kimi K2 Thinking"
    ],
    "provider": "Moonshot",
    "context_window": 256000,
    "open_source": true,
    "arena_votes": null,
    "license_type": "Modified MIT License",
    "creator": null,
    "intelligence_score": 40.89,
    "coding_score": 34.83,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": null,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": 1.07,
    "latency_seconds": 0.64,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": 66.0,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": 1008.1983,
    "terminalbench_hard": 0.311,
    "tau2": 0.93,
    "lcr": 66.3,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": 22.3,
    "gpqa": 83.8,
    "scicode": 42.4,
    "ifbench": 68.1,
    "aime25": 94.7,
    "critpt": 2.6,
    "mmmu_pro": null,
    "livecodebench": 85.3,
    "sources": [
      "Artificial Analysis"
    ],
    "source_count": 1,
    "data_tier": 2,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.917354+00:00",
        "confidence": 1.0,
        "raw_name": "Kimi K2 Thinking",
        "raw_scores": {
          "intelligence_score": 40.89,
          "coding_score": 34.83,
          "blended_cost_per_1m": 1.07,
          "latency_seconds": 0.64,
          "tokens_per_second": 66.0,
          "context_window": 256000,
          "gdpval": 1008.1983118192898,
          "terminalbench_hard": 0.311,
          "tau2": 0.93,
          "lcr": 66.3,
          "hle": 22.3,
          "gpqa": 83.8,
          "scicode": 42.4,
          "ifbench": 68.1,
          "aime25": 94.7,
          "critpt": 2.6,
          "livecodebench": 85.3
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 40.89,
        "coding_score": 34.83,
        "gdpval": 1008.1983118192898,
        "terminalbench_hard": 0.311,
        "tau2": 0.93,
        "lcr": 66.3,
        "hle": 22.3,
        "gpqa": 83.8,
        "scicode": 42.4,
        "ifbench": 68.1,
        "aime25": 94.7,
        "critpt": 2.6,
        "livecodebench": 85.3
      }
    },
    "confidence_score": 1.0,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "llama-31-tulu3-405b",
    "canonical_name": "Llama 3.1 Tulu3 405B",
    "model_name": "Llama 3.1 Tulu3 405B",
    "aliases": [
      "Llama 3.1 Tulu3 405B"
    ],
    "provider": "Meta",
    "context_window": 128000,
    "open_source": true,
    "arena_votes": null,
    "license_type": "LLAMA 3.1 COMMUNITY LICENSE AGREEMENT",
    "creator": null,
    "intelligence_score": 14.1405,
    "coding_score": null,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": null,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": null,
    "tau2": null,
    "lcr": null,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": 3.5,
    "gpqa": 51.6,
    "scicode": 30.2,
    "ifbench": null,
    "aime25": null,
    "critpt": null,
    "mmmu_pro": null,
    "livecodebench": 29.1,
    "sources": [
      "Artificial Analysis"
    ],
    "source_count": 1,
    "data_tier": 2,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.917384+00:00",
        "confidence": 0.65,
        "raw_name": "Llama 3.1 Tulu3 405B",
        "raw_scores": {
          "intelligence_score": 14.140503152934265,
          "context_window": 128000,
          "hle": 3.5,
          "gpqa": 51.6,
          "scicode": 30.2,
          "livecodebench": 29.1
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 14.140503152934265,
        "hle": 3.5,
        "gpqa": 51.6,
        "scicode": 30.2,
        "livecodebench": 29.1
      }
    },
    "confidence_score": 0.65,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "olmo-3-32b-think",
    "canonical_name": "Olmo 3 32B Think",
    "model_name": "Olmo 3 32B Think",
    "aliases": [
      "Olmo 3 32B Think",
      "olmo-3-32b-think"
    ],
    "provider": "Ai2",
    "context_window": 65536,
    "open_source": true,
    "arena_votes": 5868,
    "license_type": "Apache 2.0",
    "creator": null,
    "intelligence_score": 18.8882,
    "coding_score": 10.54,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 1305.64,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": 0.0,
    "latency_seconds": 0.0,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": 0.0,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": 0.015,
    "tau2": 0.0,
    "lcr": 0.0,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": 5.9,
    "gpqa": 61.0,
    "scicode": 28.6,
    "ifbench": 49.1,
    "aime25": 73.7,
    "critpt": 0.0,
    "mmmu_pro": null,
    "livecodebench": 67.2,
    "sources": [
      "Artificial Analysis",
      "LMSYS Chatbot Arena"
    ],
    "source_count": 2,
    "data_tier": 1,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.917424+00:00",
        "confidence": 1.0,
        "raw_name": "Olmo 3 32B Think",
        "raw_scores": {
          "intelligence_score": 18.88824843779319,
          "coding_score": 10.54,
          "blended_cost_per_1m": 0.0,
          "latency_seconds": 0.0,
          "tokens_per_second": 0.0,
          "context_window": 65536,
          "terminalbench_hard": 0.015,
          "tau2": 0.0,
          "lcr": 0.0,
          "hle": 5.9,
          "gpqa": 61.0,
          "scicode": 28.6,
          "ifbench": 49.1,
          "aime25": 73.7,
          "critpt": 0.0,
          "livecodebench": 67.2
        }
      },
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.893836+00:00",
        "confidence": 1.0,
        "raw_name": "olmo-3-32b-think",
        "raw_scores": {
          "arena_elo": 1305.64,
          "arena_votes": 5868
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 18.88824843779319,
        "coding_score": 10.54,
        "terminalbench_hard": 0.015,
        "tau2": 0.0,
        "lcr": 0.0,
        "hle": 5.9,
        "gpqa": 61.0,
        "scicode": 28.6,
        "ifbench": 49.1,
        "aime25": 73.7,
        "critpt": 0.0,
        "livecodebench": 67.2
      },
      "LMSYS Chatbot Arena": {
        "arena_elo": 1305.64
      }
    },
    "confidence_score": 1.0,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "olmo-2-7b",
    "canonical_name": "OLMo 2 7B",
    "model_name": "OLMo 2 7B",
    "aliases": [
      "OLMo 2 7B"
    ],
    "provider": null,
    "context_window": 4096,
    "open_source": true,
    "arena_votes": null,
    "license_type": "Apache 2.0",
    "creator": null,
    "intelligence_score": 9.2968,
    "coding_score": 1.23,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": null,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": 0.0,
    "latency_seconds": 0.0,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": 0.0,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": 0.0,
    "tau2": 0.0,
    "lcr": 0.0,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": 5.5,
    "gpqa": 28.8,
    "scicode": 3.7,
    "ifbench": 24.4,
    "aime25": 0.7,
    "critpt": null,
    "mmmu_pro": null,
    "livecodebench": 4.1,
    "sources": [
      "Artificial Analysis"
    ],
    "source_count": 1,
    "data_tier": 2,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.917464+00:00",
        "confidence": 1.0,
        "raw_name": "OLMo 2 7B",
        "raw_scores": {
          "intelligence_score": 9.296750708294477,
          "coding_score": 1.23,
          "blended_cost_per_1m": 0.0,
          "latency_seconds": 0.0,
          "tokens_per_second": 0.0,
          "context_window": 4096,
          "terminalbench_hard": 0.0,
          "tau2": 0.0,
          "lcr": 0.0,
          "hle": 5.5,
          "gpqa": 28.8,
          "scicode": 3.7,
          "ifbench": 24.4,
          "aime25": 0.7,
          "livecodebench": 4.1
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 9.296750708294477,
        "coding_score": 1.23,
        "terminalbench_hard": 0.0,
        "tau2": 0.0,
        "lcr": 0.0,
        "hle": 5.5,
        "gpqa": 28.8,
        "scicode": 3.7,
        "ifbench": 24.4,
        "aime25": 0.7,
        "livecodebench": 4.1
      }
    },
    "confidence_score": 1.0,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "olmo-2-32b",
    "canonical_name": "OLMo 2 32B",
    "model_name": "OLMo 2 32B",
    "aliases": [
      "OLMo 2 32B"
    ],
    "provider": null,
    "context_window": 4096,
    "open_source": true,
    "arena_votes": null,
    "license_type": "Apache 2.0",
    "creator": null,
    "intelligence_score": 10.5662,
    "coding_score": 2.66,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": null,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": 0.0,
    "latency_seconds": 0.0,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": 0.0,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": 0.0,
    "tau2": 0.0,
    "lcr": 0.0,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": 3.7,
    "gpqa": 32.8,
    "scicode": 8.0,
    "ifbench": 38.1,
    "aime25": 3.3,
    "critpt": null,
    "mmmu_pro": null,
    "livecodebench": 6.8,
    "sources": [
      "Artificial Analysis"
    ],
    "source_count": 1,
    "data_tier": 2,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.917502+00:00",
        "confidence": 1.0,
        "raw_name": "OLMo 2 32B",
        "raw_scores": {
          "intelligence_score": 10.566197101720523,
          "coding_score": 2.66,
          "blended_cost_per_1m": 0.0,
          "latency_seconds": 0.0,
          "tokens_per_second": 0.0,
          "context_window": 4096,
          "terminalbench_hard": 0.0,
          "tau2": 0.0,
          "lcr": 0.0,
          "hle": 3.7,
          "gpqa": 32.8,
          "scicode": 8.0,
          "ifbench": 38.1,
          "aime25": 3.3,
          "livecodebench": 6.8
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 10.566197101720523,
        "coding_score": 2.66,
        "terminalbench_hard": 0.0,
        "tau2": 0.0,
        "lcr": 0.0,
        "hle": 3.7,
        "gpqa": 32.8,
        "scicode": 8.0,
        "ifbench": 38.1,
        "aime25": 3.3,
        "livecodebench": 6.8
      }
    },
    "confidence_score": 1.0,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "granite-33-8b",
    "canonical_name": "Granite 3.3 8B (Non-reasoning)",
    "model_name": "Granite 3.3 8B (Non-reasoning)",
    "aliases": [
      "Granite 3.3 8B (Non-reasoning)"
    ],
    "provider": null,
    "context_window": 128000,
    "open_source": true,
    "arena_votes": null,
    "license_type": "Apache 2.0",
    "creator": null,
    "intelligence_score": 10.7897,
    "coding_score": 3.36,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": null,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": 273.7587,
    "terminalbench_hard": 0.0,
    "tau2": 0.105,
    "lcr": 4.3,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": 4.2,
    "gpqa": 33.8,
    "scicode": 10.1,
    "ifbench": 22.4,
    "aime25": 6.7,
    "critpt": 0.0,
    "mmmu_pro": null,
    "livecodebench": 12.7,
    "sources": [
      "Artificial Analysis"
    ],
    "source_count": 1,
    "data_tier": 2,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.917545+00:00",
        "confidence": 0.79,
        "raw_name": "Granite 3.3 8B (Non-reasoning)",
        "raw_scores": {
          "intelligence_score": 10.789732126843495,
          "coding_score": 3.36,
          "context_window": 128000,
          "gdpval": 273.75868668203884,
          "terminalbench_hard": 0.0,
          "tau2": 0.105,
          "lcr": 4.3,
          "hle": 4.2,
          "gpqa": 33.8,
          "scicode": 10.1,
          "ifbench": 22.4,
          "aime25": 6.7,
          "critpt": 0.0,
          "livecodebench": 12.7
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 10.789732126843495,
        "coding_score": 3.36,
        "gdpval": 273.75868668203884,
        "terminalbench_hard": 0.0,
        "tau2": 0.105,
        "lcr": 4.3,
        "hle": 4.2,
        "gpqa": 33.8,
        "scicode": 10.1,
        "ifbench": 22.4,
        "aime25": 6.7,
        "critpt": 0.0,
        "livecodebench": 12.7
      }
    },
    "confidence_score": 0.79,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "reka-flash",
    "canonical_name": "Reka Flash (Sep '24)",
    "model_name": "Reka Flash (Sep '24)",
    "aliases": [
      "Reka Flash (Sep '24)"
    ],
    "provider": null,
    "context_window": 128000,
    "open_source": null,
    "arena_votes": null,
    "license_type": null,
    "creator": null,
    "intelligence_score": 11.9673,
    "coding_score": null,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": null,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": null,
    "tau2": null,
    "lcr": null,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": null,
    "gpqa": null,
    "scicode": null,
    "ifbench": null,
    "aime25": null,
    "critpt": null,
    "mmmu_pro": null,
    "livecodebench": null,
    "sources": [
      "Artificial Analysis"
    ],
    "source_count": 1,
    "data_tier": 2,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.917565+00:00",
        "confidence": 0.44,
        "raw_name": "Reka Flash (Sep '24)",
        "raw_scores": {
          "intelligence_score": 11.967262408951813,
          "context_window": 128000
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 11.967262408951813
      }
    },
    "confidence_score": 0.44,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "hermes-3---llama-31-70b",
    "canonical_name": "Hermes 3 - Llama-3.1 70B",
    "model_name": "Hermes 3 - Llama-3.1 70B",
    "aliases": [
      "Hermes 3 - Llama-3.1 70B"
    ],
    "provider": "Meta",
    "context_window": 128000,
    "open_source": true,
    "arena_votes": null,
    "license_type": "LLAMA 3.1 COMMUNITY LICENSE AGREEMENT",
    "creator": null,
    "intelligence_score": 10.6474,
    "coding_score": null,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": null,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": 0.3,
    "latency_seconds": 0.52,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": 33.0,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": null,
    "tau2": null,
    "lcr": null,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": 4.1,
    "gpqa": 40.1,
    "scicode": 23.1,
    "ifbench": null,
    "aime25": null,
    "critpt": null,
    "mmmu_pro": null,
    "livecodebench": 18.8,
    "sources": [
      "Artificial Analysis"
    ],
    "source_count": 1,
    "data_tier": 2,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.917599+00:00",
        "confidence": 0.8600000000000001,
        "raw_name": "Hermes 3 - Llama-3.1 70B",
        "raw_scores": {
          "intelligence_score": 10.647383158323558,
          "blended_cost_per_1m": 0.3,
          "latency_seconds": 0.52,
          "tokens_per_second": 33.0,
          "context_window": 128000,
          "hle": 4.1,
          "gpqa": 40.1,
          "scicode": 23.1,
          "livecodebench": 18.8
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 10.647383158323558,
        "hle": 4.1,
        "gpqa": 40.1,
        "scicode": 23.1,
        "livecodebench": 18.8
      }
    },
    "confidence_score": 0.86,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "glm-47",
    "canonical_name": "GLM-4.7 (Reasoning)",
    "model_name": "GLM-4.7 (Reasoning)",
    "aliases": [
      "GLM-4.7 (Non-reasoning)",
      "GLM-4.7 (Reasoning)",
      "glm-4.7"
    ],
    "provider": "Z.ai",
    "context_window": 200000,
    "open_source": true,
    "arena_votes": 11937,
    "license_type": "MIT",
    "creator": null,
    "intelligence_score": 38.135,
    "coding_score": 34.135,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 1440.77,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": 1197.5172,
    "terminalbench_hard": 0.3105,
    "tau2": 0.9505,
    "lcr": 50.15,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": 15.6,
    "gpqa": 76.15,
    "scicode": 40.25,
    "ifbench": 61.25,
    "aime25": 71.5,
    "critpt": 0.85,
    "mmmu_pro": null,
    "livecodebench": 72.8,
    "sources": [
      "Artificial Analysis",
      "LMSYS Chatbot Arena"
    ],
    "source_count": 2,
    "data_tier": 1,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.917680+00:00",
        "confidence": 0.79,
        "raw_name": "GLM-4.7 (Reasoning)",
        "raw_scores": {
          "intelligence_score": 42.11,
          "coding_score": 36.26,
          "context_window": 200000,
          "gdpval": 1199.901739406455,
          "terminalbench_hard": 0.318,
          "tau2": 0.959,
          "lcr": 64.0,
          "hle": 25.1,
          "gpqa": 85.9,
          "scicode": 45.1,
          "ifbench": 67.9,
          "aime25": 95.0,
          "critpt": 1.7,
          "livecodebench": 89.4
        }
      },
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.917719+00:00",
        "confidence": 0.79,
        "raw_name": "GLM-4.7 (Non-reasoning)",
        "raw_scores": {
          "intelligence_score": 34.16,
          "coding_score": 32.01,
          "context_window": 200000,
          "gdpval": 1195.132611233983,
          "terminalbench_hard": 0.303,
          "tau2": 0.942,
          "lcr": 36.3,
          "hle": 6.1,
          "gpqa": 66.4,
          "scicode": 35.4,
          "ifbench": 54.6,
          "aime25": 48.0,
          "critpt": 0.0,
          "livecodebench": 56.2
        }
      },
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.891672+00:00",
        "confidence": 1.0,
        "raw_name": "glm-4.7",
        "raw_scores": {
          "arena_elo": 1440.77,
          "arena_votes": 11937
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 42.11,
        "coding_score": 36.26,
        "gdpval": 1199.901739406455,
        "terminalbench_hard": 0.318,
        "tau2": 0.959,
        "lcr": 64.0,
        "hle": 25.1,
        "gpqa": 85.9,
        "scicode": 45.1,
        "ifbench": 67.9,
        "aime25": 95.0,
        "critpt": 1.7,
        "livecodebench": 89.4
      },
      "LMSYS Chatbot Arena": {
        "arena_elo": 1440.77
      }
    },
    "confidence_score": 0.86,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "glm-46",
    "canonical_name": "GLM-4.6 (Non-reasoning)",
    "model_name": "GLM-4.6 (Non-reasoning)",
    "aliases": [
      "GLM-4.6 (Non-reasoning)",
      "GLM-4.6 (Reasoning)",
      "glm-4.6"
    ],
    "provider": "Z.ai",
    "context_window": 200000,
    "open_source": true,
    "arena_votes": 35128,
    "license_type": "MIT",
    "creator": null,
    "intelligence_score": 31.375,
    "coding_score": 29.855,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 1425.04,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": 1027.0941,
    "terminalbench_hard": 0.269,
    "tau2": 0.737,
    "lcr": 40.3,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": 9.25,
    "gpqa": 70.6,
    "scicode": 35.75,
    "ifbench": 40.05,
    "aime25": 65.15,
    "critpt": 0.55,
    "mmmu_pro": null,
    "livecodebench": 62.8,
    "sources": [
      "Artificial Analysis",
      "LMSYS Chatbot Arena"
    ],
    "source_count": 2,
    "data_tier": 1,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.917759+00:00",
        "confidence": 0.79,
        "raw_name": "GLM-4.6 (Non-reasoning)",
        "raw_scores": {
          "intelligence_score": 30.24,
          "coding_score": 30.23,
          "context_window": 200000,
          "gdpval": 1008.8076332057576,
          "terminalbench_hard": 0.288,
          "tau2": 0.769,
          "lcr": 26.3,
          "hle": 5.2,
          "gpqa": 63.2,
          "scicode": 33.1,
          "ifbench": 36.7,
          "aime25": 44.3,
          "critpt": 0.0,
          "livecodebench": 56.1
        }
      },
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.917798+00:00",
        "confidence": 0.79,
        "raw_name": "GLM-4.6 (Reasoning)",
        "raw_scores": {
          "intelligence_score": 32.51,
          "coding_score": 29.48,
          "context_window": 200000,
          "gdpval": 1045.3806263312604,
          "terminalbench_hard": 0.25,
          "tau2": 0.705,
          "lcr": 54.3,
          "hle": 13.3,
          "gpqa": 78.0,
          "scicode": 38.4,
          "ifbench": 43.4,
          "aime25": 86.0,
          "critpt": 1.1,
          "livecodebench": 69.5
        }
      },
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.891821+00:00",
        "confidence": 1.0,
        "raw_name": "glm-4.6",
        "raw_scores": {
          "arena_elo": 1425.04,
          "arena_votes": 35128
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 30.24,
        "coding_score": 30.23,
        "gdpval": 1008.8076332057576,
        "terminalbench_hard": 0.288,
        "tau2": 0.769,
        "lcr": 26.3,
        "hle": 5.2,
        "gpqa": 63.2,
        "scicode": 33.1,
        "ifbench": 36.7,
        "aime25": 44.3,
        "critpt": 0.0,
        "livecodebench": 56.1
      },
      "LMSYS Chatbot Arena": {
        "arena_elo": 1425.04
      }
    },
    "confidence_score": 0.86,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "glm-45-air",
    "canonical_name": "GLM-4.5-Air",
    "model_name": "GLM-4.5-Air",
    "aliases": [
      "GLM-4.5-Air",
      "glm-4.5-air"
    ],
    "provider": "Z.ai",
    "context_window": 128000,
    "open_source": true,
    "arena_votes": 31154,
    "license_type": "MIT",
    "creator": null,
    "intelligence_score": 26.3198,
    "coding_score": 23.82,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 1371.75,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": 0.42,
    "latency_seconds": 0.6,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": 121.0,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": 604.4271,
    "terminalbench_hard": 0.205,
    "tau2": 0.465,
    "lcr": 43.7,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": 6.8,
    "gpqa": 73.3,
    "scicode": 30.6,
    "ifbench": 37.6,
    "aime25": 80.7,
    "critpt": 0.0,
    "mmmu_pro": null,
    "livecodebench": 68.4,
    "sources": [
      "Artificial Analysis",
      "LMSYS Chatbot Arena"
    ],
    "source_count": 2,
    "data_tier": 1,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.917842+00:00",
        "confidence": 1.0,
        "raw_name": "GLM-4.5-Air",
        "raw_scores": {
          "intelligence_score": 26.31975907960396,
          "coding_score": 23.82,
          "blended_cost_per_1m": 0.42,
          "latency_seconds": 0.6,
          "tokens_per_second": 121.0,
          "context_window": 128000,
          "gdpval": 604.427098569423,
          "terminalbench_hard": 0.205,
          "tau2": 0.465,
          "lcr": 43.7,
          "hle": 6.8,
          "gpqa": 73.3,
          "scicode": 30.6,
          "ifbench": 37.6,
          "aime25": 80.7,
          "critpt": 0.0,
          "livecodebench": 68.4
        }
      },
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.892798+00:00",
        "confidence": 1.0,
        "raw_name": "glm-4.5-air",
        "raw_scores": {
          "arena_elo": 1371.75,
          "arena_votes": 31154
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 26.31975907960396,
        "coding_score": 23.82,
        "gdpval": 604.427098569423,
        "terminalbench_hard": 0.205,
        "tau2": 0.465,
        "lcr": 43.7,
        "hle": 6.8,
        "gpqa": 73.3,
        "scicode": 30.6,
        "ifbench": 37.6,
        "aime25": 80.7,
        "critpt": 0.0,
        "livecodebench": 68.4
      },
      "LMSYS Chatbot Arena": {
        "arena_elo": 1371.75
      }
    },
    "confidence_score": 1.0,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "glm-45v",
    "canonical_name": "GLM-4.5V (Reasoning)",
    "model_name": "GLM-4.5V (Reasoning)",
    "aliases": [
      "GLM-4.5V (Non-reasoning)",
      "GLM-4.5V (Reasoning)",
      "glm-4.5v"
    ],
    "provider": "Z.ai",
    "context_window": 64000,
    "open_source": true,
    "arena_votes": 4950,
    "license_type": "MIT",
    "creator": null,
    "intelligence_score": 16.8219,
    "coding_score": 10.85,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 1353.13,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": 532.8761,
    "terminalbench_hard": 0.0605,
    "tau2": 0.2105,
    "lcr": 0.0,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": 4.75,
    "gpqa": 62.85,
    "scicode": 20.45,
    "ifbench": 31.4,
    "aime25": 44.15,
    "critpt": 0.0,
    "mmmu_pro": 46.65,
    "livecodebench": 47.8,
    "sources": [
      "Artificial Analysis",
      "LMSYS Chatbot Arena"
    ],
    "source_count": 2,
    "data_tier": 1,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.917883+00:00",
        "confidence": 0.79,
        "raw_name": "GLM-4.5V (Reasoning)",
        "raw_scores": {
          "intelligence_score": 19.267407790488182,
          "coding_score": 10.9,
          "context_window": 64000,
          "gdpval": 558.9119795073117,
          "terminalbench_hard": 0.053,
          "tau2": 0.225,
          "lcr": 0.0,
          "hle": 5.9,
          "gpqa": 68.4,
          "scicode": 22.1,
          "ifbench": 34.2,
          "aime25": 73.0,
          "critpt": 0.0,
          "mmmu_pro": 50.5,
          "livecodebench": 60.4
        }
      },
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.917967+00:00",
        "confidence": 0.79,
        "raw_name": "GLM-4.5V (Non-reasoning)",
        "raw_scores": {
          "intelligence_score": 14.376428460400666,
          "coding_score": 10.8,
          "context_window": 64000,
          "gdpval": 506.84024448041635,
          "terminalbench_hard": 0.068,
          "tau2": 0.196,
          "lcr": 0.0,
          "hle": 3.6,
          "gpqa": 57.3,
          "scicode": 18.8,
          "ifbench": 28.6,
          "aime25": 15.3,
          "critpt": 0.0,
          "mmmu_pro": 42.8,
          "livecodebench": 35.2
        }
      },
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.893003+00:00",
        "confidence": 1.0,
        "raw_name": "glm-4.5v",
        "raw_scores": {
          "arena_elo": 1353.13,
          "arena_votes": 4950
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 19.267407790488182,
        "coding_score": 10.9,
        "gdpval": 558.9119795073117,
        "terminalbench_hard": 0.053,
        "tau2": 0.225,
        "lcr": 0.0,
        "hle": 5.9,
        "gpqa": 68.4,
        "scicode": 22.1,
        "ifbench": 34.2,
        "aime25": 73.0,
        "critpt": 0.0,
        "mmmu_pro": 50.5,
        "livecodebench": 60.4
      },
      "LMSYS Chatbot Arena": {
        "arena_elo": 1353.13
      }
    },
    "confidence_score": 0.86,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "glm-45",
    "canonical_name": "GLM-4.5 (Reasoning)",
    "model_name": "GLM-4.5 (Reasoning)",
    "aliases": [
      "GLM-4.5 (Reasoning)",
      "glm-4.5"
    ],
    "provider": "Z.ai",
    "context_window": 128000,
    "open_source": null,
    "arena_votes": 24600,
    "license_type": "MIT",
    "creator": null,
    "intelligence_score": 26.42,
    "coding_score": 26.26,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 1410.09,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": 509.9688,
    "terminalbench_hard": 0.22,
    "tau2": 0.43,
    "lcr": 48.3,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": 12.2,
    "gpqa": 78.2,
    "scicode": 34.8,
    "ifbench": 44.1,
    "aime25": 73.7,
    "critpt": 0.0,
    "mmmu_pro": null,
    "livecodebench": 73.8,
    "sources": [
      "Artificial Analysis",
      "LMSYS Chatbot Arena"
    ],
    "source_count": 2,
    "data_tier": 1,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.917926+00:00",
        "confidence": 0.79,
        "raw_name": "GLM-4.5 (Reasoning)",
        "raw_scores": {
          "intelligence_score": 26.42,
          "coding_score": 26.26,
          "context_window": 128000,
          "gdpval": 509.96876031035697,
          "terminalbench_hard": 0.22,
          "tau2": 0.43,
          "lcr": 48.3,
          "hle": 12.2,
          "gpqa": 78.2,
          "scicode": 34.8,
          "ifbench": 44.1,
          "aime25": 73.7,
          "critpt": 0.0,
          "livecodebench": 73.8
        }
      },
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.892137+00:00",
        "confidence": 1.0,
        "raw_name": "glm-4.5",
        "raw_scores": {
          "arena_elo": 1410.09,
          "arena_votes": 24600
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 26.42,
        "coding_score": 26.26,
        "gdpval": 509.96876031035697,
        "terminalbench_hard": 0.22,
        "tau2": 0.43,
        "lcr": 48.3,
        "hle": 12.2,
        "gpqa": 78.2,
        "scicode": 34.8,
        "ifbench": 44.1,
        "aime25": 73.7,
        "critpt": 0.0,
        "livecodebench": 73.8
      },
      "LMSYS Chatbot Arena": {
        "arena_elo": 1410.09
      }
    },
    "confidence_score": 0.895,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "command-r",
    "canonical_name": "Command-R+ (Apr '24)",
    "model_name": "Command-R+ (Apr '24)",
    "aliases": [
      "Command-R (Mar '24)",
      "Command-R+ (Apr '24)",
      "command-r"
    ],
    "provider": "Cohere",
    "context_window": 128000,
    "open_source": false,
    "arena_votes": 54038,
    "license_type": "CC-BY-NC 4.0 License with Acceptable Use Addendum",
    "creator": null,
    "intelligence_score": 7.8813,
    "coding_score": null,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 1226.94,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": null,
    "tau2": null,
    "lcr": null,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": 4.65,
    "gpqa": 30.35,
    "scicode": 9.0,
    "ifbench": null,
    "aime25": null,
    "critpt": null,
    "mmmu_pro": null,
    "livecodebench": 8.5,
    "sources": [
      "Artificial Analysis",
      "LMSYS Chatbot Arena"
    ],
    "source_count": 2,
    "data_tier": 1,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.917995+00:00",
        "confidence": 0.65,
        "raw_name": "Command-R+ (Apr '24)",
        "raw_scores": {
          "intelligence_score": 8.348799720910122,
          "context_window": 128000,
          "hle": 4.5,
          "gpqa": 32.3,
          "scicode": 11.8,
          "livecodebench": 12.2
        }
      },
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.918023+00:00",
        "confidence": 0.65,
        "raw_name": "Command-R (Mar '24)",
        "raw_scores": {
          "intelligence_score": 7.413887665229345,
          "context_window": 128000,
          "hle": 4.8,
          "gpqa": 28.4,
          "scicode": 6.2,
          "livecodebench": 4.8
        }
      },
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.894490+00:00",
        "confidence": 1.0,
        "raw_name": "command-r",
        "raw_scores": {
          "arena_elo": 1226.94,
          "arena_votes": 54038
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 8.348799720910122,
        "hle": 4.5,
        "gpqa": 32.3,
        "scicode": 11.8,
        "livecodebench": 12.2
      },
      "LMSYS Chatbot Arena": {
        "arena_elo": 1226.94
      }
    },
    "confidence_score": 0.7667,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "apriel-v15-15b-thinker",
    "canonical_name": "Apriel-v1.5-15B-Thinker",
    "model_name": "Apriel-v1.5-15B-Thinker",
    "aliases": [
      "Apriel-v1.5-15B-Thinker"
    ],
    "provider": null,
    "context_window": 128000,
    "open_source": true,
    "arena_votes": null,
    "license_type": "MIT",
    "creator": null,
    "intelligence_score": 28.3319,
    "coding_score": 18.68,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": null,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": 0.0,
    "latency_seconds": 0.17,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": 144.0,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": 0.106,
    "tau2": 0.684,
    "lcr": 20.0,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": 12.0,
    "gpqa": 71.3,
    "scicode": 34.8,
    "ifbench": 61.7,
    "aime25": 87.5,
    "critpt": 1.1,
    "mmmu_pro": 57.1,
    "livecodebench": 72.8,
    "sources": [
      "Artificial Analysis"
    ],
    "source_count": 1,
    "data_tier": 2,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.918067+00:00",
        "confidence": 1.0,
        "raw_name": "Apriel-v1.5-15B-Thinker",
        "raw_scores": {
          "intelligence_score": 28.33194461137262,
          "coding_score": 18.68,
          "blended_cost_per_1m": 0.0,
          "latency_seconds": 0.17,
          "tokens_per_second": 144.0,
          "context_window": 128000,
          "terminalbench_hard": 0.106,
          "tau2": 0.684,
          "lcr": 20.0,
          "hle": 12.0,
          "gpqa": 71.3,
          "scicode": 34.8,
          "ifbench": 61.7,
          "aime25": 87.5,
          "critpt": 1.1,
          "mmmu_pro": 57.1,
          "livecodebench": 72.8
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 28.33194461137262,
        "coding_score": 18.68,
        "terminalbench_hard": 0.106,
        "tau2": 0.684,
        "lcr": 20.0,
        "hle": 12.0,
        "gpqa": 71.3,
        "scicode": 34.8,
        "ifbench": 61.7,
        "aime25": 87.5,
        "critpt": 1.1,
        "mmmu_pro": 57.1,
        "livecodebench": 72.8
      }
    },
    "confidence_score": 1.0,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "jamba-15-large",
    "canonical_name": "Jamba 1.5 Large",
    "model_name": "Jamba 1.5 Large",
    "aliases": [
      "Jamba 1.5 Large",
      "jamba-1.5-large"
    ],
    "provider": "AI21 Labs",
    "context_window": 256000,
    "open_source": true,
    "arena_votes": 8659,
    "license_type": "Jamba Open Model License Agreement",
    "creator": null,
    "intelligence_score": 10.6951,
    "coding_score": null,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 1288.66,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": 3.5,
    "latency_seconds": 0.0,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": 0.0,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": null,
    "tau2": null,
    "lcr": null,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": 4.0,
    "gpqa": 42.7,
    "scicode": 16.3,
    "ifbench": null,
    "aime25": null,
    "critpt": null,
    "mmmu_pro": null,
    "livecodebench": 14.3,
    "sources": [
      "Artificial Analysis",
      "LMSYS Chatbot Arena"
    ],
    "source_count": 2,
    "data_tier": 1,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.918099+00:00",
        "confidence": 0.8600000000000001,
        "raw_name": "Jamba 1.5 Large",
        "raw_scores": {
          "intelligence_score": 10.695130465051701,
          "blended_cost_per_1m": 3.5,
          "latency_seconds": 0.0,
          "tokens_per_second": 0.0,
          "context_window": 256000,
          "hle": 4.0,
          "gpqa": 42.7,
          "scicode": 16.3,
          "livecodebench": 14.3
        }
      },
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.893961+00:00",
        "confidence": 1.0,
        "raw_name": "jamba-1.5-large",
        "raw_scores": {
          "arena_elo": 1288.66,
          "arena_votes": 8659
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 10.695130465051701,
        "hle": 4.0,
        "gpqa": 42.7,
        "scicode": 16.3,
        "livecodebench": 14.3
      },
      "LMSYS Chatbot Arena": {
        "arena_elo": 1288.66
      }
    },
    "confidence_score": 0.93,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "jamba-16-mini",
    "canonical_name": "Jamba 1.6 Mini",
    "model_name": "Jamba 1.6 Mini",
    "aliases": [
      "Jamba 1.6 Mini"
    ],
    "provider": null,
    "context_window": 256000,
    "open_source": true,
    "arena_votes": null,
    "license_type": "Jamba Open Model License Agreement",
    "creator": null,
    "intelligence_score": 7.8708,
    "coding_score": null,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": null,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": 0.25,
    "latency_seconds": 0.64,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": 151.0,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": null,
    "tau2": null,
    "lcr": null,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": 4.6,
    "gpqa": 30.0,
    "scicode": 10.1,
    "ifbench": null,
    "aime25": null,
    "critpt": null,
    "mmmu_pro": null,
    "livecodebench": 7.1,
    "sources": [
      "Artificial Analysis"
    ],
    "source_count": 1,
    "data_tier": 2,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.918131+00:00",
        "confidence": 0.8600000000000001,
        "raw_name": "Jamba 1.6 Mini",
        "raw_scores": {
          "intelligence_score": 7.870810849851021,
          "blended_cost_per_1m": 0.25,
          "latency_seconds": 0.64,
          "tokens_per_second": 151.0,
          "context_window": 256000,
          "hle": 4.6,
          "gpqa": 30.0,
          "scicode": 10.1,
          "livecodebench": 7.1
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 7.870810849851021,
        "hle": 4.6,
        "gpqa": 30.0,
        "scicode": 10.1,
        "livecodebench": 7.1
      }
    },
    "confidence_score": 0.86,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "jamba-15-mini",
    "canonical_name": "Jamba 1.5 Mini",
    "model_name": "Jamba 1.5 Mini",
    "aliases": [
      "Jamba 1.5 Mini",
      "jamba-1.5-mini"
    ],
    "provider": "AI21 Labs",
    "context_window": 256000,
    "open_source": true,
    "arena_votes": 8854,
    "license_type": "Jamba Open Model License Agreement",
    "creator": null,
    "intelligence_score": 8.0277,
    "coding_score": null,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 1238.91,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": 0.25,
    "latency_seconds": 0.0,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": 0.0,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": null,
    "tau2": null,
    "lcr": null,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": 5.1,
    "gpqa": 30.2,
    "scicode": 8.0,
    "ifbench": null,
    "aime25": null,
    "critpt": null,
    "mmmu_pro": null,
    "livecodebench": 6.2,
    "sources": [
      "Artificial Analysis",
      "LMSYS Chatbot Arena"
    ],
    "source_count": 2,
    "data_tier": 1,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.918162+00:00",
        "confidence": 0.8600000000000001,
        "raw_name": "Jamba 1.5 Mini",
        "raw_scores": {
          "intelligence_score": 8.027724014032357,
          "blended_cost_per_1m": 0.25,
          "latency_seconds": 0.0,
          "tokens_per_second": 0.0,
          "context_window": 256000,
          "hle": 5.1,
          "gpqa": 30.2,
          "scicode": 8.0,
          "livecodebench": 6.2
        }
      },
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.894391+00:00",
        "confidence": 1.0,
        "raw_name": "jamba-1.5-mini",
        "raw_scores": {
          "arena_elo": 1238.91,
          "arena_votes": 8854
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 8.027724014032357,
        "hle": 5.1,
        "gpqa": 30.2,
        "scicode": 8.0,
        "livecodebench": 6.2
      },
      "LMSYS Chatbot Arena": {
        "arena_elo": 1238.91
      }
    },
    "confidence_score": 0.93,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "jamba-16-large",
    "canonical_name": "Jamba 1.6 Large",
    "model_name": "Jamba 1.6 Large",
    "aliases": [
      "Jamba 1.6 Large"
    ],
    "provider": null,
    "context_window": 256000,
    "open_source": true,
    "arena_votes": null,
    "license_type": "Jamba Open Model License Agreement",
    "creator": null,
    "intelligence_score": 10.5553,
    "coding_score": null,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": null,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": 3.5,
    "latency_seconds": 0.84,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": 55.0,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": null,
    "tau2": null,
    "lcr": null,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": 4.0,
    "gpqa": 38.7,
    "scicode": 18.4,
    "ifbench": null,
    "aime25": null,
    "critpt": null,
    "mmmu_pro": null,
    "livecodebench": 17.2,
    "sources": [
      "Artificial Analysis"
    ],
    "source_count": 1,
    "data_tier": 2,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.918207+00:00",
        "confidence": 0.8600000000000001,
        "raw_name": "Jamba 1.6 Large",
        "raw_scores": {
          "intelligence_score": 10.555304867718469,
          "blended_cost_per_1m": 3.5,
          "latency_seconds": 0.84,
          "tokens_per_second": 55.0,
          "context_window": 256000,
          "hle": 4.0,
          "gpqa": 38.7,
          "scicode": 18.4,
          "livecodebench": 17.2
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 10.555304867718469,
        "hle": 4.0,
        "gpqa": 38.7,
        "scicode": 18.4,
        "livecodebench": 17.2
      }
    },
    "confidence_score": 0.86,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "arctic-instruct",
    "canonical_name": "Arctic Instruct",
    "model_name": "Arctic Instruct",
    "aliases": [
      "Arctic Instruct"
    ],
    "provider": null,
    "context_window": 4000,
    "open_source": true,
    "arena_votes": null,
    "license_type": "Apache 2.0",
    "creator": null,
    "intelligence_score": 8.8232,
    "coding_score": null,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": null,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": null,
    "tau2": null,
    "lcr": null,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": null,
    "gpqa": null,
    "scicode": null,
    "ifbench": null,
    "aime25": null,
    "critpt": null,
    "mmmu_pro": null,
    "livecodebench": null,
    "sources": [
      "Artificial Analysis"
    ],
    "source_count": 1,
    "data_tier": 2,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.918231+00:00",
        "confidence": 0.44,
        "raw_name": "Arctic Instruct",
        "raw_scores": {
          "intelligence_score": 8.823244716278813,
          "context_window": 4000
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 8.823244716278813
      }
    },
    "confidence_score": 0.44,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "qwen25-max",
    "canonical_name": "Qwen2.5 Max",
    "model_name": "Qwen2.5 Max",
    "aliases": [
      "Qwen2.5 Max",
      "qwen2.5-max"
    ],
    "provider": "Alibaba",
    "context_window": 32000,
    "open_source": null,
    "arena_votes": 33204,
    "license_type": "Proprietary",
    "creator": null,
    "intelligence_score": 16.2829,
    "coding_score": null,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 1374.18,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": 2.8,
    "latency_seconds": 1.15,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": 38.0,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": null,
    "tau2": null,
    "lcr": null,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": 4.5,
    "gpqa": 58.7,
    "scicode": 33.7,
    "ifbench": null,
    "aime25": null,
    "critpt": null,
    "mmmu_pro": null,
    "livecodebench": 35.9,
    "sources": [
      "Artificial Analysis",
      "LMSYS Chatbot Arena"
    ],
    "source_count": 2,
    "data_tier": 1,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.918264+00:00",
        "confidence": 0.8600000000000001,
        "raw_name": "Qwen2.5 Max",
        "raw_scores": {
          "intelligence_score": 16.282944203161424,
          "blended_cost_per_1m": 2.8,
          "latency_seconds": 1.15,
          "tokens_per_second": 38.0,
          "context_window": 32000,
          "hle": 4.5,
          "gpqa": 58.7,
          "scicode": 33.7,
          "livecodebench": 35.9
        }
      },
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.892767+00:00",
        "confidence": 1.0,
        "raw_name": "qwen2.5-max",
        "raw_scores": {
          "arena_elo": 1374.18,
          "arena_votes": 33204
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 16.282944203161424,
        "hle": 4.5,
        "gpqa": 58.7,
        "scicode": 33.7,
        "livecodebench": 35.9
      },
      "LMSYS Chatbot Arena": {
        "arena_elo": 1374.18
      }
    },
    "confidence_score": 0.93,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "qwen25-instruct-72b",
    "canonical_name": "Qwen2.5 Instruct 72B",
    "model_name": "Qwen2.5 Instruct 72B",
    "aliases": [
      "Qwen2.5 Instruct 72B"
    ],
    "provider": "Alibaba",
    "context_window": 131072,
    "open_source": true,
    "arena_votes": null,
    "license_type": "Qwen License Agreement",
    "creator": null,
    "intelligence_score": 15.5578,
    "coding_score": 11.94,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": null,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": 0.045,
    "tau2": 0.345,
    "lcr": 20.3,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": 4.2,
    "gpqa": 49.1,
    "scicode": 26.7,
    "ifbench": 36.9,
    "aime25": 14.0,
    "critpt": 0.0,
    "mmmu_pro": null,
    "livecodebench": 27.6,
    "sources": [
      "Artificial Analysis"
    ],
    "source_count": 1,
    "data_tier": 2,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.918302+00:00",
        "confidence": 0.79,
        "raw_name": "Qwen2.5 Instruct 72B",
        "raw_scores": {
          "intelligence_score": 15.557766275790943,
          "coding_score": 11.94,
          "context_window": 131072,
          "terminalbench_hard": 0.045,
          "tau2": 0.345,
          "lcr": 20.3,
          "hle": 4.2,
          "gpqa": 49.1,
          "scicode": 26.7,
          "ifbench": 36.9,
          "aime25": 14.0,
          "critpt": 0.0,
          "livecodebench": 27.6
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 15.557766275790943,
        "coding_score": 11.94,
        "terminalbench_hard": 0.045,
        "tau2": 0.345,
        "lcr": 20.3,
        "hle": 4.2,
        "gpqa": 49.1,
        "scicode": 26.7,
        "ifbench": 36.9,
        "aime25": 14.0,
        "critpt": 0.0,
        "livecodebench": 27.6
      }
    },
    "confidence_score": 0.79,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "qwen25-coder-instruct-32b",
    "canonical_name": "Qwen2.5 Coder Instruct 32B",
    "model_name": "Qwen2.5 Coder Instruct 32B",
    "aliases": [
      "Qwen2.5 Coder Instruct 32B"
    ],
    "provider": "Alibaba",
    "context_window": 131072,
    "open_source": true,
    "arena_votes": null,
    "license_type": "Apache 2.0",
    "creator": null,
    "intelligence_score": 12.8656,
    "coding_score": null,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": null,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": null,
    "tau2": null,
    "lcr": null,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": 3.8,
    "gpqa": 41.7,
    "scicode": 27.1,
    "ifbench": null,
    "aime25": null,
    "critpt": null,
    "mmmu_pro": null,
    "livecodebench": 29.5,
    "sources": [
      "Artificial Analysis"
    ],
    "source_count": 1,
    "data_tier": 2,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.918334+00:00",
        "confidence": 0.65,
        "raw_name": "Qwen2.5 Coder Instruct 32B",
        "raw_scores": {
          "intelligence_score": 12.865568210057065,
          "context_window": 131072,
          "hle": 3.8,
          "gpqa": 41.7,
          "scicode": 27.1,
          "livecodebench": 29.5
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 12.865568210057065,
        "hle": 3.8,
        "gpqa": 41.7,
        "scicode": 27.1,
        "livecodebench": 29.5
      }
    },
    "confidence_score": 0.65,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "qwen25-turbo",
    "canonical_name": "Qwen2.5 Turbo",
    "model_name": "Qwen2.5 Turbo",
    "aliases": [
      "Qwen2.5 Turbo"
    ],
    "provider": "Alibaba",
    "context_window": 1000000,
    "open_source": true,
    "arena_votes": null,
    "license_type": null,
    "creator": null,
    "intelligence_score": 11.9736,
    "coding_score": null,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": null,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": 0.09,
    "latency_seconds": 1.01,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": 64.0,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": null,
    "tau2": null,
    "lcr": null,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": 4.2,
    "gpqa": 41.0,
    "scicode": 15.3,
    "ifbench": null,
    "aime25": null,
    "critpt": null,
    "mmmu_pro": null,
    "livecodebench": 16.3,
    "sources": [
      "Artificial Analysis"
    ],
    "source_count": 1,
    "data_tier": 2,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.918367+00:00",
        "confidence": 0.8600000000000001,
        "raw_name": "Qwen2.5 Turbo",
        "raw_scores": {
          "intelligence_score": 11.973563045943266,
          "blended_cost_per_1m": 0.09,
          "latency_seconds": 1.01,
          "tokens_per_second": 64.0,
          "context_window": 1000000,
          "hle": 4.2,
          "gpqa": 41.0,
          "scicode": 15.3,
          "livecodebench": 16.3
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 11.973563045943266,
        "hle": 4.2,
        "gpqa": 41.0,
        "scicode": 15.3,
        "livecodebench": 16.3
      }
    },
    "confidence_score": 0.86,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "qwen2-instruct-72b",
    "canonical_name": "Qwen2 Instruct 72B",
    "model_name": "Qwen2 Instruct 72B",
    "aliases": [
      "Qwen2 Instruct 72B"
    ],
    "provider": "Alibaba",
    "context_window": 131072,
    "open_source": true,
    "arena_votes": null,
    "license_type": "Tongyi Qianwen LICENSE AGREEMENT",
    "creator": null,
    "intelligence_score": 11.6625,
    "coding_score": null,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": null,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": null,
    "tau2": null,
    "lcr": null,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": 3.7,
    "gpqa": 37.1,
    "scicode": 22.9,
    "ifbench": null,
    "aime25": null,
    "critpt": null,
    "mmmu_pro": null,
    "livecodebench": 15.9,
    "sources": [
      "Artificial Analysis"
    ],
    "source_count": 1,
    "data_tier": 2,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.918396+00:00",
        "confidence": 0.65,
        "raw_name": "Qwen2 Instruct 72B",
        "raw_scores": {
          "intelligence_score": 11.662530563545237,
          "context_window": 131072,
          "hle": 3.7,
          "gpqa": 37.1,
          "scicode": 22.9,
          "livecodebench": 15.9
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 11.662530563545237,
        "hle": 3.7,
        "gpqa": 37.1,
        "scicode": 22.9,
        "livecodebench": 15.9
      }
    },
    "confidence_score": 0.65,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "qwen3-235b-a22b",
    "canonical_name": "Qwen3 235B A22B (Reasoning)",
    "model_name": "Qwen3 235B A22B (Reasoning)",
    "aliases": [
      "Qwen3 235B A22B (Non-reasoning)",
      "Qwen3 235B A22B (Reasoning)",
      "qwen3-235b-a22b"
    ],
    "provider": "Alibaba",
    "context_window": 32768,
    "open_source": true,
    "arena_votes": 27019,
    "license_type": "Apache 2.0",
    "creator": null,
    "intelligence_score": 18.8835,
    "coding_score": 15.67,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 1374.77,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": 782.4399,
    "terminalbench_hard": 0.061,
    "tau2": 0.256,
    "lcr": 0.0,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": 8.2,
    "gpqa": 65.65,
    "scicode": 34.9,
    "ifbench": 37.65,
    "aime25": 52.85,
    "critpt": 0.0,
    "mmmu_pro": null,
    "livecodebench": 48.25,
    "sources": [
      "Artificial Analysis",
      "LMSYS Chatbot Arena"
    ],
    "source_count": 2,
    "data_tier": 1,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.918436+00:00",
        "confidence": 0.79,
        "raw_name": "Qwen3 235B A22B (Reasoning)",
        "raw_scores": {
          "intelligence_score": 21.79989063206562,
          "coding_score": 17.35,
          "context_window": 32768,
          "gdpval": 784.3977070003737,
          "terminalbench_hard": 0.061,
          "tau2": 0.24,
          "lcr": 0.0,
          "hle": 11.7,
          "gpqa": 70.0,
          "scicode": 39.9,
          "ifbench": 38.7,
          "aime25": 82.0,
          "critpt": 0.0,
          "livecodebench": 62.2
        }
      },
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.918668+00:00",
        "confidence": 0.79,
        "raw_name": "Qwen3 235B A22B (Non-reasoning)",
        "raw_scores": {
          "intelligence_score": 15.967093575630875,
          "coding_score": 13.99,
          "context_window": 32768,
          "gdpval": 780.4820066255605,
          "terminalbench_hard": 0.061,
          "tau2": 0.272,
          "lcr": 0.0,
          "hle": 4.7,
          "gpqa": 61.3,
          "scicode": 29.9,
          "ifbench": 36.6,
          "aime25": 23.7,
          "critpt": 0.0,
          "livecodebench": 34.3
        }
      },
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.892742+00:00",
        "confidence": 1.0,
        "raw_name": "qwen3-235b-a22b",
        "raw_scores": {
          "arena_elo": 1374.77,
          "arena_votes": 27019
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 21.79989063206562,
        "coding_score": 17.35,
        "gdpval": 784.3977070003737,
        "terminalbench_hard": 0.061,
        "tau2": 0.24,
        "lcr": 0.0,
        "hle": 11.7,
        "gpqa": 70.0,
        "scicode": 39.9,
        "ifbench": 38.7,
        "aime25": 82.0,
        "critpt": 0.0,
        "livecodebench": 62.2
      },
      "LMSYS Chatbot Arena": {
        "arena_elo": 1374.77
      }
    },
    "confidence_score": 0.86,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "qwen3-32b",
    "canonical_name": "Qwen3 32B (Non-reasoning)",
    "model_name": "Qwen3 32B (Non-reasoning)",
    "aliases": [
      "Qwen3 32B (Non-reasoning)",
      "Qwen3 32B (Reasoning)",
      "qwen3-32b"
    ],
    "provider": "Alibaba",
    "context_window": 32768,
    "open_source": true,
    "arena_votes": 3932,
    "license_type": "Apache 2.0",
    "creator": null,
    "intelligence_score": 17.4601,
    "coding_score": 13.83,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 1347.0,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": 541.8342,
    "terminalbench_hard": 0.03,
    "tau2": 0.298,
    "lcr": 0.0,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": 6.3927,
    "gpqa": 60.4583,
    "scicode": 31.8715,
    "ifbench": 34.0113,
    "aime25": 47.5854,
    "critpt": 0.3,
    "mmmu_pro": null,
    "livecodebench": 42.298,
    "sources": [
      "Artificial Analysis",
      "LMSYS Chatbot Arena"
    ],
    "source_count": 2,
    "data_tier": 1,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.918473+00:00",
        "confidence": 0.72,
        "raw_name": "Qwen3 32B (Non-reasoning)",
        "raw_scores": {
          "intelligence_score": 14.532286886975053,
          "context_window": 32768,
          "lcr": 0.0,
          "hle": 4.3,
          "gpqa": 53.5,
          "scicode": 28.0,
          "ifbench": 31.5,
          "aime25": 19.7,
          "livecodebench": 28.8
        }
      },
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.918514+00:00",
        "confidence": 0.79,
        "raw_name": "Qwen3 32B (Reasoning)",
        "raw_scores": {
          "intelligence_score": 20.128565048930692,
          "coding_score": 13.83,
          "context_window": 32768,
          "gdpval": 541.8342120733842,
          "terminalbench_hard": 0.03,
          "tau2": 0.298,
          "lcr": 0.0,
          "hle": 8.3,
          "gpqa": 66.8,
          "scicode": 35.4,
          "ifbench": 36.3,
          "aime25": 73.0,
          "critpt": 0.3,
          "livecodebench": 54.6
        }
      },
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.893107+00:00",
        "confidence": 1.0,
        "raw_name": "qwen3-32b",
        "raw_scores": {
          "arena_elo": 1347.0,
          "arena_votes": 3932
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 14.532286886975053,
        "lcr": 0.0,
        "hle": 4.3,
        "gpqa": 53.5,
        "scicode": 28.0,
        "ifbench": 31.5,
        "aime25": 19.7,
        "livecodebench": 28.8,
        "coding_score": 13.83,
        "gdpval": 541.8342120733842,
        "terminalbench_hard": 0.03,
        "tau2": 0.298,
        "critpt": 0.3
      },
      "LMSYS Chatbot Arena": {
        "arena_elo": 1347.0
      }
    },
    "confidence_score": 0.8367,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "qwen3-14b",
    "canonical_name": "Qwen3 14B (Non-reasoning)",
    "model_name": "Qwen3 14B (Non-reasoning)",
    "aliases": [
      "Qwen3 14B (Non-reasoning)",
      "Qwen3 14B (Reasoning)"
    ],
    "provider": "Alibaba",
    "context_window": 32768,
    "open_source": true,
    "arena_votes": null,
    "license_type": "Apache 2.0",
    "creator": null,
    "intelligence_score": 17.2095,
    "coding_score": 12.715,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": null,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": 523.8007,
    "terminalbench_hard": 0.0455,
    "tau2": 0.3335,
    "lcr": 0.0,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": 4.25,
    "gpqa": 53.7,
    "scicode": 29.05,
    "ifbench": 32.2,
    "aime25": 56.85,
    "critpt": 0.0,
    "mmmu_pro": null,
    "livecodebench": 40.15,
    "sources": [
      "Artificial Analysis"
    ],
    "source_count": 1,
    "data_tier": 2,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.918555+00:00",
        "confidence": 0.79,
        "raw_name": "Qwen3 14B (Non-reasoning)",
        "raw_scores": {
          "intelligence_score": 15.665329414925122,
          "coding_score": 12.37,
          "context_window": 32768,
          "gdpval": 519.9410374553477,
          "terminalbench_hard": 0.053,
          "tau2": 0.322,
          "lcr": 0.0,
          "hle": 4.2,
          "gpqa": 47.0,
          "scicode": 26.5,
          "ifbench": 23.9,
          "aime25": 58.0,
          "critpt": 0.0,
          "livecodebench": 28.0
        }
      },
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.918949+00:00",
        "confidence": 0.79,
        "raw_name": "Qwen3 14B (Reasoning)",
        "raw_scores": {
          "intelligence_score": 18.753585351905503,
          "coding_score": 13.06,
          "context_window": 32768,
          "gdpval": 527.6604279357574,
          "terminalbench_hard": 0.038,
          "tau2": 0.345,
          "lcr": 0.0,
          "hle": 4.3,
          "gpqa": 60.4,
          "scicode": 31.6,
          "ifbench": 40.5,
          "aime25": 55.7,
          "critpt": 0.0,
          "livecodebench": 52.3
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 15.665329414925122,
        "coding_score": 12.37,
        "gdpval": 519.9410374553477,
        "terminalbench_hard": 0.053,
        "tau2": 0.322,
        "lcr": 0.0,
        "hle": 4.2,
        "gpqa": 47.0,
        "scicode": 26.5,
        "ifbench": 23.9,
        "aime25": 58.0,
        "critpt": 0.0,
        "livecodebench": 28.0
      }
    },
    "confidence_score": 0.79,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "qwen3-4b",
    "canonical_name": "Qwen3 4B (Reasoning)",
    "model_name": "Qwen3 4B (Reasoning)",
    "aliases": [
      "Qwen3 4B (Non-reasoning)",
      "Qwen3 4B (Reasoning)"
    ],
    "provider": "Alibaba",
    "context_window": 32000,
    "open_source": true,
    "arena_votes": null,
    "license_type": "Apache 2.0",
    "creator": null,
    "intelligence_score": 13.4004,
    "coding_score": null,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": null,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": null,
    "tau2": 0.19,
    "lcr": 0.0,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": 4.4358,
    "gpqa": 46.3168,
    "scicode": 9.7628,
    "ifbench": 32.5,
    "aime25": 22.3,
    "critpt": null,
    "mmmu_pro": null,
    "livecodebench": 35.4927,
    "sources": [
      "Artificial Analysis"
    ],
    "source_count": 1,
    "data_tier": 2,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.918589+00:00",
        "confidence": 0.72,
        "raw_name": "Qwen3 4B (Reasoning)",
        "raw_scores": {
          "intelligence_score": 14.220680621459485,
          "context_window": 32000,
          "tau2": 0.19,
          "lcr": 0.0,
          "hle": 5.1,
          "gpqa": 52.2,
          "scicode": 3.5,
          "ifbench": 32.5,
          "aime25": 22.3,
          "livecodebench": 46.5
        }
      },
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.918815+00:00",
        "confidence": 0.65,
        "raw_name": "Qwen3 4B (Non-reasoning)",
        "raw_scores": {
          "intelligence_score": 12.49184791077126,
          "context_window": 32000,
          "hle": 3.7,
          "gpqa": 39.8,
          "scicode": 16.7,
          "livecodebench": 23.3
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 14.220680621459485,
        "tau2": 0.19,
        "lcr": 0.0,
        "hle": 5.1,
        "gpqa": 52.2,
        "scicode": 3.5,
        "ifbench": 32.5,
        "aime25": 22.3,
        "livecodebench": 46.5
      }
    },
    "confidence_score": 0.685,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "qwen3-30b-a3b",
    "canonical_name": "Qwen3 30B A3B (Non-reasoning)",
    "model_name": "Qwen3 30B A3B (Non-reasoning)",
    "aliases": [
      "Qwen3 30B A3B (Non-reasoning)",
      "Qwen3 30B A3B (Reasoning)",
      "qwen3-30b-a3b"
    ],
    "provider": "Alibaba",
    "context_window": 32768,
    "open_source": true,
    "arena_votes": 27282,
    "license_type": "Apache 2.0",
    "creator": null,
    "intelligence_score": 16.8274,
    "coding_score": 12.175,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 1328.13,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": 460.4354,
    "terminalbench_hard": 0.0455,
    "tau2": 0.241,
    "lcr": 0.0,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": 5.6,
    "gpqa": 56.55,
    "scicode": 27.45,
    "ifbench": 36.7,
    "aime25": 47.0,
    "critpt": 0.0,
    "mmmu_pro": null,
    "livecodebench": 41.4,
    "sources": [
      "Artificial Analysis",
      "LMSYS Chatbot Arena"
    ],
    "source_count": 2,
    "data_tier": 1,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.918629+00:00",
        "confidence": 0.79,
        "raw_name": "Qwen3 30B A3B (Non-reasoning)",
        "raw_scores": {
          "intelligence_score": 14.56528895780426,
          "coding_score": 13.34,
          "context_window": 32768,
          "gdpval": 375.6972802726648,
          "terminalbench_hard": 0.068,
          "tau2": 0.222,
          "lcr": 0.0,
          "hle": 4.6,
          "gpqa": 51.5,
          "scicode": 26.4,
          "ifbench": 31.9,
          "aime25": 21.7,
          "critpt": 0.0,
          "livecodebench": 32.2
        }
      },
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.918709+00:00",
        "confidence": 0.79,
        "raw_name": "Qwen3 30B A3B (Reasoning)",
        "raw_scores": {
          "intelligence_score": 19.089528772355642,
          "coding_score": 11.01,
          "context_window": 32768,
          "gdpval": 545.173436814443,
          "terminalbench_hard": 0.023,
          "tau2": 0.26,
          "lcr": 0.0,
          "hle": 6.6,
          "gpqa": 61.6,
          "scicode": 28.5,
          "ifbench": 41.5,
          "aime25": 72.3,
          "critpt": 0.0,
          "livecodebench": 50.6
        }
      },
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.893443+00:00",
        "confidence": 1.0,
        "raw_name": "qwen3-30b-a3b",
        "raw_scores": {
          "arena_elo": 1328.13,
          "arena_votes": 27282
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 14.56528895780426,
        "coding_score": 13.34,
        "gdpval": 375.6972802726648,
        "terminalbench_hard": 0.068,
        "tau2": 0.222,
        "lcr": 0.0,
        "hle": 4.6,
        "gpqa": 51.5,
        "scicode": 26.4,
        "ifbench": 31.9,
        "aime25": 21.7,
        "critpt": 0.0,
        "livecodebench": 32.2
      },
      "LMSYS Chatbot Arena": {
        "arena_elo": 1328.13
      }
    },
    "confidence_score": 0.86,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "qwen3-8b",
    "canonical_name": "Qwen3 8B (Reasoning)",
    "model_name": "Qwen3 8B (Reasoning)",
    "aliases": [
      "Qwen3 8B (Non-reasoning)",
      "Qwen3 8B (Reasoning)"
    ],
    "provider": "Alibaba",
    "context_window": 131072,
    "open_source": true,
    "arena_votes": null,
    "license_type": "Apache 2.0",
    "creator": null,
    "intelligence_score": 14.2648,
    "coding_score": 8.075,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": null,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": 537.1311,
    "terminalbench_hard": 0.023,
    "tau2": 0.2635,
    "lcr": 0.0,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": 3.5,
    "gpqa": 52.05,
    "scicode": 19.7,
    "ifbench": 31.05,
    "aime25": 21.65,
    "critpt": 0.0,
    "mmmu_pro": null,
    "livecodebench": 30.4,
    "sources": [
      "Artificial Analysis"
    ],
    "source_count": 1,
    "data_tier": 2,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.918749+00:00",
        "confidence": 0.79,
        "raw_name": "Qwen3 8B (Reasoning)",
        "raw_scores": {
          "intelligence_score": 15.279926185040281,
          "coding_score": 9.04,
          "context_window": 131072,
          "gdpval": 546.9951230282186,
          "terminalbench_hard": 0.023,
          "tau2": 0.278,
          "lcr": 0.0,
          "hle": 4.2,
          "gpqa": 58.9,
          "scicode": 22.6,
          "ifbench": 33.5,
          "aime25": 19.0,
          "critpt": 0.0,
          "livecodebench": 40.6
        }
      },
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.918886+00:00",
        "confidence": 0.79,
        "raw_name": "Qwen3 8B (Non-reasoning)",
        "raw_scores": {
          "intelligence_score": 13.249634311307114,
          "coding_score": 7.11,
          "context_window": 32768,
          "gdpval": 527.2670509536936,
          "terminalbench_hard": 0.023,
          "tau2": 0.249,
          "lcr": 0.0,
          "hle": 2.8,
          "gpqa": 45.2,
          "scicode": 16.8,
          "ifbench": 28.6,
          "aime25": 24.3,
          "critpt": 0.0,
          "livecodebench": 20.2
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 15.279926185040281,
        "coding_score": 9.04,
        "gdpval": 546.9951230282186,
        "terminalbench_hard": 0.023,
        "tau2": 0.278,
        "lcr": 0.0,
        "hle": 4.2,
        "gpqa": 58.9,
        "scicode": 22.6,
        "ifbench": 33.5,
        "aime25": 19.0,
        "critpt": 0.0,
        "livecodebench": 40.6
      }
    },
    "confidence_score": 0.79,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "qwq-32b",
    "canonical_name": "QwQ 32B",
    "model_name": "QwQ 32B",
    "aliases": [
      "QwQ 32B",
      "qwq-32b"
    ],
    "provider": "Alibaba",
    "context_window": 131072,
    "open_source": true,
    "arena_votes": 26000,
    "license_type": "Apache 2.0",
    "creator": null,
    "intelligence_score": 19.7242,
    "coding_score": null,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 1335.78,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": null,
    "tau2": null,
    "lcr": 25.0,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": 8.2,
    "gpqa": 59.3,
    "scicode": 35.8,
    "ifbench": 38.8,
    "aime25": 29.0,
    "critpt": null,
    "mmmu_pro": null,
    "livecodebench": 63.1,
    "sources": [
      "Artificial Analysis",
      "LMSYS Chatbot Arena"
    ],
    "source_count": 2,
    "data_tier": 1,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.918786+00:00",
        "confidence": 0.72,
        "raw_name": "QwQ 32B",
        "raw_scores": {
          "intelligence_score": 19.724216680037447,
          "context_window": 131072,
          "lcr": 25.0,
          "hle": 8.2,
          "gpqa": 59.3,
          "scicode": 35.8,
          "ifbench": 38.8,
          "aime25": 29.0,
          "livecodebench": 63.1
        }
      },
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.893318+00:00",
        "confidence": 1.0,
        "raw_name": "qwq-32b",
        "raw_scores": {
          "arena_elo": 1335.78,
          "arena_votes": 26000
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 19.724216680037447,
        "lcr": 25.0,
        "hle": 8.2,
        "gpqa": 59.3,
        "scicode": 35.8,
        "ifbench": 38.8,
        "aime25": 29.0,
        "livecodebench": 63.1
      },
      "LMSYS Chatbot Arena": {
        "arena_elo": 1335.78
      }
    },
    "confidence_score": 0.86,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "qwq-32b-preview",
    "canonical_name": "QwQ 32B-Preview",
    "model_name": "QwQ 32B-Preview",
    "aliases": [
      "QwQ 32B-Preview",
      "qwq-32b-preview"
    ],
    "provider": "Alibaba",
    "context_window": 32768,
    "open_source": true,
    "arena_votes": 3233,
    "license_type": "Apache 2.0",
    "creator": null,
    "intelligence_score": 15.174,
    "coding_score": null,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 1157.0,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": 0.14,
    "latency_seconds": 0.27,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": 55.0,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": null,
    "tau2": null,
    "lcr": null,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": 4.8,
    "gpqa": 55.7,
    "scicode": 3.8,
    "ifbench": null,
    "aime25": null,
    "critpt": null,
    "mmmu_pro": null,
    "livecodebench": 33.7,
    "sources": [
      "Artificial Analysis",
      "LMSYS Chatbot Arena"
    ],
    "source_count": 2,
    "data_tier": 1,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.918847+00:00",
        "confidence": 0.8600000000000001,
        "raw_name": "QwQ 32B-Preview",
        "raw_scores": {
          "intelligence_score": 15.173958740020906,
          "blended_cost_per_1m": 0.14,
          "latency_seconds": 0.27,
          "tokens_per_second": 55.0,
          "context_window": 32768,
          "hle": 4.8,
          "gpqa": 55.7,
          "scicode": 3.8,
          "livecodebench": 33.7
        }
      },
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.894964+00:00",
        "confidence": 1.0,
        "raw_name": "qwq-32b-preview",
        "raw_scores": {
          "arena_elo": 1157.0,
          "arena_votes": 3233
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 15.173958740020906,
        "hle": 4.8,
        "gpqa": 55.7,
        "scicode": 3.8,
        "livecodebench": 33.7
      },
      "LMSYS Chatbot Arena": {
        "arena_elo": 1157.0
      }
    },
    "confidence_score": 0.93,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "qwen15-chat-110b",
    "canonical_name": "Qwen1.5 Chat 110B",
    "model_name": "Qwen1.5 Chat 110B",
    "aliases": [
      "Qwen1.5 Chat 110B"
    ],
    "provider": "Alibaba",
    "context_window": 32000,
    "open_source": true,
    "arena_votes": null,
    "license_type": "Tongyi Qianwen LICENSE AGREEMENT",
    "creator": null,
    "intelligence_score": 9.5482,
    "coding_score": null,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": null,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": 0.0,
    "latency_seconds": 0.0,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": 0.0,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": null,
    "tau2": null,
    "lcr": null,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": null,
    "gpqa": 28.9,
    "scicode": null,
    "ifbench": null,
    "aime25": null,
    "critpt": null,
    "mmmu_pro": null,
    "livecodebench": null,
    "sources": [
      "Artificial Analysis"
    ],
    "source_count": 1,
    "data_tier": 2,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.918910+00:00",
        "confidence": 0.72,
        "raw_name": "Qwen1.5 Chat 110B",
        "raw_scores": {
          "intelligence_score": 9.5481702885671,
          "blended_cost_per_1m": 0.0,
          "latency_seconds": 0.0,
          "tokens_per_second": 0.0,
          "context_window": 32000,
          "gpqa": 28.9
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 9.5481702885671,
        "gpqa": 28.9
      }
    },
    "confidence_score": 0.72,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "qwen25-instruct-32b",
    "canonical_name": "Qwen2.5 Instruct 32B",
    "model_name": "Qwen2.5 Instruct 32B",
    "aliases": [
      "Qwen2.5 Instruct 32B"
    ],
    "provider": "Alibaba",
    "context_window": 128000,
    "open_source": true,
    "arena_votes": null,
    "license_type": "Apache 2.0",
    "creator": null,
    "intelligence_score": 13.2365,
    "coding_score": null,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": null,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": 0.0,
    "latency_seconds": 0.0,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": 0.0,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": null,
    "tau2": null,
    "lcr": null,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": 3.8,
    "gpqa": 46.6,
    "scicode": 22.9,
    "ifbench": null,
    "aime25": null,
    "critpt": null,
    "mmmu_pro": null,
    "livecodebench": 24.8,
    "sources": [
      "Artificial Analysis"
    ],
    "source_count": 1,
    "data_tier": 2,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.918980+00:00",
        "confidence": 0.8600000000000001,
        "raw_name": "Qwen2.5 Instruct 32B",
        "raw_scores": {
          "intelligence_score": 13.236526421623505,
          "blended_cost_per_1m": 0.0,
          "latency_seconds": 0.0,
          "tokens_per_second": 0.0,
          "context_window": 128000,
          "hle": 3.8,
          "gpqa": 46.6,
          "scicode": 22.9,
          "livecodebench": 24.8
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 13.236526421623505,
        "hle": 3.8,
        "gpqa": 46.6,
        "scicode": 22.9,
        "livecodebench": 24.8
      }
    },
    "confidence_score": 0.86,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "qwen25-coder-instruct-7b",
    "canonical_name": "Qwen2.5 Coder Instruct 7B",
    "model_name": "Qwen2.5 Coder Instruct 7B",
    "aliases": [
      "Qwen2.5 Coder Instruct 7B"
    ],
    "provider": "Alibaba",
    "context_window": 131072,
    "open_source": true,
    "arena_votes": null,
    "license_type": "Apache 2.0",
    "creator": null,
    "intelligence_score": 9.9825,
    "coding_score": null,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": null,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": null,
    "tau2": null,
    "lcr": null,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": 4.8,
    "gpqa": 33.9,
    "scicode": 14.8,
    "ifbench": null,
    "aime25": null,
    "critpt": null,
    "mmmu_pro": null,
    "livecodebench": 12.6,
    "sources": [
      "Artificial Analysis"
    ],
    "source_count": 1,
    "data_tier": 2,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.919010+00:00",
        "confidence": 0.65,
        "raw_name": "Qwen2.5 Coder Instruct 7B ",
        "raw_scores": {
          "intelligence_score": 9.982467098648165,
          "context_window": 131072,
          "hle": 4.8,
          "gpqa": 33.9,
          "scicode": 14.8,
          "livecodebench": 12.6
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 9.982467098648165,
        "hle": 4.8,
        "gpqa": 33.9,
        "scicode": 14.8,
        "livecodebench": 12.6
      }
    },
    "confidence_score": 0.65,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "qwen-chat-72b",
    "canonical_name": "Qwen Chat 72B",
    "model_name": "Qwen Chat 72B",
    "aliases": [
      "Qwen Chat 72B"
    ],
    "provider": "Alibaba",
    "context_window": 33792,
    "open_source": null,
    "arena_votes": null,
    "license_type": null,
    "creator": null,
    "intelligence_score": 8.8232,
    "coding_score": null,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": null,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": 0.0,
    "latency_seconds": 0.0,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": 0.0,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": null,
    "tau2": null,
    "lcr": null,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": null,
    "gpqa": null,
    "scicode": null,
    "ifbench": null,
    "aime25": null,
    "critpt": null,
    "mmmu_pro": null,
    "livecodebench": null,
    "sources": [
      "Artificial Analysis"
    ],
    "source_count": 1,
    "data_tier": 2,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.919032+00:00",
        "confidence": 0.65,
        "raw_name": "Qwen Chat 72B",
        "raw_scores": {
          "intelligence_score": 8.823244716278813,
          "blended_cost_per_1m": 0.0,
          "latency_seconds": 0.0,
          "tokens_per_second": 0.0,
          "context_window": 33792
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 8.823244716278813
      }
    },
    "confidence_score": 0.65,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "seed-oss-36b-instruct",
    "canonical_name": "Seed-OSS-36B-Instruct",
    "model_name": "Seed-OSS-36B-Instruct",
    "aliases": [
      "Seed-OSS-36B-Instruct"
    ],
    "provider": null,
    "context_window": 512000,
    "open_source": true,
    "arena_votes": null,
    "license_type": "Apache 2.0",
    "creator": null,
    "intelligence_score": 25.16,
    "coding_score": 16.7,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": null,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": 0.3,
    "latency_seconds": 2.1,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": 32.0,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": 802.1433,
    "terminalbench_hard": 0.068,
    "tau2": 0.494,
    "lcr": 57.7,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": 9.1,
    "gpqa": 72.6,
    "scicode": 36.5,
    "ifbench": 41.9,
    "aime25": 84.7,
    "critpt": 0.0,
    "mmmu_pro": null,
    "livecodebench": 76.5,
    "sources": [
      "Artificial Analysis"
    ],
    "source_count": 1,
    "data_tier": 2,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "Artificial Analysis",
        "scraped_at": "2026-02-27T11:07:24.919122+00:00",
        "confidence": 1.0,
        "raw_name": "Seed-OSS-36B-Instruct",
        "raw_scores": {
          "intelligence_score": 25.16,
          "coding_score": 16.7,
          "blended_cost_per_1m": 0.3,
          "latency_seconds": 2.1,
          "tokens_per_second": 32.0,
          "context_window": 512000,
          "gdpval": 802.1433235124343,
          "terminalbench_hard": 0.068,
          "tau2": 0.494,
          "lcr": 57.7,
          "hle": 9.1,
          "gpqa": 72.6,
          "scicode": 36.5,
          "ifbench": 41.9,
          "aime25": 84.7,
          "critpt": 0.0,
          "livecodebench": 76.5
        }
      }
    ],
    "benchmark_breakdown": {
      "Artificial Analysis": {
        "intelligence_score": 25.16,
        "coding_score": 16.7,
        "gdpval": 802.1433235124343,
        "terminalbench_hard": 0.068,
        "tau2": 0.494,
        "lcr": 57.7,
        "hle": 9.1,
        "gpqa": 72.6,
        "scicode": 36.5,
        "ifbench": 41.9,
        "aime25": 84.7,
        "critpt": 0.0,
        "livecodebench": 76.5
      }
    },
    "confidence_score": 1.0,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "grok-420-beta1",
    "canonical_name": "grok-4.20-beta1",
    "model_name": "grok-4.20-beta1",
    "aliases": [
      "grok-4.20-beta1"
    ],
    "provider": "xAI",
    "context_window": null,
    "open_source": null,
    "arena_votes": 3818,
    "license_type": "Proprietary",
    "creator": null,
    "intelligence_score": null,
    "coding_score": null,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 1495.42,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": null,
    "tau2": null,
    "lcr": null,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": null,
    "gpqa": null,
    "scicode": null,
    "ifbench": null,
    "aime25": null,
    "critpt": null,
    "mmmu_pro": null,
    "livecodebench": null,
    "sources": [
      "LMSYS Chatbot Arena"
    ],
    "source_count": 1,
    "data_tier": 3,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.891317+00:00",
        "confidence": 1.0,
        "raw_name": "grok-4.20-beta1",
        "raw_scores": {
          "arena_elo": 1495.42,
          "arena_votes": 3818
        }
      }
    ],
    "benchmark_breakdown": {
      "LMSYS Chatbot Arena": {
        "arena_elo": 1495.42
      }
    },
    "confidence_score": 1.0,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "gemini-3-flash",
    "canonical_name": "gemini-3-flash (thinking-minimal)",
    "model_name": "gemini-3-flash (thinking-minimal)",
    "aliases": [
      "gemini-3-flash",
      "gemini-3-flash (thinking-minimal)"
    ],
    "provider": "Google",
    "context_window": null,
    "open_source": null,
    "arena_votes": 29334,
    "license_type": "Proprietary",
    "creator": null,
    "intelligence_score": null,
    "coding_score": null,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 1467.19,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": null,
    "tau2": null,
    "lcr": null,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": null,
    "gpqa": null,
    "scicode": null,
    "ifbench": null,
    "aime25": null,
    "critpt": null,
    "mmmu_pro": null,
    "livecodebench": null,
    "sources": [
      "LMSYS Chatbot Arena"
    ],
    "source_count": 1,
    "data_tier": 3,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.891358+00:00",
        "confidence": 1.0,
        "raw_name": "gemini-3-flash",
        "raw_scores": {
          "arena_elo": 1473.23,
          "arena_votes": 29334
        }
      },
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.891453+00:00",
        "confidence": 1.0,
        "raw_name": "gemini-3-flash (thinking-minimal)",
        "raw_scores": {
          "arena_elo": 1461.15,
          "arena_votes": 20672
        }
      }
    ],
    "benchmark_breakdown": {
      "LMSYS Chatbot Arena": {
        "arena_elo": 1473.23
      }
    },
    "confidence_score": 1.0,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "grok-41",
    "canonical_name": "grok-4.1-thinking",
    "model_name": "grok-4.1-thinking",
    "aliases": [
      "grok-4.1",
      "grok-4.1-thinking"
    ],
    "provider": "xAI",
    "context_window": null,
    "open_source": null,
    "arena_votes": 41700,
    "license_type": "Proprietary",
    "creator": null,
    "intelligence_score": null,
    "coding_score": null,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 1467.7,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": null,
    "tau2": null,
    "lcr": null,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": null,
    "gpqa": null,
    "scicode": null,
    "ifbench": null,
    "aime25": null,
    "critpt": null,
    "mmmu_pro": null,
    "livecodebench": null,
    "sources": [
      "LMSYS Chatbot Arena"
    ],
    "source_count": 1,
    "data_tier": 3,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.891374+00:00",
        "confidence": 1.0,
        "raw_name": "grok-4.1-thinking",
        "raw_scores": {
          "arena_elo": 1472.97,
          "arena_votes": 37474
        }
      },
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.891439+00:00",
        "confidence": 1.0,
        "raw_name": "grok-4.1",
        "raw_scores": {
          "arena_elo": 1462.43,
          "arena_votes": 41700
        }
      }
    ],
    "benchmark_breakdown": {
      "LMSYS Chatbot Arena": {
        "arena_elo": 1472.97
      }
    },
    "confidence_score": 1.0,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "dola-seed-20",
    "canonical_name": "dola-seed-2.0-preview",
    "model_name": "dola-seed-2.0-preview",
    "aliases": [
      "dola-seed-2.0-preview"
    ],
    "provider": "Bytedance",
    "context_window": null,
    "open_source": null,
    "arena_votes": 4620,
    "license_type": "Proprietary",
    "creator": null,
    "intelligence_score": null,
    "coding_score": null,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 1469.86,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": null,
    "tau2": null,
    "lcr": null,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": null,
    "gpqa": null,
    "scicode": null,
    "ifbench": null,
    "aime25": null,
    "critpt": null,
    "mmmu_pro": null,
    "livecodebench": null,
    "sources": [
      "LMSYS Chatbot Arena"
    ],
    "source_count": 1,
    "data_tier": 3,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.891408+00:00",
        "confidence": 1.0,
        "raw_name": "dola-seed-2.0-preview",
        "raw_scores": {
          "arena_elo": 1469.86,
          "arena_votes": 4620
        }
      }
    ],
    "benchmark_breakdown": {
      "LMSYS Chatbot Arena": {
        "arena_elo": 1469.86
      }
    },
    "confidence_score": 1.0,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "ernie-50",
    "canonical_name": "ernie-5.0-0110",
    "model_name": "ernie-5.0-0110",
    "aliases": [
      "ernie-5.0-0110"
    ],
    "provider": "Baidu",
    "context_window": null,
    "open_source": null,
    "arena_votes": 13833,
    "license_type": "Proprietary",
    "creator": null,
    "intelligence_score": null,
    "coding_score": null,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 1452.82,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": null,
    "tau2": null,
    "lcr": null,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": null,
    "gpqa": null,
    "scicode": null,
    "ifbench": null,
    "aime25": null,
    "critpt": null,
    "mmmu_pro": null,
    "livecodebench": null,
    "sources": [
      "LMSYS Chatbot Arena"
    ],
    "source_count": 1,
    "data_tier": 3,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.891520+00:00",
        "confidence": 1.0,
        "raw_name": "ernie-5.0-0110",
        "raw_scores": {
          "arena_elo": 1452.82,
          "arena_votes": 13833
        }
      }
    ],
    "benchmark_breakdown": {
      "LMSYS Chatbot Arena": {
        "arena_elo": 1452.82
      }
    },
    "confidence_score": 1.0,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "claude-sonnet-45",
    "canonical_name": "claude-sonnet-4.5-20250929-thinking-32k",
    "model_name": "claude-sonnet-4.5-20250929-thinking-32k",
    "aliases": [
      "claude-sonnet-4.5-20250929",
      "claude-sonnet-4.5-20250929-thinking-32k"
    ],
    "provider": "Anthropic",
    "context_window": null,
    "open_source": null,
    "arena_votes": 48912,
    "license_type": "Proprietary",
    "creator": null,
    "intelligence_score": null,
    "coding_score": null,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 1449.755,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": null,
    "tau2": null,
    "lcr": null,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": null,
    "gpqa": null,
    "scicode": null,
    "ifbench": null,
    "aime25": null,
    "critpt": null,
    "mmmu_pro": null,
    "livecodebench": null,
    "sources": [
      "LMSYS Chatbot Arena"
    ],
    "source_count": 1,
    "data_tier": 3,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.891550+00:00",
        "confidence": 1.0,
        "raw_name": "claude-sonnet-4.5-20250929-thinking-32k",
        "raw_scores": {
          "arena_elo": 1449.92,
          "arena_votes": 48912
        }
      },
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.891566+00:00",
        "confidence": 1.0,
        "raw_name": "claude-sonnet-4.5-20250929",
        "raw_scores": {
          "arena_elo": 1449.59,
          "arena_votes": 46720
        }
      }
    ],
    "benchmark_breakdown": {
      "LMSYS Chatbot Arena": {
        "arena_elo": 1449.92
      }
    },
    "confidence_score": 1.0,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "ernie-50-preview",
    "canonical_name": "ernie-5.0-preview-1203",
    "model_name": "ernie-5.0-preview-1203",
    "aliases": [
      "ernie-5.0-preview-1022",
      "ernie-5.0-preview-1203"
    ],
    "provider": "Baidu",
    "context_window": null,
    "open_source": null,
    "arena_votes": 9725,
    "license_type": "Proprietary",
    "creator": null,
    "intelligence_score": null,
    "coding_score": null,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 1433.805,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": null,
    "tau2": null,
    "lcr": null,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": null,
    "gpqa": null,
    "scicode": null,
    "ifbench": null,
    "aime25": null,
    "critpt": null,
    "mmmu_pro": null,
    "livecodebench": null,
    "sources": [
      "LMSYS Chatbot Arena"
    ],
    "source_count": 1,
    "data_tier": 3,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.891591+00:00",
        "confidence": 1.0,
        "raw_name": "ernie-5.0-preview-1203",
        "raw_scores": {
          "arena_elo": 1449.09,
          "arena_votes": 9725
        }
      },
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.891949+00:00",
        "confidence": 1.0,
        "raw_name": "ernie-5.0-preview-1022",
        "raw_scores": {
          "arena_elo": 1418.52,
          "arena_votes": 4561
        }
      }
    ],
    "benchmark_breakdown": {
      "LMSYS Chatbot Arena": {
        "arena_elo": 1449.09
      }
    },
    "confidence_score": 1.0,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "gpt-45-preview",
    "canonical_name": "gpt-4.5-preview-2025-02-27",
    "model_name": "gpt-4.5-preview-2025-02-27",
    "aliases": [
      "gpt-4.5-preview-2025-02-27"
    ],
    "provider": "OpenAI",
    "context_window": null,
    "open_source": null,
    "arena_votes": 14549,
    "license_type": "Proprietary",
    "creator": null,
    "intelligence_score": null,
    "coding_score": null,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 1444.26,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": null,
    "tau2": null,
    "lcr": null,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": null,
    "gpqa": null,
    "scicode": null,
    "ifbench": null,
    "aime25": null,
    "critpt": null,
    "mmmu_pro": null,
    "livecodebench": null,
    "sources": [
      "LMSYS Chatbot Arena"
    ],
    "source_count": 1,
    "data_tier": 3,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.891635+00:00",
        "confidence": 1.0,
        "raw_name": "gpt-4.5-preview-2025-02-27",
        "raw_scores": {
          "arena_elo": 1444.26,
          "arena_votes": 14549
        }
      }
    ],
    "benchmark_breakdown": {
      "LMSYS Chatbot Arena": {
        "arena_elo": 1444.26
      }
    },
    "confidence_score": 1.0,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "kimi-k25-instant",
    "canonical_name": "kimi-k2.5-instant",
    "model_name": "kimi-k2.5-instant",
    "aliases": [
      "kimi-k2.5-instant"
    ],
    "provider": "Moonshot",
    "context_window": null,
    "open_source": null,
    "arena_votes": 7125,
    "license_type": "Modified MIT",
    "creator": null,
    "intelligence_score": null,
    "coding_score": null,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 1434.17,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": null,
    "tau2": null,
    "lcr": null,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": null,
    "gpqa": null,
    "scicode": null,
    "ifbench": null,
    "aime25": null,
    "critpt": null,
    "mmmu_pro": null,
    "livecodebench": null,
    "sources": [
      "LMSYS Chatbot Arena"
    ],
    "source_count": 1,
    "data_tier": 3,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.891723+00:00",
        "confidence": 1.0,
        "raw_name": "kimi-k2.5-instant",
        "raw_scores": {
          "arena_elo": 1434.17,
          "arena_votes": 7125
        }
      }
    ],
    "benchmark_breakdown": {
      "LMSYS Chatbot Arena": {
        "arena_elo": 1434.17
      }
    },
    "confidence_score": 1.0,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "kimi-k2-turbo",
    "canonical_name": "kimi-k2-thinking-turbo",
    "model_name": "kimi-k2-thinking-turbo",
    "aliases": [
      "kimi-k2-thinking-turbo"
    ],
    "provider": "Moonshot",
    "context_window": null,
    "open_source": null,
    "arena_votes": 36099,
    "license_type": "Modified MIT",
    "creator": null,
    "intelligence_score": null,
    "coding_score": null,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 1428.7,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": null,
    "tau2": null,
    "lcr": null,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": null,
    "gpqa": null,
    "scicode": null,
    "ifbench": null,
    "aime25": null,
    "critpt": null,
    "mmmu_pro": null,
    "livecodebench": null,
    "sources": [
      "LMSYS Chatbot Arena"
    ],
    "source_count": 1,
    "data_tier": 3,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.891795+00:00",
        "confidence": 1.0,
        "raw_name": "kimi-k2-thinking-turbo",
        "raw_scores": {
          "arena_elo": 1428.7,
          "arena_votes": 36099
        }
      }
    ],
    "benchmark_breakdown": {
      "LMSYS Chatbot Arena": {
        "arena_elo": 1428.7
      }
    },
    "confidence_score": 1.0,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "qwen3-235b",
    "canonical_name": "qwen3-235b-a22b-instruct-2507",
    "model_name": "qwen3-235b-a22b-instruct-2507",
    "aliases": [
      "qwen3-235b-a22b-instruct-2507",
      "qwen3-235b-a22b-no-thinking"
    ],
    "provider": "Alibaba",
    "context_window": null,
    "open_source": null,
    "arena_votes": 71551,
    "license_type": "Apache 2.0",
    "creator": null,
    "intelligence_score": null,
    "coding_score": null,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 1412.025,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": null,
    "tau2": null,
    "lcr": null,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": null,
    "gpqa": null,
    "scicode": null,
    "ifbench": null,
    "aime25": null,
    "critpt": null,
    "mmmu_pro": null,
    "livecodebench": null,
    "sources": [
      "LMSYS Chatbot Arena"
    ],
    "source_count": 1,
    "data_tier": 3,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.891885+00:00",
        "confidence": 1.0,
        "raw_name": "qwen3-235b-a22b-instruct-2507",
        "raw_scores": {
          "arena_elo": 1422.1,
          "arena_votes": 71551
        }
      },
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.892231+00:00",
        "confidence": 1.0,
        "raw_name": "qwen3-235b-a22b-no-thinking",
        "raw_scores": {
          "arena_elo": 1401.95,
          "arena_votes": 39295
        }
      }
    ],
    "benchmark_breakdown": {
      "LMSYS Chatbot Arena": {
        "arena_elo": 1422.1
      }
    },
    "confidence_score": 1.0,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "amazon-nova-experimental-chat-26-01-10",
    "canonical_name": "amazon-nova-experimental-chat-26-01-10",
    "model_name": "amazon-nova-experimental-chat-26-01-10",
    "aliases": [
      "amazon-nova-experimental-chat-26-01-10"
    ],
    "provider": "Amazon",
    "context_window": null,
    "open_source": null,
    "arena_votes": 3421,
    "license_type": "Proprietary",
    "creator": null,
    "intelligence_score": null,
    "coding_score": null,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 1415.62,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": null,
    "tau2": null,
    "lcr": null,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": null,
    "gpqa": null,
    "scicode": null,
    "ifbench": null,
    "aime25": null,
    "critpt": null,
    "mmmu_pro": null,
    "livecodebench": null,
    "sources": [
      "LMSYS Chatbot Arena"
    ],
    "source_count": 1,
    "data_tier": 3,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.892037+00:00",
        "confidence": 1.0,
        "raw_name": "amazon-nova-experimental-chat-26-01-10",
        "raw_scores": {
          "arena_elo": 1415.62,
          "arena_votes": 3421
        }
      }
    ],
    "benchmark_breakdown": {
      "LMSYS Chatbot Arena": {
        "arena_elo": 1415.62
      }
    },
    "confidence_score": 1.0,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "longcat-flash-chat",
    "canonical_name": "longcat-flash-chat",
    "model_name": "longcat-flash-chat",
    "aliases": [
      "longcat-flash-chat"
    ],
    "provider": "Meituan",
    "context_window": null,
    "open_source": null,
    "arena_votes": 11486,
    "license_type": "MIT",
    "creator": null,
    "intelligence_score": null,
    "coding_score": null,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 1399.96,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": null,
    "tau2": null,
    "lcr": null,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": null,
    "gpqa": null,
    "scicode": null,
    "ifbench": null,
    "aime25": null,
    "critpt": null,
    "mmmu_pro": null,
    "livecodebench": null,
    "sources": [
      "LMSYS Chatbot Arena"
    ],
    "source_count": 1,
    "data_tier": 3,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.892283+00:00",
        "confidence": 1.0,
        "raw_name": "longcat-flash-chat",
        "raw_scores": {
          "arena_elo": 1399.96,
          "arena_votes": 11486
        }
      }
    ],
    "benchmark_breakdown": {
      "LMSYS Chatbot Arena": {
        "arena_elo": 1399.96
      }
    },
    "confidence_score": 1.0,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "claude-sonnet-4",
    "canonical_name": "claude-sonnet-4-20250514-thinking-32k",
    "model_name": "claude-sonnet-4-20250514-thinking-32k",
    "aliases": [
      "claude-sonnet-4-20250514",
      "claude-sonnet-4-20250514-thinking-32k"
    ],
    "provider": "Anthropic",
    "context_window": null,
    "open_source": null,
    "arena_votes": 41365,
    "license_type": "Proprietary",
    "creator": null,
    "intelligence_score": null,
    "coding_score": null,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 1394.675,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": null,
    "tau2": null,
    "lcr": null,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": null,
    "gpqa": null,
    "scicode": null,
    "ifbench": null,
    "aime25": null,
    "critpt": null,
    "mmmu_pro": null,
    "livecodebench": null,
    "sources": [
      "LMSYS Chatbot Arena"
    ],
    "source_count": 1,
    "data_tier": 3,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.892295+00:00",
        "confidence": 1.0,
        "raw_name": "claude-sonnet-4-20250514-thinking-32k",
        "raw_scores": {
          "arena_elo": 1399.79,
          "arena_votes": 35975
        }
      },
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.892447+00:00",
        "confidence": 1.0,
        "raw_name": "claude-sonnet-4-20250514",
        "raw_scores": {
          "arena_elo": 1389.56,
          "arena_votes": 41365
        }
      }
    ],
    "benchmark_breakdown": {
      "LMSYS Chatbot Arena": {
        "arena_elo": 1399.79
      }
    },
    "confidence_score": 1.0,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "qwen3-235b-a22b-thinking-2507",
    "canonical_name": "qwen3-235b-a22b-thinking-2507",
    "model_name": "qwen3-235b-a22b-thinking-2507",
    "aliases": [
      "qwen3-235b-a22b-thinking-2507"
    ],
    "provider": "Alibaba",
    "context_window": null,
    "open_source": null,
    "arena_votes": 9186,
    "license_type": "Apache 2.0",
    "creator": null,
    "intelligence_score": null,
    "coding_score": null,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 1398.63,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": null,
    "tau2": null,
    "lcr": null,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": null,
    "gpqa": null,
    "scicode": null,
    "ifbench": null,
    "aime25": null,
    "critpt": null,
    "mmmu_pro": null,
    "livecodebench": null,
    "sources": [
      "LMSYS Chatbot Arena"
    ],
    "source_count": 1,
    "data_tier": 3,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.892308+00:00",
        "confidence": 1.0,
        "raw_name": "qwen3-235b-a22b-thinking-2507",
        "raw_scores": {
          "arena_elo": 1398.63,
          "arena_votes": 9186
        }
      }
    ],
    "benchmark_breakdown": {
      "LMSYS Chatbot Arena": {
        "arena_elo": 1398.63
      }
    },
    "confidence_score": 1.0,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "nova-experimental",
    "canonical_name": "amazon-nova-experimental-chat-12-10",
    "model_name": "amazon-nova-experimental-chat-12-10",
    "aliases": [
      "amazon-nova-experimental-chat-12-10"
    ],
    "provider": "Amazon",
    "context_window": null,
    "open_source": null,
    "arena_votes": 3699,
    "license_type": "Proprietary",
    "creator": null,
    "intelligence_score": null,
    "coding_score": null,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 1394.64,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": null,
    "tau2": null,
    "lcr": null,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": null,
    "gpqa": null,
    "scicode": null,
    "ifbench": null,
    "aime25": null,
    "critpt": null,
    "mmmu_pro": null,
    "livecodebench": null,
    "sources": [
      "LMSYS Chatbot Arena"
    ],
    "source_count": 1,
    "data_tier": 3,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.892348+00:00",
        "confidence": 1.0,
        "raw_name": "amazon-nova-experimental-chat-12-10",
        "raw_scores": {
          "arena_elo": 1394.64,
          "arena_votes": 3699
        }
      }
    ],
    "benchmark_breakdown": {
      "LMSYS Chatbot Arena": {
        "arena_elo": 1394.64
      }
    },
    "confidence_score": 1.0,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "hunyuan-vision-15",
    "canonical_name": "hunyuan-vision-1.5-thinking",
    "model_name": "hunyuan-vision-1.5-thinking",
    "aliases": [
      "hunyuan-vision-1.5-thinking"
    ],
    "provider": "Tencent",
    "context_window": null,
    "open_source": null,
    "arena_votes": 2216,
    "license_type": "Proprietary",
    "creator": null,
    "intelligence_score": null,
    "coding_score": null,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 1393.73,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": null,
    "tau2": null,
    "lcr": null,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": null,
    "gpqa": null,
    "scicode": null,
    "ifbench": null,
    "aime25": null,
    "critpt": null,
    "mmmu_pro": null,
    "livecodebench": null,
    "sources": [
      "LMSYS Chatbot Arena"
    ],
    "source_count": 1,
    "data_tier": 3,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.892373+00:00",
        "confidence": 1.0,
        "raw_name": "hunyuan-vision-1.5-thinking",
        "raw_scores": {
          "arena_elo": 1393.73,
          "arena_votes": 2216
        }
      }
    ],
    "benchmark_breakdown": {
      "LMSYS Chatbot Arena": {
        "arena_elo": 1393.73
      }
    },
    "confidence_score": 1.0,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "mai-1-preview",
    "canonical_name": "mai-1-preview",
    "model_name": "mai-1-preview",
    "aliases": [
      "mai-1-preview"
    ],
    "provider": "Microsoft AI",
    "context_window": null,
    "open_source": null,
    "arena_votes": 18018,
    "license_type": "Proprietary",
    "creator": null,
    "intelligence_score": null,
    "coding_score": null,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 1392.0,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": null,
    "tau2": null,
    "lcr": null,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": null,
    "gpqa": null,
    "scicode": null,
    "ifbench": null,
    "aime25": null,
    "critpt": null,
    "mmmu_pro": null,
    "livecodebench": null,
    "sources": [
      "LMSYS Chatbot Arena"
    ],
    "source_count": 1,
    "data_tier": 3,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.892385+00:00",
        "confidence": 1.0,
        "raw_name": "mai-1-preview",
        "raw_scores": {
          "arena_elo": 1392.0,
          "arena_votes": 18018
        }
      }
    ],
    "benchmark_breakdown": {
      "LMSYS Chatbot Arena": {
        "arena_elo": 1392.0
      }
    },
    "confidence_score": 1.0,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "step-35-flash",
    "canonical_name": "step-3.5-flash",
    "model_name": "step-3.5-flash",
    "aliases": [
      "step-3.5-flash"
    ],
    "provider": "StepFun",
    "context_window": null,
    "open_source": null,
    "arena_votes": 8624,
    "license_type": "Apache 2.0",
    "creator": null,
    "intelligence_score": null,
    "coding_score": null,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 1389.7,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": null,
    "tau2": null,
    "lcr": null,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": null,
    "gpqa": null,
    "scicode": null,
    "ifbench": null,
    "aime25": null,
    "critpt": null,
    "mmmu_pro": null,
    "livecodebench": null,
    "sources": [
      "LMSYS Chatbot Arena"
    ],
    "source_count": 1,
    "data_tier": 3,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.892435+00:00",
        "confidence": 1.0,
        "raw_name": "step-3.5-flash",
        "raw_scores": {
          "arena_elo": 1389.7,
          "arena_votes": 8624
        }
      }
    ],
    "benchmark_breakdown": {
      "LMSYS Chatbot Arena": {
        "arena_elo": 1389.7
      }
    },
    "confidence_score": 1.0,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "claude-37-sonnet-20250219-thinking-32k",
    "canonical_name": "claude-3.7-sonnet-20250219-thinking-32k",
    "model_name": "claude-3.7-sonnet-20250219-thinking-32k",
    "aliases": [
      "claude-3.7-sonnet-20250219-thinking-32k"
    ],
    "provider": "Anthropic",
    "context_window": null,
    "open_source": null,
    "arena_votes": 39731,
    "license_type": "Proprietary",
    "creator": null,
    "intelligence_score": null,
    "coding_score": null,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 1388.19,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": null,
    "tau2": null,
    "lcr": null,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": null,
    "gpqa": null,
    "scicode": null,
    "ifbench": null,
    "aime25": null,
    "critpt": null,
    "mmmu_pro": null,
    "livecodebench": null,
    "sources": [
      "LMSYS Chatbot Arena"
    ],
    "source_count": 1,
    "data_tier": 3,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.892570+00:00",
        "confidence": 1.0,
        "raw_name": "claude-3.7-sonnet-20250219-thinking-32k",
        "raw_scores": {
          "arena_elo": 1388.19,
          "arena_votes": 39731
        }
      }
    ],
    "benchmark_breakdown": {
      "LMSYS Chatbot Arena": {
        "arena_elo": 1388.19
      }
    },
    "confidence_score": 1.0,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "hunyuan-t1-20250711",
    "canonical_name": "hunyuan-t1-20250711",
    "model_name": "hunyuan-t1-20250711",
    "aliases": [
      "hunyuan-t1-20250711"
    ],
    "provider": "Tencent",
    "context_window": null,
    "open_source": null,
    "arena_votes": 4767,
    "license_type": "Proprietary",
    "creator": null,
    "intelligence_score": null,
    "coding_score": null,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 1386.69,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": null,
    "tau2": null,
    "lcr": null,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": null,
    "gpqa": null,
    "scicode": null,
    "ifbench": null,
    "aime25": null,
    "critpt": null,
    "mmmu_pro": null,
    "livecodebench": null,
    "sources": [
      "LMSYS Chatbot Arena"
    ],
    "source_count": 1,
    "data_tier": 3,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.892614+00:00",
        "confidence": 1.0,
        "raw_name": "hunyuan-t1-20250711",
        "raw_scores": {
          "arena_elo": 1386.69,
          "arena_votes": 4767
        }
      }
    ],
    "benchmark_breakdown": {
      "LMSYS Chatbot Arena": {
        "arena_elo": 1386.69
      }
    },
    "confidence_score": 1.0,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "minimax-m21-preview",
    "canonical_name": "minimax-m2.1-preview",
    "model_name": "minimax-m2.1-preview",
    "aliases": [
      "minimax-m2.1-preview"
    ],
    "provider": "MiniMax",
    "context_window": null,
    "open_source": null,
    "arena_votes": 17092,
    "license_type": "MIT",
    "creator": null,
    "intelligence_score": null,
    "coding_score": null,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 1385.37,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": null,
    "tau2": null,
    "lcr": null,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": null,
    "gpqa": null,
    "scicode": null,
    "ifbench": null,
    "aime25": null,
    "critpt": null,
    "mmmu_pro": null,
    "livecodebench": null,
    "sources": [
      "LMSYS Chatbot Arena"
    ],
    "source_count": 1,
    "data_tier": 3,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.892639+00:00",
        "confidence": 1.0,
        "raw_name": "minimax-m2.1-preview",
        "raw_scores": {
          "arena_elo": 1385.37,
          "arena_votes": 17092
        }
      }
    ],
    "benchmark_breakdown": {
      "LMSYS Chatbot Arena": {
        "arena_elo": 1385.37
      }
    },
    "confidence_score": 1.0,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "mistral-medium-2505",
    "canonical_name": "mistral-medium-2505",
    "model_name": "mistral-medium-2505",
    "aliases": [
      "mistral-medium-2505"
    ],
    "provider": "Mistral",
    "context_window": null,
    "open_source": null,
    "arena_votes": 34386,
    "license_type": "Proprietary",
    "creator": null,
    "intelligence_score": null,
    "coding_score": null,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 1384.53,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": null,
    "tau2": null,
    "lcr": null,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": null,
    "gpqa": null,
    "scicode": null,
    "ifbench": null,
    "aime25": null,
    "critpt": null,
    "mmmu_pro": null,
    "livecodebench": null,
    "sources": [
      "LMSYS Chatbot Arena"
    ],
    "source_count": 1,
    "data_tier": 3,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.892651+00:00",
        "confidence": 1.0,
        "raw_name": "mistral-medium-2505",
        "raw_scores": {
          "arena_elo": 1384.53,
          "arena_votes": 34386
        }
      }
    ],
    "benchmark_breakdown": {
      "LMSYS Chatbot Arena": {
        "arena_elo": 1384.53
      }
    },
    "confidence_score": 1.0,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "qwen3-30b-a3b-instruct-2507",
    "canonical_name": "qwen3-30b-a3b-instruct-2507",
    "model_name": "qwen3-30b-a3b-instruct-2507",
    "aliases": [
      "qwen3-30b-a3b-instruct-2507"
    ],
    "provider": "Alibaba",
    "context_window": null,
    "open_source": null,
    "arena_votes": 23940,
    "license_type": "Apache 2.0",
    "creator": null,
    "intelligence_score": null,
    "coding_score": null,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 1383.75,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": null,
    "tau2": null,
    "lcr": null,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": null,
    "gpqa": null,
    "scicode": null,
    "ifbench": null,
    "aime25": null,
    "critpt": null,
    "mmmu_pro": null,
    "livecodebench": null,
    "sources": [
      "LMSYS Chatbot Arena"
    ],
    "source_count": 1,
    "data_tier": 3,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.892664+00:00",
        "confidence": 1.0,
        "raw_name": "qwen3-30b-a3b-instruct-2507",
        "raw_scores": {
          "arena_elo": 1383.75,
          "arena_votes": 23940
        }
      }
    ],
    "benchmark_breakdown": {
      "LMSYS Chatbot Arena": {
        "arena_elo": 1383.75
      }
    },
    "confidence_score": 1.0,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "hunyuan-turbos-20250416",
    "canonical_name": "hunyuan-turbos-20250416",
    "model_name": "hunyuan-turbos-20250416",
    "aliases": [
      "hunyuan-turbos-20250416"
    ],
    "provider": "Tencent",
    "context_window": null,
    "open_source": null,
    "arena_votes": 11000,
    "license_type": "Proprietary",
    "creator": null,
    "intelligence_score": null,
    "coding_score": null,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 1382.57,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": null,
    "tau2": null,
    "lcr": null,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": null,
    "gpqa": null,
    "scicode": null,
    "ifbench": null,
    "aime25": null,
    "critpt": null,
    "mmmu_pro": null,
    "livecodebench": null,
    "sources": [
      "LMSYS Chatbot Arena"
    ],
    "source_count": 1,
    "data_tier": 3,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.892677+00:00",
        "confidence": 1.0,
        "raw_name": "hunyuan-turbos-20250416",
        "raw_scores": {
          "arena_elo": 1382.57,
          "arena_votes": 11000
        }
      }
    ],
    "benchmark_breakdown": {
      "LMSYS Chatbot Arena": {
        "arena_elo": 1382.57
      }
    },
    "confidence_score": 1.0,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "gemini-25-flash-lite-preview-09-2025-no-thinking",
    "canonical_name": "gemini-2.5-flash-lite-preview-09-2025-no-thinking",
    "model_name": "gemini-2.5-flash-lite-preview-09-2025-no-thinking",
    "aliases": [
      "gemini-2.5-flash-lite-preview-09-2025-no-thinking"
    ],
    "provider": "Google",
    "context_window": null,
    "open_source": null,
    "arena_votes": 46858,
    "license_type": "Proprietary",
    "creator": null,
    "intelligence_score": null,
    "coding_score": null,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 1379.57,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": null,
    "tau2": null,
    "lcr": null,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": null,
    "gpqa": null,
    "scicode": null,
    "ifbench": null,
    "aime25": null,
    "critpt": null,
    "mmmu_pro": null,
    "livecodebench": null,
    "sources": [
      "LMSYS Chatbot Arena"
    ],
    "source_count": 1,
    "data_tier": 3,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.892702+00:00",
        "confidence": 1.0,
        "raw_name": "gemini-2.5-flash-lite-preview-09-2025-no-thinking",
        "raw_scores": {
          "arena_elo": 1379.57,
          "arena_votes": 46858
        }
      }
    ],
    "benchmark_breakdown": {
      "LMSYS Chatbot Arena": {
        "arena_elo": 1379.57
      }
    },
    "confidence_score": 1.0,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "trinity-large",
    "canonical_name": "trinity-large",
    "model_name": "trinity-large",
    "aliases": [
      "trinity-large"
    ],
    "provider": "Arcee AI",
    "context_window": null,
    "open_source": null,
    "arena_votes": 2166,
    "license_type": "Apache 2.0",
    "creator": null,
    "intelligence_score": null,
    "coding_score": null,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 1375.1,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": null,
    "tau2": null,
    "lcr": null,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": null,
    "gpqa": null,
    "scicode": null,
    "ifbench": null,
    "aime25": null,
    "critpt": null,
    "mmmu_pro": null,
    "livecodebench": null,
    "sources": [
      "LMSYS Chatbot Arena"
    ],
    "source_count": 1,
    "data_tier": 3,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.892729+00:00",
        "confidence": 1.0,
        "raw_name": "trinity-large",
        "raw_scores": {
          "arena_elo": 1375.1,
          "arena_votes": 2166
        }
      }
    ],
    "benchmark_breakdown": {
      "LMSYS Chatbot Arena": {
        "arena_elo": 1375.1
      }
    },
    "confidence_score": 1.0,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "gemini-25-flash-lite-preview-06-17-thinking",
    "canonical_name": "gemini-2.5-flash-lite-preview-06-17-thinking",
    "model_name": "gemini-2.5-flash-lite-preview-06-17-thinking",
    "aliases": [
      "gemini-2.5-flash-lite-preview-06-17-thinking"
    ],
    "provider": "Google",
    "context_window": null,
    "open_source": null,
    "arena_votes": 33674,
    "license_type": "Proprietary",
    "creator": null,
    "intelligence_score": null,
    "coding_score": null,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 1374.68,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": null,
    "tau2": null,
    "lcr": null,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": null,
    "gpqa": null,
    "scicode": null,
    "ifbench": null,
    "aime25": null,
    "critpt": null,
    "mmmu_pro": null,
    "livecodebench": null,
    "sources": [
      "LMSYS Chatbot Arena"
    ],
    "source_count": 1,
    "data_tier": 3,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.892755+00:00",
        "confidence": 1.0,
        "raw_name": "gemini-2.5-flash-lite-preview-06-17-thinking",
        "raw_scores": {
          "arena_elo": 1374.68,
          "arena_votes": 33674
        }
      }
    ],
    "benchmark_breakdown": {
      "LMSYS Chatbot Arena": {
        "arena_elo": 1374.68
      }
    },
    "confidence_score": 1.0,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "qwen3-next-80b-a3b-thinking",
    "canonical_name": "qwen3-next-80b-a3b-thinking",
    "model_name": "qwen3-next-80b-a3b-thinking",
    "aliases": [
      "qwen3-next-80b-a3b-thinking"
    ],
    "provider": "Alibaba",
    "context_window": null,
    "open_source": null,
    "arena_votes": 13767,
    "license_type": "Apache 2.0",
    "creator": null,
    "intelligence_score": null,
    "coding_score": null,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 1368.86,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": null,
    "tau2": null,
    "lcr": null,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": null,
    "gpqa": null,
    "scicode": null,
    "ifbench": null,
    "aime25": null,
    "critpt": null,
    "mmmu_pro": null,
    "livecodebench": null,
    "sources": [
      "LMSYS Chatbot Arena"
    ],
    "source_count": 1,
    "data_tier": 3,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.892828+00:00",
        "confidence": 1.0,
        "raw_name": "qwen3-next-80b-a3b-thinking",
        "raw_scores": {
          "arena_elo": 1368.86,
          "arena_votes": 13767
        }
      }
    ],
    "benchmark_breakdown": {
      "LMSYS Chatbot Arena": {
        "arena_elo": 1368.86
      }
    },
    "confidence_score": 1.0,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "minimax-m1",
    "canonical_name": "minimax-m1",
    "model_name": "minimax-m1",
    "aliases": [
      "minimax-m1"
    ],
    "provider": "MiniMax",
    "context_window": null,
    "open_source": null,
    "arena_votes": 36564,
    "license_type": "Apache 2.0",
    "creator": null,
    "intelligence_score": null,
    "coding_score": null,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 1367.33,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": null,
    "tau2": null,
    "lcr": null,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": null,
    "gpqa": null,
    "scicode": null,
    "ifbench": null,
    "aime25": null,
    "critpt": null,
    "mmmu_pro": null,
    "livecodebench": null,
    "sources": [
      "LMSYS Chatbot Arena"
    ],
    "source_count": 1,
    "data_tier": 3,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.892840+00:00",
        "confidence": 1.0,
        "raw_name": "minimax-m1",
        "raw_scores": {
          "arena_elo": 1367.33,
          "arena_votes": 36564
        }
      }
    ],
    "benchmark_breakdown": {
      "LMSYS Chatbot Arena": {
        "arena_elo": 1367.33
      }
    },
    "confidence_score": 1.0,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "amazon-nova-experimental-chat-11-10",
    "canonical_name": "amazon-nova-experimental-chat-11-10",
    "model_name": "amazon-nova-experimental-chat-11-10",
    "aliases": [
      "amazon-nova-experimental-chat-11-10"
    ],
    "provider": "Amazon",
    "context_window": null,
    "open_source": null,
    "arena_votes": 19082,
    "license_type": "Proprietary",
    "creator": null,
    "intelligence_score": null,
    "coding_score": null,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 1365.22,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": null,
    "tau2": null,
    "lcr": null,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": null,
    "gpqa": null,
    "scicode": null,
    "ifbench": null,
    "aime25": null,
    "critpt": null,
    "mmmu_pro": null,
    "livecodebench": null,
    "sources": [
      "LMSYS Chatbot Arena"
    ],
    "source_count": 1,
    "data_tier": 3,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.892865+00:00",
        "confidence": 1.0,
        "raw_name": "amazon-nova-experimental-chat-11-10",
        "raw_scores": {
          "arena_elo": 1365.22,
          "arena_votes": 19082
        }
      }
    ],
    "benchmark_breakdown": {
      "LMSYS Chatbot Arena": {
        "arena_elo": 1365.22
      }
    },
    "confidence_score": 1.0,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "gemma-3-27b-it",
    "canonical_name": "gemma-3-27b-it",
    "model_name": "gemma-3-27b-it",
    "aliases": [
      "gemma-3-27b-it"
    ],
    "provider": "Google",
    "context_window": null,
    "open_source": null,
    "arena_votes": 48453,
    "license_type": "Gemma",
    "creator": null,
    "intelligence_score": null,
    "coding_score": null,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 1365.18,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": null,
    "tau2": null,
    "lcr": null,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": null,
    "gpqa": null,
    "scicode": null,
    "ifbench": null,
    "aime25": null,
    "critpt": null,
    "mmmu_pro": null,
    "livecodebench": null,
    "sources": [
      "LMSYS Chatbot Arena"
    ],
    "source_count": 1,
    "data_tier": 3,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.892878+00:00",
        "confidence": 1.0,
        "raw_name": "gemma-3-27b-it",
        "raw_scores": {
          "arena_elo": 1365.18,
          "arena_votes": 48453
        }
      }
    ],
    "benchmark_breakdown": {
      "LMSYS Chatbot Arena": {
        "arena_elo": 1365.18
      }
    },
    "confidence_score": 1.0,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "o3-mini-high",
    "canonical_name": "o3-mini-high",
    "model_name": "o3-mini-high",
    "aliases": [
      "o3-mini-high"
    ],
    "provider": "OpenAI",
    "context_window": null,
    "open_source": null,
    "arena_votes": 18584,
    "license_type": "Proprietary",
    "creator": null,
    "intelligence_score": null,
    "coding_score": null,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 1363.97,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": null,
    "tau2": null,
    "lcr": null,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": null,
    "gpqa": null,
    "scicode": null,
    "ifbench": null,
    "aime25": null,
    "critpt": null,
    "mmmu_pro": null,
    "livecodebench": null,
    "sources": [
      "LMSYS Chatbot Arena"
    ],
    "source_count": 1,
    "data_tier": 3,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.892891+00:00",
        "confidence": 1.0,
        "raw_name": "o3-mini-high",
        "raw_scores": {
          "arena_elo": 1363.97,
          "arena_votes": 18584
        }
      }
    ],
    "benchmark_breakdown": {
      "LMSYS Chatbot Arena": {
        "arena_elo": 1363.97
      }
    },
    "confidence_score": 1.0,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "grok-3-mini-beta",
    "canonical_name": "grok-3-mini-beta",
    "model_name": "grok-3-mini-beta",
    "aliases": [
      "grok-3-mini-beta"
    ],
    "provider": "xAI",
    "context_window": null,
    "open_source": null,
    "arena_votes": 23615,
    "license_type": "Proprietary",
    "creator": null,
    "intelligence_score": null,
    "coding_score": null,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 1356.75,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": null,
    "tau2": null,
    "lcr": null,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": null,
    "gpqa": null,
    "scicode": null,
    "ifbench": null,
    "aime25": null,
    "critpt": null,
    "mmmu_pro": null,
    "livecodebench": null,
    "sources": [
      "LMSYS Chatbot Arena"
    ],
    "source_count": 1,
    "data_tier": 3,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.892941+00:00",
        "confidence": 1.0,
        "raw_name": "grok-3-mini-beta",
        "raw_scores": {
          "arena_elo": 1356.75,
          "arena_votes": 23615
        }
      }
    ],
    "benchmark_breakdown": {
      "LMSYS Chatbot Arena": {
        "arena_elo": 1356.75
      }
    },
    "confidence_score": 1.0,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "mistral-small-2506",
    "canonical_name": "mistral-small-2506",
    "model_name": "mistral-small-2506",
    "aliases": [
      "mistral-small-2506"
    ],
    "provider": "Mistral",
    "context_window": null,
    "open_source": null,
    "arena_votes": 18237,
    "license_type": "Apache 2.0",
    "creator": null,
    "intelligence_score": null,
    "coding_score": null,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 1356.02,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": null,
    "tau2": null,
    "lcr": null,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": null,
    "gpqa": null,
    "scicode": null,
    "ifbench": null,
    "aime25": null,
    "critpt": null,
    "mmmu_pro": null,
    "livecodebench": null,
    "sources": [
      "LMSYS Chatbot Arena"
    ],
    "source_count": 1,
    "data_tier": 3,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.892966+00:00",
        "confidence": 1.0,
        "raw_name": "mistral-small-2506",
        "raw_scores": {
          "arena_elo": 1356.02,
          "arena_votes": 18237
        }
      }
    ],
    "benchmark_breakdown": {
      "LMSYS Chatbot Arena": {
        "arena_elo": 1356.02
      }
    },
    "confidence_score": 1.0,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "gemini-20-flash-lite-preview-02-05",
    "canonical_name": "gemini-2.0-flash-lite-preview-02-05",
    "model_name": "gemini-2.0-flash-lite-preview-02-05",
    "aliases": [
      "gemini-2.0-flash-lite-preview-02-05"
    ],
    "provider": "Google",
    "context_window": null,
    "open_source": null,
    "arena_votes": 24951,
    "license_type": "Proprietary",
    "creator": null,
    "intelligence_score": null,
    "coding_score": null,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 1353.28,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": null,
    "tau2": null,
    "lcr": null,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": null,
    "gpqa": null,
    "scicode": null,
    "ifbench": null,
    "aime25": null,
    "critpt": null,
    "mmmu_pro": null,
    "livecodebench": null,
    "sources": [
      "LMSYS Chatbot Arena"
    ],
    "source_count": 1,
    "data_tier": 3,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.892991+00:00",
        "confidence": 1.0,
        "raw_name": "gemini-2.0-flash-lite-preview-02-05",
        "raw_scores": {
          "arena_elo": 1353.28,
          "arena_votes": 24951
        }
      }
    ],
    "benchmark_breakdown": {
      "LMSYS Chatbot Arena": {
        "arena_elo": 1353.28
      }
    },
    "confidence_score": 1.0,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "amazon-nova-experimental-chat-10-20",
    "canonical_name": "amazon-nova-experimental-chat-10-20",
    "model_name": "amazon-nova-experimental-chat-10-20",
    "aliases": [
      "amazon-nova-experimental-chat-10-20"
    ],
    "provider": "Amazon",
    "context_window": null,
    "open_source": null,
    "arena_votes": 11338,
    "license_type": "Proprietary",
    "creator": null,
    "intelligence_score": null,
    "coding_score": null,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 1350.29,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": null,
    "tau2": null,
    "lcr": null,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": null,
    "gpqa": null,
    "scicode": null,
    "ifbench": null,
    "aime25": null,
    "critpt": null,
    "mmmu_pro": null,
    "livecodebench": null,
    "sources": [
      "LMSYS Chatbot Arena"
    ],
    "source_count": 1,
    "data_tier": 3,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.893041+00:00",
        "confidence": 1.0,
        "raw_name": "amazon-nova-experimental-chat-10-20",
        "raw_scores": {
          "arena_elo": 1350.29,
          "arena_votes": 11338
        }
      }
    ],
    "benchmark_breakdown": {
      "LMSYS Chatbot Arena": {
        "arena_elo": 1350.29
      }
    },
    "confidence_score": 1.0,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "hunyuan-turbos-20250226",
    "canonical_name": "hunyuan-turbos-20250226",
    "model_name": "hunyuan-turbos-20250226",
    "aliases": [
      "hunyuan-turbos-20250226"
    ],
    "provider": "Tencent",
    "context_window": null,
    "open_source": null,
    "arena_votes": 2226,
    "license_type": "Proprietary",
    "creator": null,
    "intelligence_score": null,
    "coding_score": null,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 1348.79,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": null,
    "tau2": null,
    "lcr": null,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": null,
    "gpqa": null,
    "scicode": null,
    "ifbench": null,
    "aime25": null,
    "critpt": null,
    "mmmu_pro": null,
    "livecodebench": null,
    "sources": [
      "LMSYS Chatbot Arena"
    ],
    "source_count": 1,
    "data_tier": 3,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.893053+00:00",
        "confidence": 1.0,
        "raw_name": "hunyuan-turbos-20250226",
        "raw_scores": {
          "arena_elo": 1348.79,
          "arena_votes": 2226
        }
      }
    ],
    "benchmark_breakdown": {
      "LMSYS Chatbot Arena": {
        "arena_elo": 1348.79
      }
    },
    "confidence_score": 1.0,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "amazon-nova-experimental-chat-10-09",
    "canonical_name": "amazon-nova-experimental-chat-10-09",
    "model_name": "amazon-nova-experimental-chat-10-09",
    "aliases": [
      "amazon-nova-experimental-chat-10-09"
    ],
    "provider": "Amazon",
    "context_window": null,
    "open_source": null,
    "arena_votes": 2874,
    "license_type": "Proprietary",
    "creator": null,
    "intelligence_score": null,
    "coding_score": null,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 1347.44,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": null,
    "tau2": null,
    "lcr": null,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": null,
    "gpqa": null,
    "scicode": null,
    "ifbench": null,
    "aime25": null,
    "critpt": null,
    "mmmu_pro": null,
    "livecodebench": null,
    "sources": [
      "LMSYS Chatbot Arena"
    ],
    "source_count": 1,
    "data_tier": 3,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.893081+00:00",
        "confidence": 1.0,
        "raw_name": "amazon-nova-experimental-chat-10-09",
        "raw_scores": {
          "arena_elo": 1347.44,
          "arena_votes": 2874
        }
      }
    ],
    "benchmark_breakdown": {
      "LMSYS Chatbot Arena": {
        "arena_elo": 1347.44
      }
    },
    "confidence_score": 1.0,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "step-3",
    "canonical_name": "step-3",
    "model_name": "step-3",
    "aliases": [
      "step-3"
    ],
    "provider": "StepFun",
    "context_window": null,
    "open_source": null,
    "arena_votes": 6567,
    "license_type": "Apache 2.0",
    "creator": null,
    "intelligence_score": null,
    "coding_score": null,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 1346.54,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": null,
    "tau2": null,
    "lcr": null,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": null,
    "gpqa": null,
    "scicode": null,
    "ifbench": null,
    "aime25": null,
    "critpt": null,
    "mmmu_pro": null,
    "livecodebench": null,
    "sources": [
      "LMSYS Chatbot Arena"
    ],
    "source_count": 1,
    "data_tier": 3,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.893145+00:00",
        "confidence": 1.0,
        "raw_name": "step-3",
        "raw_scores": {
          "arena_elo": 1346.54,
          "arena_votes": 6567
        }
      }
    ],
    "benchmark_breakdown": {
      "LMSYS Chatbot Arena": {
        "arena_elo": 1346.54
      }
    },
    "confidence_score": 1.0,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "qwen-plus-0125",
    "canonical_name": "qwen-plus-0125",
    "model_name": "qwen-plus-0125",
    "aliases": [
      "qwen-plus-0125"
    ],
    "provider": "Alibaba",
    "context_window": null,
    "open_source": null,
    "arena_votes": 5823,
    "license_type": "Proprietary",
    "creator": null,
    "intelligence_score": null,
    "coding_score": null,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 1346.19,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": null,
    "tau2": null,
    "lcr": null,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": null,
    "gpqa": null,
    "scicode": null,
    "ifbench": null,
    "aime25": null,
    "critpt": null,
    "mmmu_pro": null,
    "livecodebench": null,
    "sources": [
      "LMSYS Chatbot Arena"
    ],
    "source_count": 1,
    "data_tier": 3,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.893158+00:00",
        "confidence": 1.0,
        "raw_name": "qwen-plus-0125",
        "raw_scores": {
          "arena_elo": 1346.19,
          "arena_votes": 5823
        }
      }
    ],
    "benchmark_breakdown": {
      "LMSYS Chatbot Arena": {
        "arena_elo": 1346.19
      }
    },
    "confidence_score": 1.0,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "glm-4-plus-0111",
    "canonical_name": "glm-4-plus-0111",
    "model_name": "glm-4-plus-0111",
    "aliases": [
      "glm-4-plus-0111"
    ],
    "provider": "Zhipu",
    "context_window": null,
    "open_source": null,
    "arena_votes": 5760,
    "license_type": "Proprietary",
    "creator": null,
    "intelligence_score": null,
    "coding_score": null,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 1343.16,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": null,
    "tau2": null,
    "lcr": null,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": null,
    "gpqa": null,
    "scicode": null,
    "ifbench": null,
    "aime25": null,
    "critpt": null,
    "mmmu_pro": null,
    "livecodebench": null,
    "sources": [
      "LMSYS Chatbot Arena"
    ],
    "source_count": 1,
    "data_tier": 3,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.893183+00:00",
        "confidence": 1.0,
        "raw_name": "glm-4-plus-0111",
        "raw_scores": {
          "arena_elo": 1343.16,
          "arena_votes": 5760
        }
      }
    ],
    "benchmark_breakdown": {
      "LMSYS Chatbot Arena": {
        "arena_elo": 1343.16
      }
    },
    "confidence_score": 1.0,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "claude-35-sonnet-20240620",
    "canonical_name": "claude-3.5-sonnet-20240620",
    "model_name": "claude-3.5-sonnet-20240620",
    "aliases": [
      "claude-3.5-sonnet-20240620"
    ],
    "provider": "Anthropic",
    "context_window": null,
    "open_source": null,
    "arena_votes": 82417,
    "license_type": "Proprietary",
    "creator": null,
    "intelligence_score": null,
    "coding_score": null,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 1342.44,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": null,
    "tau2": null,
    "lcr": null,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": null,
    "gpqa": null,
    "scicode": null,
    "ifbench": null,
    "aime25": null,
    "critpt": null,
    "mmmu_pro": null,
    "livecodebench": null,
    "sources": [
      "LMSYS Chatbot Arena"
    ],
    "source_count": 1,
    "data_tier": 3,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.893227+00:00",
        "confidence": 1.0,
        "raw_name": "claude-3.5-sonnet-20240620",
        "raw_scores": {
          "arena_elo": 1342.44,
          "arena_votes": 82417
        }
      }
    ],
    "benchmark_breakdown": {
      "LMSYS Chatbot Arena": {
        "arena_elo": 1342.44
      }
    },
    "confidence_score": 1.0,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "gemma-3-12b-it",
    "canonical_name": "gemma-3-12b-it",
    "model_name": "gemma-3-12b-it",
    "aliases": [
      "gemma-3-12b-it"
    ],
    "provider": "Google",
    "context_window": null,
    "open_source": null,
    "arena_votes": 3829,
    "license_type": "Gemma",
    "creator": null,
    "intelligence_score": null,
    "coding_score": null,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 1341.62,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": null,
    "tau2": null,
    "lcr": null,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": null,
    "gpqa": null,
    "scicode": null,
    "ifbench": null,
    "aime25": null,
    "critpt": null,
    "mmmu_pro": null,
    "livecodebench": null,
    "sources": [
      "LMSYS Chatbot Arena"
    ],
    "source_count": 1,
    "data_tier": 3,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.893243+00:00",
        "confidence": 1.0,
        "raw_name": "gemma-3-12b-it",
        "raw_scores": {
          "arena_elo": 1341.62,
          "arena_votes": 3829
        }
      }
    ],
    "benchmark_breakdown": {
      "LMSYS Chatbot Arena": {
        "arena_elo": 1341.62
      }
    },
    "confidence_score": 1.0,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "nvidia-llama-33-nemotron-super-49b-v15",
    "canonical_name": "nvidia-llama-3.3-nemotron-super-49b-v1.5",
    "model_name": "nvidia-llama-3.3-nemotron-super-49b-v1.5",
    "aliases": [
      "nvidia-llama-3.3-nemotron-super-49b-v1.5"
    ],
    "provider": "Nvidia",
    "context_window": null,
    "open_source": null,
    "arena_votes": 3400,
    "license_type": "Nvidia Open",
    "creator": null,
    "intelligence_score": null,
    "coding_score": null,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 1341.3,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": null,
    "tau2": null,
    "lcr": null,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": null,
    "gpqa": null,
    "scicode": null,
    "ifbench": null,
    "aime25": null,
    "critpt": null,
    "mmmu_pro": null,
    "livecodebench": null,
    "sources": [
      "LMSYS Chatbot Arena"
    ],
    "source_count": 1,
    "data_tier": 3,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.893256+00:00",
        "confidence": 1.0,
        "raw_name": "nvidia-llama-3.3-nemotron-super-49b-v1.5",
        "raw_scores": {
          "arena_elo": 1341.3,
          "arena_votes": 3400
        }
      }
    ],
    "benchmark_breakdown": {
      "LMSYS Chatbot Arena": {
        "arena_elo": 1341.3
      }
    },
    "confidence_score": 1.0,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "hunyuan-turbo-0110",
    "canonical_name": "hunyuan-turbo-0110",
    "model_name": "hunyuan-turbo-0110",
    "aliases": [
      "hunyuan-turbo-0110"
    ],
    "provider": "Tencent",
    "context_window": null,
    "open_source": null,
    "arena_votes": 2295,
    "license_type": "Proprietary",
    "creator": null,
    "intelligence_score": null,
    "coding_score": null,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 1340.43,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": null,
    "tau2": null,
    "lcr": null,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": null,
    "gpqa": null,
    "scicode": null,
    "ifbench": null,
    "aime25": null,
    "critpt": null,
    "mmmu_pro": null,
    "livecodebench": null,
    "sources": [
      "LMSYS Chatbot Arena"
    ],
    "source_count": 1,
    "data_tier": 3,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.893268+00:00",
        "confidence": 1.0,
        "raw_name": "hunyuan-turbo-0110",
        "raw_scores": {
          "arena_elo": 1340.43,
          "arena_votes": 2295
        }
      }
    ],
    "benchmark_breakdown": {
      "LMSYS Chatbot Arena": {
        "arena_elo": 1340.43
      }
    },
    "confidence_score": 1.0,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "nova-2-lite",
    "canonical_name": "nova-2-lite",
    "model_name": "nova-2-lite",
    "aliases": [
      "nova-2-lite"
    ],
    "provider": "Amazon",
    "context_window": null,
    "open_source": null,
    "arena_votes": 12111,
    "license_type": "Proprietary",
    "creator": null,
    "intelligence_score": null,
    "coding_score": null,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 1337.17,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": null,
    "tau2": null,
    "lcr": null,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": null,
    "gpqa": null,
    "scicode": null,
    "ifbench": null,
    "aime25": null,
    "critpt": null,
    "mmmu_pro": null,
    "livecodebench": null,
    "sources": [
      "LMSYS Chatbot Arena"
    ],
    "source_count": 1,
    "data_tier": 3,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.893293+00:00",
        "confidence": 1.0,
        "raw_name": "nova-2-lite",
        "raw_scores": {
          "arena_elo": 1337.17,
          "arena_votes": 12111
        }
      }
    ],
    "benchmark_breakdown": {
      "LMSYS Chatbot Arena": {
        "arena_elo": 1337.17
      }
    },
    "confidence_score": 1.0,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "llama-31-405b-instruct-bf16",
    "canonical_name": "llama-3.1-405b-instruct-bf16",
    "model_name": "llama-3.1-405b-instruct-bf16",
    "aliases": [
      "llama-3.1-405b-instruct-bf16"
    ],
    "provider": "Meta",
    "context_window": null,
    "open_source": null,
    "arena_votes": 41392,
    "license_type": "Llama 3.1 Community",
    "creator": null,
    "intelligence_score": null,
    "coding_score": null,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 1335.21,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": null,
    "tau2": null,
    "lcr": null,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": null,
    "gpqa": null,
    "scicode": null,
    "ifbench": null,
    "aime25": null,
    "critpt": null,
    "mmmu_pro": null,
    "livecodebench": null,
    "sources": [
      "LMSYS Chatbot Arena"
    ],
    "source_count": 1,
    "data_tier": 3,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.893331+00:00",
        "confidence": 1.0,
        "raw_name": "llama-3.1-405b-instruct-bf16",
        "raw_scores": {
          "arena_elo": 1335.21,
          "arena_votes": 41392
        }
      }
    ],
    "benchmark_breakdown": {
      "LMSYS Chatbot Arena": {
        "arena_elo": 1335.21
      }
    },
    "confidence_score": 1.0,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "grok-2-2024-08-13",
    "canonical_name": "grok-2-2024-08-13",
    "model_name": "grok-2-2024-08-13",
    "aliases": [
      "grok-2-2024-08-13"
    ],
    "provider": "xAI",
    "context_window": null,
    "open_source": null,
    "arena_votes": 63495,
    "license_type": "Proprietary",
    "creator": null,
    "intelligence_score": null,
    "coding_score": null,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 1335.09,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": null,
    "tau2": null,
    "lcr": null,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": null,
    "gpqa": null,
    "scicode": null,
    "ifbench": null,
    "aime25": null,
    "critpt": null,
    "mmmu_pro": null,
    "livecodebench": null,
    "sources": [
      "LMSYS Chatbot Arena"
    ],
    "source_count": 1,
    "data_tier": 3,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.893344+00:00",
        "confidence": 1.0,
        "raw_name": "grok-2-2024-08-13",
        "raw_scores": {
          "arena_elo": 1335.09,
          "arena_votes": 63495
        }
      }
    ],
    "benchmark_breakdown": {
      "LMSYS Chatbot Arena": {
        "arena_elo": 1335.09
      }
    },
    "confidence_score": 1.0,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "gpt-4o-2024-08-06",
    "canonical_name": "gpt-4o-2024-08-06",
    "model_name": "gpt-4o-2024-08-06",
    "aliases": [
      "gpt-4o-2024-08-06"
    ],
    "provider": "OpenAI",
    "context_window": null,
    "open_source": null,
    "arena_votes": 45498,
    "license_type": "Proprietary",
    "creator": null,
    "intelligence_score": null,
    "coding_score": null,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 1334.94,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": null,
    "tau2": null,
    "lcr": null,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": null,
    "gpqa": null,
    "scicode": null,
    "ifbench": null,
    "aime25": null,
    "critpt": null,
    "mmmu_pro": null,
    "livecodebench": null,
    "sources": [
      "LMSYS Chatbot Arena"
    ],
    "source_count": 1,
    "data_tier": 3,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.893356+00:00",
        "confidence": 1.0,
        "raw_name": "gpt-4o-2024-08-06",
        "raw_scores": {
          "arena_elo": 1334.94,
          "arena_votes": 45498
        }
      }
    ],
    "benchmark_breakdown": {
      "LMSYS Chatbot Arena": {
        "arena_elo": 1334.94
      }
    },
    "confidence_score": 1.0,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "gemini-advanced-0514",
    "canonical_name": "gemini-advanced-0514",
    "model_name": "gemini-advanced-0514",
    "aliases": [
      "gemini-advanced-0514"
    ],
    "provider": "Google",
    "context_window": null,
    "open_source": null,
    "arena_votes": 50142,
    "license_type": "Proprietary",
    "creator": null,
    "intelligence_score": null,
    "coding_score": null,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 1334.77,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": null,
    "tau2": null,
    "lcr": null,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": null,
    "gpqa": null,
    "scicode": null,
    "ifbench": null,
    "aime25": null,
    "critpt": null,
    "mmmu_pro": null,
    "livecodebench": null,
    "sources": [
      "LMSYS Chatbot Arena"
    ],
    "source_count": 1,
    "data_tier": 3,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.893369+00:00",
        "confidence": 1.0,
        "raw_name": "gemini-advanced-0514",
        "raw_scores": {
          "arena_elo": 1334.77,
          "arena_votes": 50142
        }
      }
    ],
    "benchmark_breakdown": {
      "LMSYS Chatbot Arena": {
        "arena_elo": 1334.77
      }
    },
    "confidence_score": 1.0,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "step-2-16k-exp-202412",
    "canonical_name": "step-2-16k-exp-202412",
    "model_name": "step-2-16k-exp-202412",
    "aliases": [
      "step-2-16k-exp-202412"
    ],
    "provider": "StepFun",
    "context_window": null,
    "open_source": null,
    "arena_votes": 4829,
    "license_type": "Proprietary",
    "creator": null,
    "intelligence_score": null,
    "coding_score": null,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 1334.03,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": null,
    "tau2": null,
    "lcr": null,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": null,
    "gpqa": null,
    "scicode": null,
    "ifbench": null,
    "aime25": null,
    "critpt": null,
    "mmmu_pro": null,
    "livecodebench": null,
    "sources": [
      "LMSYS Chatbot Arena"
    ],
    "source_count": 1,
    "data_tier": 3,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.893381+00:00",
        "confidence": 1.0,
        "raw_name": "step-2-16k-exp-202412",
        "raw_scores": {
          "arena_elo": 1334.03,
          "arena_votes": 4829
        }
      }
    ],
    "benchmark_breakdown": {
      "LMSYS Chatbot Arena": {
        "arena_elo": 1334.03
      }
    },
    "confidence_score": 1.0,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "llama-31-405b-instruct-fp8",
    "canonical_name": "llama-3.1-405b-instruct-fp8",
    "model_name": "llama-3.1-405b-instruct-fp8",
    "aliases": [
      "llama-3.1-405b-instruct-fp8"
    ],
    "provider": "Meta",
    "context_window": null,
    "open_source": null,
    "arena_votes": 59655,
    "license_type": "Llama 3.1 Community",
    "creator": null,
    "intelligence_score": null,
    "coding_score": null,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 1333.39,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": null,
    "tau2": null,
    "lcr": null,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": null,
    "gpqa": null,
    "scicode": null,
    "ifbench": null,
    "aime25": null,
    "critpt": null,
    "mmmu_pro": null,
    "livecodebench": null,
    "sources": [
      "LMSYS Chatbot Arena"
    ],
    "source_count": 1,
    "data_tier": 3,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.893393+00:00",
        "confidence": 1.0,
        "raw_name": "llama-3.1-405b-instruct-fp8",
        "raw_scores": {
          "arena_elo": 1333.39,
          "arena_votes": 59655
        }
      }
    ],
    "benchmark_breakdown": {
      "LMSYS Chatbot Arena": {
        "arena_elo": 1333.39
      }
    },
    "confidence_score": 1.0,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "molmo-2-8b",
    "canonical_name": "molmo-2-8b",
    "model_name": "molmo-2-8b",
    "aliases": [
      "molmo-2-8b"
    ],
    "provider": "Ai2",
    "context_window": null,
    "open_source": null,
    "arena_votes": 816,
    "license_type": "Apache 2.0",
    "creator": null,
    "intelligence_score": null,
    "coding_score": null,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 1329.22,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": null,
    "tau2": null,
    "lcr": null,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": null,
    "gpqa": null,
    "scicode": null,
    "ifbench": null,
    "aime25": null,
    "critpt": null,
    "mmmu_pro": null,
    "livecodebench": null,
    "sources": [
      "LMSYS Chatbot Arena"
    ],
    "source_count": 1,
    "data_tier": 3,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.893418+00:00",
        "confidence": 1.0,
        "raw_name": "molmo-2-8b",
        "raw_scores": {
          "arena_elo": 1329.22,
          "arena_votes": 816
        }
      }
    ],
    "benchmark_breakdown": {
      "LMSYS Chatbot Arena": {
        "arena_elo": 1329.22
      }
    },
    "confidence_score": 1.0,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "yi-lightning",
    "canonical_name": "yi-lightning",
    "model_name": "yi-lightning",
    "aliases": [
      "yi-lightning"
    ],
    "provider": "01 AI",
    "context_window": null,
    "open_source": null,
    "arena_votes": 27340,
    "license_type": "Proprietary",
    "creator": null,
    "intelligence_score": null,
    "coding_score": null,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 1328.48,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": null,
    "tau2": null,
    "lcr": null,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": null,
    "gpqa": null,
    "scicode": null,
    "ifbench": null,
    "aime25": null,
    "critpt": null,
    "mmmu_pro": null,
    "livecodebench": null,
    "sources": [
      "LMSYS Chatbot Arena"
    ],
    "source_count": 1,
    "data_tier": 3,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.893431+00:00",
        "confidence": 1.0,
        "raw_name": "yi-lightning",
        "raw_scores": {
          "arena_elo": 1328.48,
          "arena_votes": 27340
        }
      }
    ],
    "benchmark_breakdown": {
      "LMSYS Chatbot Arena": {
        "arena_elo": 1328.48
      }
    },
    "confidence_score": 1.0,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "llama-33-nemotron-49b-super-v1",
    "canonical_name": "llama-3.3-nemotron-49b-super-v1",
    "model_name": "llama-3.3-nemotron-49b-super-v1",
    "aliases": [
      "llama-3.3-nemotron-49b-super-v1"
    ],
    "provider": "Nvidia",
    "context_window": null,
    "open_source": null,
    "arena_votes": 2230,
    "license_type": "Nvidia",
    "creator": null,
    "intelligence_score": null,
    "coding_score": null,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 1327.07,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": null,
    "tau2": null,
    "lcr": null,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": null,
    "gpqa": null,
    "scicode": null,
    "ifbench": null,
    "aime25": null,
    "critpt": null,
    "mmmu_pro": null,
    "livecodebench": null,
    "sources": [
      "LMSYS Chatbot Arena"
    ],
    "source_count": 1,
    "data_tier": 3,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.893468+00:00",
        "confidence": 1.0,
        "raw_name": "llama-3.3-nemotron-49b-super-v1",
        "raw_scores": {
          "arena_elo": 1327.07,
          "arena_votes": 2230
        }
      }
    ],
    "benchmark_breakdown": {
      "LMSYS Chatbot Arena": {
        "arena_elo": 1327.07
      }
    },
    "confidence_score": 1.0,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "hunyuan-large-2025-02-10",
    "canonical_name": "hunyuan-large-2025-02-10",
    "model_name": "hunyuan-large-2025-02-10",
    "aliases": [
      "hunyuan-large-2025-02-10"
    ],
    "provider": "Tencent",
    "context_window": null,
    "open_source": null,
    "arena_votes": 3738,
    "license_type": "Proprietary",
    "creator": null,
    "intelligence_score": null,
    "coding_score": null,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 1326.42,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": null,
    "tau2": null,
    "lcr": null,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": null,
    "gpqa": null,
    "scicode": null,
    "ifbench": null,
    "aime25": null,
    "critpt": null,
    "mmmu_pro": null,
    "livecodebench": null,
    "sources": [
      "LMSYS Chatbot Arena"
    ],
    "source_count": 1,
    "data_tier": 3,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.893480+00:00",
        "confidence": 1.0,
        "raw_name": "hunyuan-large-2025-02-10",
        "raw_scores": {
          "arena_elo": 1326.42,
          "arena_votes": 3738
        }
      }
    ],
    "benchmark_breakdown": {
      "LMSYS Chatbot Arena": {
        "arena_elo": 1326.42
      }
    },
    "confidence_score": 1.0,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "deepseek-v25-1210",
    "canonical_name": "deepseek-v2.5-1210",
    "model_name": "deepseek-v2.5-1210",
    "aliases": [
      "deepseek-v2.5-1210"
    ],
    "provider": "DeepSeek",
    "context_window": null,
    "open_source": null,
    "arena_votes": 6793,
    "license_type": "DeepSeek",
    "creator": null,
    "intelligence_score": null,
    "coding_score": null,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 1323.24,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": null,
    "tau2": null,
    "lcr": null,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": null,
    "gpqa": null,
    "scicode": null,
    "ifbench": null,
    "aime25": null,
    "critpt": null,
    "mmmu_pro": null,
    "livecodebench": null,
    "sources": [
      "LMSYS Chatbot Arena"
    ],
    "source_count": 1,
    "data_tier": 3,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.893522+00:00",
        "confidence": 1.0,
        "raw_name": "deepseek-v2.5-1210",
        "raw_scores": {
          "arena_elo": 1323.24,
          "arena_votes": 6793
        }
      }
    ],
    "benchmark_breakdown": {
      "LMSYS Chatbot Arena": {
        "arena_elo": 1323.24
      }
    },
    "confidence_score": 1.0,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "step-1o-turbo-202506",
    "canonical_name": "step-1o-turbo-202506",
    "model_name": "step-1o-turbo-202506",
    "aliases": [
      "step-1o-turbo-202506"
    ],
    "provider": "StepFun",
    "context_window": null,
    "open_source": null,
    "arena_votes": 9622,
    "license_type": "Proprietary",
    "creator": null,
    "intelligence_score": null,
    "coding_score": null,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 1321.97,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": null,
    "tau2": null,
    "lcr": null,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": null,
    "gpqa": null,
    "scicode": null,
    "ifbench": null,
    "aime25": null,
    "critpt": null,
    "mmmu_pro": null,
    "livecodebench": null,
    "sources": [
      "LMSYS Chatbot Arena"
    ],
    "source_count": 1,
    "data_tier": 3,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.893559+00:00",
        "confidence": 1.0,
        "raw_name": "step-1o-turbo-202506",
        "raw_scores": {
          "arena_elo": 1321.97,
          "arena_votes": 9622
        }
      }
    ],
    "benchmark_breakdown": {
      "LMSYS Chatbot Arena": {
        "arena_elo": 1321.97
      }
    },
    "confidence_score": 1.0,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "glm-4-plus",
    "canonical_name": "glm-4-plus",
    "model_name": "glm-4-plus",
    "aliases": [
      "glm-4-plus"
    ],
    "provider": "Zhipu AI",
    "context_window": null,
    "open_source": null,
    "arena_votes": 26134,
    "license_type": "Proprietary",
    "creator": null,
    "intelligence_score": null,
    "coding_score": null,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 1319.33,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": null,
    "tau2": null,
    "lcr": null,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": null,
    "gpqa": null,
    "scicode": null,
    "ifbench": null,
    "aime25": null,
    "critpt": null,
    "mmmu_pro": null,
    "livecodebench": null,
    "sources": [
      "LMSYS Chatbot Arena"
    ],
    "source_count": 1,
    "data_tier": 3,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.893609+00:00",
        "confidence": 1.0,
        "raw_name": "glm-4-plus",
        "raw_scores": {
          "arena_elo": 1319.33,
          "arena_votes": 26134
        }
      }
    ],
    "benchmark_breakdown": {
      "LMSYS Chatbot Arena": {
        "arena_elo": 1319.33
      }
    },
    "confidence_score": 1.0,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "llama-33-70b-instruct",
    "canonical_name": "llama-3.3-70b-instruct",
    "model_name": "llama-3.3-70b-instruct",
    "aliases": [
      "llama-3.3-70b-instruct"
    ],
    "provider": "Meta",
    "context_window": null,
    "open_source": null,
    "arena_votes": 55454,
    "license_type": "Llama-3.3",
    "creator": null,
    "intelligence_score": null,
    "coding_score": null,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 1319.32,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": null,
    "tau2": null,
    "lcr": null,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": null,
    "gpqa": null,
    "scicode": null,
    "ifbench": null,
    "aime25": null,
    "critpt": null,
    "mmmu_pro": null,
    "livecodebench": null,
    "sources": [
      "LMSYS Chatbot Arena"
    ],
    "source_count": 1,
    "data_tier": 3,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.893622+00:00",
        "confidence": 1.0,
        "raw_name": "llama-3.3-70b-instruct",
        "raw_scores": {
          "arena_elo": 1319.32,
          "arena_votes": 55454
        }
      }
    ],
    "benchmark_breakdown": {
      "LMSYS Chatbot Arena": {
        "arena_elo": 1319.32
      }
    },
    "confidence_score": 1.0,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "gemma-3n-e4b-it",
    "canonical_name": "gemma-3n-e4b-it",
    "model_name": "gemma-3n-e4b-it",
    "aliases": [
      "gemma-3n-e4b-it"
    ],
    "provider": "Google",
    "context_window": null,
    "open_source": null,
    "arena_votes": 23193,
    "license_type": "Gemma",
    "creator": null,
    "intelligence_score": null,
    "coding_score": null,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 1319.29,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": null,
    "tau2": null,
    "lcr": null,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": null,
    "gpqa": null,
    "scicode": null,
    "ifbench": null,
    "aime25": null,
    "critpt": null,
    "mmmu_pro": null,
    "livecodebench": null,
    "sources": [
      "LMSYS Chatbot Arena"
    ],
    "source_count": 1,
    "data_tier": 3,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.893634+00:00",
        "confidence": 1.0,
        "raw_name": "gemma-3n-e4b-it",
        "raw_scores": {
          "arena_elo": 1319.29,
          "arena_votes": 23193
        }
      }
    ],
    "benchmark_breakdown": {
      "LMSYS Chatbot Arena": {
        "arena_elo": 1319.29
      }
    },
    "confidence_score": 1.0,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "qwen-max-0919",
    "canonical_name": "qwen-max-0919",
    "model_name": "qwen-max-0919",
    "aliases": [
      "qwen-max-0919"
    ],
    "provider": "Alibaba",
    "context_window": null,
    "open_source": null,
    "arena_votes": 16479,
    "license_type": "Qwen",
    "creator": null,
    "intelligence_score": null,
    "coding_score": null,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 1318.05,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": null,
    "tau2": null,
    "lcr": null,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": null,
    "gpqa": null,
    "scicode": null,
    "ifbench": null,
    "aime25": null,
    "critpt": null,
    "mmmu_pro": null,
    "livecodebench": null,
    "sources": [
      "LMSYS Chatbot Arena"
    ],
    "source_count": 1,
    "data_tier": 3,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.893646+00:00",
        "confidence": 1.0,
        "raw_name": "qwen-max-0919",
        "raw_scores": {
          "arena_elo": 1318.05,
          "arena_votes": 16479
        }
      }
    ],
    "benchmark_breakdown": {
      "LMSYS Chatbot Arena": {
        "arena_elo": 1318.05
      }
    },
    "confidence_score": 1.0,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "nvidia-nemotron-3-nano-30b-a3b-bf16",
    "canonical_name": "nvidia-nemotron-3-nano-30b-a3b-bf16",
    "model_name": "nvidia-nemotron-3-nano-30b-a3b-bf16",
    "aliases": [
      "nvidia-nemotron-3-nano-30b-a3b-bf16"
    ],
    "provider": "Nvidia",
    "context_window": null,
    "open_source": null,
    "arena_votes": 15408,
    "license_type": "NVIDIA Open Model",
    "creator": null,
    "intelligence_score": null,
    "coding_score": null,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 1317.0,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": null,
    "tau2": null,
    "lcr": null,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": null,
    "gpqa": null,
    "scicode": null,
    "ifbench": null,
    "aime25": null,
    "critpt": null,
    "mmmu_pro": null,
    "livecodebench": null,
    "sources": [
      "LMSYS Chatbot Arena"
    ],
    "source_count": 1,
    "data_tier": 3,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.893684+00:00",
        "confidence": 1.0,
        "raw_name": "nvidia-nemotron-3-nano-30b-a3b-bf16",
        "raw_scores": {
          "arena_elo": 1317.0,
          "arena_votes": 15408
        }
      }
    ],
    "benchmark_breakdown": {
      "LMSYS Chatbot Arena": {
        "arena_elo": 1317.0
      }
    },
    "confidence_score": 1.0,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "qwen25-plus-1127",
    "canonical_name": "qwen2.5-plus-1127",
    "model_name": "qwen2.5-plus-1127",
    "aliases": [
      "qwen2.5-plus-1127"
    ],
    "provider": "Alibaba",
    "context_window": null,
    "open_source": null,
    "arena_votes": 10179,
    "license_type": "Proprietary",
    "creator": null,
    "intelligence_score": null,
    "coding_score": null,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 1315.38,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": null,
    "tau2": null,
    "lcr": null,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": null,
    "gpqa": null,
    "scicode": null,
    "ifbench": null,
    "aime25": null,
    "critpt": null,
    "mmmu_pro": null,
    "livecodebench": null,
    "sources": [
      "LMSYS Chatbot Arena"
    ],
    "source_count": 1,
    "data_tier": 3,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.893697+00:00",
        "confidence": 1.0,
        "raw_name": "qwen2.5-plus-1127",
        "raw_scores": {
          "arena_elo": 1315.38,
          "arena_votes": 10179
        }
      }
    ],
    "benchmark_breakdown": {
      "LMSYS Chatbot Arena": {
        "arena_elo": 1315.38
      }
    },
    "confidence_score": 1.0,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "athene-v2-chat",
    "canonical_name": "athene-v2-chat",
    "model_name": "athene-v2-chat",
    "aliases": [
      "athene-v2-chat"
    ],
    "provider": "NexusFlow",
    "context_window": null,
    "open_source": null,
    "arena_votes": 24746,
    "license_type": "NexusFlow",
    "creator": null,
    "intelligence_score": null,
    "coding_score": null,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 1314.44,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": null,
    "tau2": null,
    "lcr": null,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": null,
    "gpqa": null,
    "scicode": null,
    "ifbench": null,
    "aime25": null,
    "critpt": null,
    "mmmu_pro": null,
    "livecodebench": null,
    "sources": [
      "LMSYS Chatbot Arena"
    ],
    "source_count": 1,
    "data_tier": 3,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.893709+00:00",
        "confidence": 1.0,
        "raw_name": "athene-v2-chat",
        "raw_scores": {
          "arena_elo": 1314.44,
          "arena_votes": 24746
        }
      }
    ],
    "benchmark_breakdown": {
      "LMSYS Chatbot Arena": {
        "arena_elo": 1314.44
      }
    },
    "confidence_score": 1.0,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "mistral-large-2407",
    "canonical_name": "mistral-large-2407",
    "model_name": "mistral-large-2407",
    "aliases": [
      "mistral-large-2407"
    ],
    "provider": "Mistral",
    "context_window": null,
    "open_source": null,
    "arena_votes": 45460,
    "license_type": "Mistral Research",
    "creator": null,
    "intelligence_score": null,
    "coding_score": null,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 1314.0,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": null,
    "tau2": null,
    "lcr": null,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": null,
    "gpqa": null,
    "scicode": null,
    "ifbench": null,
    "aime25": null,
    "critpt": null,
    "mmmu_pro": null,
    "livecodebench": null,
    "sources": [
      "LMSYS Chatbot Arena"
    ],
    "source_count": 1,
    "data_tier": 3,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.893722+00:00",
        "confidence": 1.0,
        "raw_name": "mistral-large-2407",
        "raw_scores": {
          "arena_elo": 1314.0,
          "arena_votes": 45460
        }
      }
    ],
    "benchmark_breakdown": {
      "LMSYS Chatbot Arena": {
        "arena_elo": 1314.0
      }
    },
    "confidence_score": 1.0,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "gpt-4-0125-preview",
    "canonical_name": "gpt-4-0125-preview",
    "model_name": "gpt-4-0125-preview",
    "aliases": [
      "gpt-4-0125-preview"
    ],
    "provider": "OpenAI",
    "context_window": null,
    "open_source": null,
    "arena_votes": 93439,
    "license_type": "Proprietary",
    "creator": null,
    "intelligence_score": null,
    "coding_score": null,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 1313.13,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": null,
    "tau2": null,
    "lcr": null,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": null,
    "gpqa": null,
    "scicode": null,
    "ifbench": null,
    "aime25": null,
    "critpt": null,
    "mmmu_pro": null,
    "livecodebench": null,
    "sources": [
      "LMSYS Chatbot Arena"
    ],
    "source_count": 1,
    "data_tier": 3,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.893734+00:00",
        "confidence": 1.0,
        "raw_name": "gpt-4-0125-preview",
        "raw_scores": {
          "arena_elo": 1313.13,
          "arena_votes": 93439
        }
      }
    ],
    "benchmark_breakdown": {
      "LMSYS Chatbot Arena": {
        "arena_elo": 1313.13
      }
    },
    "confidence_score": 1.0,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "gpt-4-1106-preview",
    "canonical_name": "gpt-4-1106-preview",
    "model_name": "gpt-4-1106-preview",
    "aliases": [
      "gpt-4-1106-preview"
    ],
    "provider": "OpenAI",
    "context_window": null,
    "open_source": null,
    "arena_votes": 100107,
    "license_type": "Proprietary",
    "creator": null,
    "intelligence_score": null,
    "coding_score": null,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 1313.07,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": null,
    "tau2": null,
    "lcr": null,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": null,
    "gpqa": null,
    "scicode": null,
    "ifbench": null,
    "aime25": null,
    "critpt": null,
    "mmmu_pro": null,
    "livecodebench": null,
    "sources": [
      "LMSYS Chatbot Arena"
    ],
    "source_count": 1,
    "data_tier": 3,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.893747+00:00",
        "confidence": 1.0,
        "raw_name": "gpt-4-1106-preview",
        "raw_scores": {
          "arena_elo": 1313.07,
          "arena_votes": 100107
        }
      }
    ],
    "benchmark_breakdown": {
      "LMSYS Chatbot Arena": {
        "arena_elo": 1313.07
      }
    },
    "confidence_score": 1.0,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "hunyuan-standard-2025-02-10",
    "canonical_name": "hunyuan-standard-2025-02-10",
    "model_name": "hunyuan-standard-2025-02-10",
    "aliases": [
      "hunyuan-standard-2025-02-10"
    ],
    "provider": "Tencent",
    "context_window": null,
    "open_source": null,
    "arena_votes": 3905,
    "license_type": "Proprietary",
    "creator": null,
    "intelligence_score": null,
    "coding_score": null,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 1311.55,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": null,
    "tau2": null,
    "lcr": null,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": null,
    "gpqa": null,
    "scicode": null,
    "ifbench": null,
    "aime25": null,
    "critpt": null,
    "mmmu_pro": null,
    "livecodebench": null,
    "sources": [
      "LMSYS Chatbot Arena"
    ],
    "source_count": 1,
    "data_tier": 3,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.893760+00:00",
        "confidence": 1.0,
        "raw_name": "hunyuan-standard-2025-02-10",
        "raw_scores": {
          "arena_elo": 1311.55,
          "arena_votes": 3905
        }
      }
    ],
    "benchmark_breakdown": {
      "LMSYS Chatbot Arena": {
        "arena_elo": 1311.55
      }
    },
    "confidence_score": 1.0,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "mercury",
    "canonical_name": "mercury",
    "model_name": "mercury",
    "aliases": [
      "mercury"
    ],
    "provider": "Inception AI",
    "context_window": null,
    "open_source": null,
    "arena_votes": 1886,
    "license_type": "Proprietary",
    "creator": null,
    "intelligence_score": null,
    "coding_score": null,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 1308.55,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": null,
    "tau2": null,
    "lcr": null,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": null,
    "gpqa": null,
    "scicode": null,
    "ifbench": null,
    "aime25": null,
    "critpt": null,
    "mmmu_pro": null,
    "livecodebench": null,
    "sources": [
      "LMSYS Chatbot Arena"
    ],
    "source_count": 1,
    "data_tier": 3,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.893785+00:00",
        "confidence": 1.0,
        "raw_name": "mercury",
        "raw_scores": {
          "arena_elo": 1308.55,
          "arena_votes": 1886
        }
      }
    ],
    "benchmark_breakdown": {
      "LMSYS Chatbot Arena": {
        "arena_elo": 1308.55
      }
    },
    "confidence_score": 1.0,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "grok-2-mini-2024-08-13",
    "canonical_name": "grok-2-mini-2024-08-13",
    "model_name": "grok-2-mini-2024-08-13",
    "aliases": [
      "grok-2-mini-2024-08-13"
    ],
    "provider": "xAI",
    "context_window": null,
    "open_source": null,
    "arena_votes": 52574,
    "license_type": "Proprietary",
    "creator": null,
    "intelligence_score": null,
    "coding_score": null,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 1307.96,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": null,
    "tau2": null,
    "lcr": null,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": null,
    "gpqa": null,
    "scicode": null,
    "ifbench": null,
    "aime25": null,
    "critpt": null,
    "mmmu_pro": null,
    "livecodebench": null,
    "sources": [
      "LMSYS Chatbot Arena"
    ],
    "source_count": 1,
    "data_tier": 3,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.893798+00:00",
        "confidence": 1.0,
        "raw_name": "grok-2-mini-2024-08-13",
        "raw_scores": {
          "arena_elo": 1307.96,
          "arena_votes": 52574
        }
      }
    ],
    "benchmark_breakdown": {
      "LMSYS Chatbot Arena": {
        "arena_elo": 1307.96
      }
    },
    "confidence_score": 1.0,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "athene-70b-0725",
    "canonical_name": "athene-70b-0725",
    "model_name": "athene-70b-0725",
    "aliases": [
      "athene-70b-0725"
    ],
    "provider": "NexusFlow",
    "context_window": null,
    "open_source": null,
    "arena_votes": 19622,
    "license_type": "CC-BY-NC-4.0",
    "creator": null,
    "intelligence_score": null,
    "coding_score": null,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 1305.96,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": null,
    "tau2": null,
    "lcr": null,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": null,
    "gpqa": null,
    "scicode": null,
    "ifbench": null,
    "aime25": null,
    "critpt": null,
    "mmmu_pro": null,
    "livecodebench": null,
    "sources": [
      "LMSYS Chatbot Arena"
    ],
    "source_count": 1,
    "data_tier": 3,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.893823+00:00",
        "confidence": 1.0,
        "raw_name": "athene-70b-0725",
        "raw_scores": {
          "arena_elo": 1305.96,
          "arena_votes": 19622
        }
      }
    ],
    "benchmark_breakdown": {
      "LMSYS Chatbot Arena": {
        "arena_elo": 1305.96
      }
    },
    "confidence_score": 1.0,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "mistral-large-2411",
    "canonical_name": "mistral-large-2411",
    "model_name": "mistral-large-2411",
    "aliases": [
      "mistral-large-2411"
    ],
    "provider": "Mistral",
    "context_window": null,
    "open_source": null,
    "arena_votes": 28081,
    "license_type": "MRL",
    "creator": null,
    "intelligence_score": null,
    "coding_score": null,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 1305.07,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": null,
    "tau2": null,
    "lcr": null,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": null,
    "gpqa": null,
    "scicode": null,
    "ifbench": null,
    "aime25": null,
    "critpt": null,
    "mmmu_pro": null,
    "livecodebench": null,
    "sources": [
      "LMSYS Chatbot Arena"
    ],
    "source_count": 1,
    "data_tier": 3,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.893848+00:00",
        "confidence": 1.0,
        "raw_name": "mistral-large-2411",
        "raw_scores": {
          "arena_elo": 1305.07,
          "arena_votes": 28081
        }
      }
    ],
    "benchmark_breakdown": {
      "LMSYS Chatbot Arena": {
        "arena_elo": 1305.07
      }
    },
    "confidence_score": 1.0,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "magistral-medium-2506",
    "canonical_name": "magistral-medium-2506",
    "model_name": "magistral-medium-2506",
    "aliases": [
      "magistral-medium-2506"
    ],
    "provider": "Mistral",
    "context_window": null,
    "open_source": null,
    "arena_votes": 11985,
    "license_type": "Proprietary",
    "creator": null,
    "intelligence_score": null,
    "coding_score": null,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 1304.79,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": null,
    "tau2": null,
    "lcr": null,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": null,
    "gpqa": null,
    "scicode": null,
    "ifbench": null,
    "aime25": null,
    "critpt": null,
    "mmmu_pro": null,
    "livecodebench": null,
    "sources": [
      "LMSYS Chatbot Arena"
    ],
    "source_count": 1,
    "data_tier": 3,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.893861+00:00",
        "confidence": 1.0,
        "raw_name": "magistral-medium-2506",
        "raw_scores": {
          "arena_elo": 1304.79,
          "arena_votes": 11985
        }
      }
    ],
    "benchmark_breakdown": {
      "LMSYS Chatbot Arena": {
        "arena_elo": 1304.79
      }
    },
    "confidence_score": 1.0,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "mistral-small-31-24b-instruct-2503",
    "canonical_name": "mistral-small-3.1-24b-instruct-2503",
    "model_name": "mistral-small-3.1-24b-instruct-2503",
    "aliases": [
      "mistral-small-3.1-24b-instruct-2503"
    ],
    "provider": "Mistral",
    "context_window": null,
    "open_source": null,
    "arena_votes": 33897,
    "license_type": "Apache 2.0",
    "creator": null,
    "intelligence_score": null,
    "coding_score": null,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 1304.36,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": null,
    "tau2": null,
    "lcr": null,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": null,
    "gpqa": null,
    "scicode": null,
    "ifbench": null,
    "aime25": null,
    "critpt": null,
    "mmmu_pro": null,
    "livecodebench": null,
    "sources": [
      "LMSYS Chatbot Arena"
    ],
    "source_count": 1,
    "data_tier": 3,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.893874+00:00",
        "confidence": 1.0,
        "raw_name": "mistral-small-3.1-24b-instruct-2503",
        "raw_scores": {
          "arena_elo": 1304.36,
          "arena_votes": 33897
        }
      }
    ],
    "benchmark_breakdown": {
      "LMSYS Chatbot Arena": {
        "arena_elo": 1304.36
      }
    },
    "confidence_score": 1.0,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "gemma-3-4b-it",
    "canonical_name": "gemma-3-4b-it",
    "model_name": "gemma-3-4b-it",
    "aliases": [
      "gemma-3-4b-it"
    ],
    "provider": "Google",
    "context_window": null,
    "open_source": null,
    "arena_votes": 4177,
    "license_type": "Gemma",
    "creator": null,
    "intelligence_score": null,
    "coding_score": null,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 1303.26,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": null,
    "tau2": null,
    "lcr": null,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": null,
    "gpqa": null,
    "scicode": null,
    "ifbench": null,
    "aime25": null,
    "critpt": null,
    "mmmu_pro": null,
    "livecodebench": null,
    "sources": [
      "LMSYS Chatbot Arena"
    ],
    "source_count": 1,
    "data_tier": 3,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.893886+00:00",
        "confidence": 1.0,
        "raw_name": "gemma-3-4b-it",
        "raw_scores": {
          "arena_elo": 1303.26,
          "arena_votes": 4177
        }
      }
    ],
    "benchmark_breakdown": {
      "LMSYS Chatbot Arena": {
        "arena_elo": 1303.26
      }
    },
    "confidence_score": 1.0,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "qwen25-72b-instruct",
    "canonical_name": "qwen2.5-72b-instruct",
    "model_name": "qwen2.5-72b-instruct",
    "aliases": [
      "qwen2.5-72b-instruct"
    ],
    "provider": "Alibaba",
    "context_window": null,
    "open_source": null,
    "arena_votes": 39409,
    "license_type": "Qwen",
    "creator": null,
    "intelligence_score": null,
    "coding_score": null,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 1302.64,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": null,
    "tau2": null,
    "lcr": null,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": null,
    "gpqa": null,
    "scicode": null,
    "ifbench": null,
    "aime25": null,
    "critpt": null,
    "mmmu_pro": null,
    "livecodebench": null,
    "sources": [
      "LMSYS Chatbot Arena"
    ],
    "source_count": 1,
    "data_tier": 3,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.893899+00:00",
        "confidence": 1.0,
        "raw_name": "qwen2.5-72b-instruct",
        "raw_scores": {
          "arena_elo": 1302.64,
          "arena_votes": 39409
        }
      }
    ],
    "benchmark_breakdown": {
      "LMSYS Chatbot Arena": {
        "arena_elo": 1302.64
      }
    },
    "confidence_score": 1.0,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "llama-31-nemotron-70b-instruct",
    "canonical_name": "llama-3.1-nemotron-70b-instruct",
    "model_name": "llama-3.1-nemotron-70b-instruct",
    "aliases": [
      "llama-3.1-nemotron-70b-instruct"
    ],
    "provider": "Nvidia",
    "context_window": null,
    "open_source": null,
    "arena_votes": 7136,
    "license_type": "Llama 3.1",
    "creator": null,
    "intelligence_score": null,
    "coding_score": null,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 1298.63,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": null,
    "tau2": null,
    "lcr": null,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": null,
    "gpqa": null,
    "scicode": null,
    "ifbench": null,
    "aime25": null,
    "critpt": null,
    "mmmu_pro": null,
    "livecodebench": null,
    "sources": [
      "LMSYS Chatbot Arena"
    ],
    "source_count": 1,
    "data_tier": 3,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.893912+00:00",
        "confidence": 1.0,
        "raw_name": "llama-3.1-nemotron-70b-instruct",
        "raw_scores": {
          "arena_elo": 1298.63,
          "arena_votes": 7136
        }
      }
    ],
    "benchmark_breakdown": {
      "LMSYS Chatbot Arena": {
        "arena_elo": 1298.63
      }
    },
    "confidence_score": 1.0,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "hunyuan-large-vision",
    "canonical_name": "hunyuan-large-vision",
    "model_name": "hunyuan-large-vision",
    "aliases": [
      "hunyuan-large-vision"
    ],
    "provider": "Tencent",
    "context_window": null,
    "open_source": null,
    "arena_votes": 5565,
    "license_type": "Proprietary",
    "creator": null,
    "intelligence_score": null,
    "coding_score": null,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 1295.96,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": null,
    "tau2": null,
    "lcr": null,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": null,
    "gpqa": null,
    "scicode": null,
    "ifbench": null,
    "aime25": null,
    "critpt": null,
    "mmmu_pro": null,
    "livecodebench": null,
    "sources": [
      "LMSYS Chatbot Arena"
    ],
    "source_count": 1,
    "data_tier": 3,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.893924+00:00",
        "confidence": 1.0,
        "raw_name": "hunyuan-large-vision",
        "raw_scores": {
          "arena_elo": 1295.96,
          "arena_votes": 5565
        }
      }
    ],
    "benchmark_breakdown": {
      "LMSYS Chatbot Arena": {
        "arena_elo": 1295.96
      }
    },
    "confidence_score": 1.0,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "llama-31-70b-instruct",
    "canonical_name": "llama-3.1-70b-instruct",
    "model_name": "llama-3.1-70b-instruct",
    "aliases": [
      "llama-3.1-70b-instruct"
    ],
    "provider": "Meta",
    "context_window": null,
    "open_source": null,
    "arena_votes": 55234,
    "license_type": "Llama 3.1 Community",
    "creator": null,
    "intelligence_score": null,
    "coding_score": null,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 1293.43,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": null,
    "tau2": null,
    "lcr": null,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": null,
    "gpqa": null,
    "scicode": null,
    "ifbench": null,
    "aime25": null,
    "critpt": null,
    "mmmu_pro": null,
    "livecodebench": null,
    "sources": [
      "LMSYS Chatbot Arena"
    ],
    "source_count": 1,
    "data_tier": 3,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.893937+00:00",
        "confidence": 1.0,
        "raw_name": "llama-3.1-70b-instruct",
        "raw_scores": {
          "arena_elo": 1293.43,
          "arena_votes": 55234
        }
      }
    ],
    "benchmark_breakdown": {
      "LMSYS Chatbot Arena": {
        "arena_elo": 1293.43
      }
    },
    "confidence_score": 1.0,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "gemma-2-27b-it",
    "canonical_name": "gemma-2-27b-it",
    "model_name": "gemma-2-27b-it",
    "aliases": [
      "gemma-2-27b-it"
    ],
    "provider": "Google",
    "context_window": null,
    "open_source": null,
    "arena_votes": 75764,
    "license_type": "Gemma license",
    "creator": null,
    "intelligence_score": null,
    "coding_score": null,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 1288.04,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": null,
    "tau2": null,
    "lcr": null,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": null,
    "gpqa": null,
    "scicode": null,
    "ifbench": null,
    "aime25": null,
    "critpt": null,
    "mmmu_pro": null,
    "livecodebench": null,
    "sources": [
      "LMSYS Chatbot Arena"
    ],
    "source_count": 1,
    "data_tier": 3,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.893974+00:00",
        "confidence": 1.0,
        "raw_name": "gemma-2-27b-it",
        "raw_scores": {
          "arena_elo": 1288.04,
          "arena_votes": 75764
        }
      }
    ],
    "benchmark_breakdown": {
      "LMSYS Chatbot Arena": {
        "arena_elo": 1288.04
      }
    },
    "confidence_score": 1.0,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "reka-core-20240904",
    "canonical_name": "reka-core-20240904",
    "model_name": "reka-core-20240904",
    "aliases": [
      "reka-core-20240904"
    ],
    "provider": "Reka AI",
    "context_window": null,
    "open_source": null,
    "arena_votes": 7309,
    "license_type": "Proprietary",
    "creator": null,
    "intelligence_score": null,
    "coding_score": null,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 1287.81,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": null,
    "tau2": null,
    "lcr": null,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": null,
    "gpqa": null,
    "scicode": null,
    "ifbench": null,
    "aime25": null,
    "critpt": null,
    "mmmu_pro": null,
    "livecodebench": null,
    "sources": [
      "LMSYS Chatbot Arena"
    ],
    "source_count": 1,
    "data_tier": 3,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.893986+00:00",
        "confidence": 1.0,
        "raw_name": "reka-core-20240904",
        "raw_scores": {
          "arena_elo": 1287.81,
          "arena_votes": 7309
        }
      }
    ],
    "benchmark_breakdown": {
      "LMSYS Chatbot Arena": {
        "arena_elo": 1287.81
      }
    },
    "confidence_score": 1.0,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "ibm-granite-h-small",
    "canonical_name": "ibm-granite-h-small",
    "model_name": "ibm-granite-h-small",
    "aliases": [
      "ibm-granite-h-small"
    ],
    "provider": "IBM",
    "context_window": null,
    "open_source": null,
    "arena_votes": 5622,
    "license_type": "Apache 2.0",
    "creator": null,
    "intelligence_score": null,
    "coding_score": null,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 1287.04,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": null,
    "tau2": null,
    "lcr": null,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": null,
    "gpqa": null,
    "scicode": null,
    "ifbench": null,
    "aime25": null,
    "critpt": null,
    "mmmu_pro": null,
    "livecodebench": null,
    "sources": [
      "LMSYS Chatbot Arena"
    ],
    "source_count": 1,
    "data_tier": 3,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.893999+00:00",
        "confidence": 1.0,
        "raw_name": "ibm-granite-h-small",
        "raw_scores": {
          "arena_elo": 1287.04,
          "arena_votes": 5622
        }
      }
    ],
    "benchmark_breakdown": {
      "LMSYS Chatbot Arena": {
        "arena_elo": 1287.04
      }
    },
    "confidence_score": 1.0,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "gpt-4-0314",
    "canonical_name": "gpt-4-0314",
    "model_name": "gpt-4-0314",
    "aliases": [
      "gpt-4-0314"
    ],
    "provider": "OpenAI",
    "context_window": null,
    "open_source": null,
    "arena_votes": 54167,
    "license_type": "Proprietary",
    "creator": null,
    "intelligence_score": null,
    "coding_score": null,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 1286.93,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": null,
    "tau2": null,
    "lcr": null,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": null,
    "gpqa": null,
    "scicode": null,
    "ifbench": null,
    "aime25": null,
    "critpt": null,
    "mmmu_pro": null,
    "livecodebench": null,
    "sources": [
      "LMSYS Chatbot Arena"
    ],
    "source_count": 1,
    "data_tier": 3,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.894011+00:00",
        "confidence": 1.0,
        "raw_name": "gpt-4-0314",
        "raw_scores": {
          "arena_elo": 1286.93,
          "arena_votes": 54167
        }
      }
    ],
    "benchmark_breakdown": {
      "LMSYS Chatbot Arena": {
        "arena_elo": 1286.93
      }
    },
    "confidence_score": 1.0,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "llama-31-tulu-3-70b",
    "canonical_name": "llama-3.1-tulu-3-70b",
    "model_name": "llama-3.1-tulu-3-70b",
    "aliases": [
      "llama-3.1-tulu-3-70b"
    ],
    "provider": "Ai2",
    "context_window": null,
    "open_source": null,
    "arena_votes": 2846,
    "license_type": "Llama 3.1",
    "creator": null,
    "intelligence_score": null,
    "coding_score": null,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 1286.53,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": null,
    "tau2": null,
    "lcr": null,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": null,
    "gpqa": null,
    "scicode": null,
    "ifbench": null,
    "aime25": null,
    "critpt": null,
    "mmmu_pro": null,
    "livecodebench": null,
    "sources": [
      "LMSYS Chatbot Arena"
    ],
    "source_count": 1,
    "data_tier": 3,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.894024+00:00",
        "confidence": 1.0,
        "raw_name": "llama-3.1-tulu-3-70b",
        "raw_scores": {
          "arena_elo": 1286.53,
          "arena_votes": 2846
        }
      }
    ],
    "benchmark_breakdown": {
      "LMSYS Chatbot Arena": {
        "arena_elo": 1286.53
      }
    },
    "confidence_score": 1.0,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "llama-31-nemotron-51b-instruct",
    "canonical_name": "llama-3.1-nemotron-51b-instruct",
    "model_name": "llama-3.1-nemotron-51b-instruct",
    "aliases": [
      "llama-3.1-nemotron-51b-instruct"
    ],
    "provider": "Nvidia",
    "context_window": null,
    "open_source": null,
    "arena_votes": 3749,
    "license_type": "Llama 3.1",
    "creator": null,
    "intelligence_score": null,
    "coding_score": null,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 1286.25,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": null,
    "tau2": null,
    "lcr": null,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": null,
    "gpqa": null,
    "scicode": null,
    "ifbench": null,
    "aime25": null,
    "critpt": null,
    "mmmu_pro": null,
    "livecodebench": null,
    "sources": [
      "LMSYS Chatbot Arena"
    ],
    "source_count": 1,
    "data_tier": 3,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.894036+00:00",
        "confidence": 1.0,
        "raw_name": "llama-3.1-nemotron-51b-instruct",
        "raw_scores": {
          "arena_elo": 1286.25,
          "arena_votes": 3749
        }
      }
    ],
    "benchmark_breakdown": {
      "LMSYS Chatbot Arena": {
        "arena_elo": 1286.25
      }
    },
    "confidence_score": 1.0,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "claude-3-sonnet-20240229",
    "canonical_name": "claude-3-sonnet-20240229",
    "model_name": "claude-3-sonnet-20240229",
    "aliases": [
      "claude-3-sonnet-20240229"
    ],
    "provider": "Anthropic",
    "context_window": null,
    "open_source": null,
    "arena_votes": 109289,
    "license_type": "Proprietary",
    "creator": null,
    "intelligence_score": null,
    "coding_score": null,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 1280.97,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": null,
    "tau2": null,
    "lcr": null,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": null,
    "gpqa": null,
    "scicode": null,
    "ifbench": null,
    "aime25": null,
    "critpt": null,
    "mmmu_pro": null,
    "livecodebench": null,
    "sources": [
      "LMSYS Chatbot Arena"
    ],
    "source_count": 1,
    "data_tier": 3,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.894074+00:00",
        "confidence": 1.0,
        "raw_name": "claude-3-sonnet-20240229",
        "raw_scores": {
          "arena_elo": 1280.97,
          "arena_votes": 109289
        }
      }
    ],
    "benchmark_breakdown": {
      "LMSYS Chatbot Arena": {
        "arena_elo": 1280.97
      }
    },
    "confidence_score": 1.0,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "gemma-2-9b-it-simpo",
    "canonical_name": "gemma-2-9b-it-simpo",
    "model_name": "gemma-2-9b-it-simpo",
    "aliases": [
      "gemma-2-9b-it-simpo"
    ],
    "provider": "Princeton",
    "context_window": null,
    "open_source": null,
    "arena_votes": 10069,
    "license_type": "MIT",
    "creator": null,
    "intelligence_score": null,
    "coding_score": null,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 1279.39,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": null,
    "tau2": null,
    "lcr": null,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": null,
    "gpqa": null,
    "scicode": null,
    "ifbench": null,
    "aime25": null,
    "critpt": null,
    "mmmu_pro": null,
    "livecodebench": null,
    "sources": [
      "LMSYS Chatbot Arena"
    ],
    "source_count": 1,
    "data_tier": 3,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.894086+00:00",
        "confidence": 1.0,
        "raw_name": "gemma-2-9b-it-simpo",
        "raw_scores": {
          "arena_elo": 1279.39,
          "arena_votes": 10069
        }
      }
    ],
    "benchmark_breakdown": {
      "LMSYS Chatbot Arena": {
        "arena_elo": 1279.39
      }
    },
    "confidence_score": 1.0,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "nemotron-4-340b-instruct",
    "canonical_name": "nemotron-4-340b-instruct",
    "model_name": "nemotron-4-340b-instruct",
    "aliases": [
      "nemotron-4-340b-instruct"
    ],
    "provider": "Nvidia",
    "context_window": null,
    "open_source": null,
    "arena_votes": 19661,
    "license_type": "NVIDIA Open Model",
    "creator": null,
    "intelligence_score": null,
    "coding_score": null,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 1277.38,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": null,
    "tau2": null,
    "lcr": null,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": null,
    "gpqa": null,
    "scicode": null,
    "ifbench": null,
    "aime25": null,
    "critpt": null,
    "mmmu_pro": null,
    "livecodebench": null,
    "sources": [
      "LMSYS Chatbot Arena"
    ],
    "source_count": 1,
    "data_tier": 3,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.894099+00:00",
        "confidence": 1.0,
        "raw_name": "nemotron-4-340b-instruct",
        "raw_scores": {
          "arena_elo": 1277.38,
          "arena_votes": 19661
        }
      }
    ],
    "benchmark_breakdown": {
      "LMSYS Chatbot Arena": {
        "arena_elo": 1277.38
      }
    },
    "confidence_score": 1.0,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "command-r-plus-08-2024",
    "canonical_name": "command-r-plus-08-2024",
    "model_name": "command-r-plus-08-2024",
    "aliases": [
      "command-r-plus-08-2024"
    ],
    "provider": "Cohere",
    "context_window": null,
    "open_source": null,
    "arena_votes": 9869,
    "license_type": "CC-BY-NC-4.0",
    "creator": null,
    "intelligence_score": null,
    "coding_score": null,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 1276.4,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": null,
    "tau2": null,
    "lcr": null,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": null,
    "gpqa": null,
    "scicode": null,
    "ifbench": null,
    "aime25": null,
    "critpt": null,
    "mmmu_pro": null,
    "livecodebench": null,
    "sources": [
      "LMSYS Chatbot Arena"
    ],
    "source_count": 1,
    "data_tier": 3,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.894111+00:00",
        "confidence": 1.0,
        "raw_name": "command-r-plus-08-2024",
        "raw_scores": {
          "arena_elo": 1276.4,
          "arena_votes": 9869
        }
      }
    ],
    "benchmark_breakdown": {
      "LMSYS Chatbot Arena": {
        "arena_elo": 1276.4
      }
    },
    "confidence_score": 1.0,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "llama-3-70b-instruct",
    "canonical_name": "llama-3-70b-instruct",
    "model_name": "llama-3-70b-instruct",
    "aliases": [
      "llama-3-70b-instruct"
    ],
    "provider": "Meta",
    "context_window": null,
    "open_source": null,
    "arena_votes": 156880,
    "license_type": "Llama 3 Community",
    "creator": null,
    "intelligence_score": null,
    "coding_score": null,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 1275.95,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": null,
    "tau2": null,
    "lcr": null,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": null,
    "gpqa": null,
    "scicode": null,
    "ifbench": null,
    "aime25": null,
    "critpt": null,
    "mmmu_pro": null,
    "livecodebench": null,
    "sources": [
      "LMSYS Chatbot Arena"
    ],
    "source_count": 1,
    "data_tier": 3,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.894124+00:00",
        "confidence": 1.0,
        "raw_name": "llama-3-70b-instruct",
        "raw_scores": {
          "arena_elo": 1275.95,
          "arena_votes": 156880
        }
      }
    ],
    "benchmark_breakdown": {
      "LMSYS Chatbot Arena": {
        "arena_elo": 1275.95
      }
    },
    "confidence_score": 1.0,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "gpt-4-0613",
    "canonical_name": "gpt-4-0613",
    "model_name": "gpt-4-0613",
    "aliases": [
      "gpt-4-0613"
    ],
    "provider": "OpenAI",
    "context_window": null,
    "open_source": null,
    "arena_votes": 88721,
    "license_type": "Proprietary",
    "creator": null,
    "intelligence_score": null,
    "coding_score": null,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 1275.06,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": null,
    "tau2": null,
    "lcr": null,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": null,
    "gpqa": null,
    "scicode": null,
    "ifbench": null,
    "aime25": null,
    "critpt": null,
    "mmmu_pro": null,
    "livecodebench": null,
    "sources": [
      "LMSYS Chatbot Arena"
    ],
    "source_count": 1,
    "data_tier": 3,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.894136+00:00",
        "confidence": 1.0,
        "raw_name": "gpt-4-0613",
        "raw_scores": {
          "arena_elo": 1275.06,
          "arena_votes": 88721
        }
      }
    ],
    "benchmark_breakdown": {
      "LMSYS Chatbot Arena": {
        "arena_elo": 1275.06
      }
    },
    "confidence_score": 1.0,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "mistral-small-24b-instruct-2501",
    "canonical_name": "mistral-small-24b-instruct-2501",
    "model_name": "mistral-small-24b-instruct-2501",
    "aliases": [
      "mistral-small-24b-instruct-2501"
    ],
    "provider": "Mistral",
    "context_window": null,
    "open_source": null,
    "arena_votes": 14677,
    "license_type": "Apache 2.0",
    "creator": null,
    "intelligence_score": null,
    "coding_score": null,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 1273.85,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": null,
    "tau2": null,
    "lcr": null,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": null,
    "gpqa": null,
    "scicode": null,
    "ifbench": null,
    "aime25": null,
    "critpt": null,
    "mmmu_pro": null,
    "livecodebench": null,
    "sources": [
      "LMSYS Chatbot Arena"
    ],
    "source_count": 1,
    "data_tier": 3,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.894148+00:00",
        "confidence": 1.0,
        "raw_name": "mistral-small-24b-instruct-2501",
        "raw_scores": {
          "arena_elo": 1273.85,
          "arena_votes": 14677
        }
      }
    ],
    "benchmark_breakdown": {
      "LMSYS Chatbot Arena": {
        "arena_elo": 1273.85
      }
    },
    "confidence_score": 1.0,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "glm-4-0520",
    "canonical_name": "glm-4-0520",
    "model_name": "glm-4-0520",
    "aliases": [
      "glm-4-0520"
    ],
    "provider": "Zhipu AI",
    "context_window": null,
    "open_source": null,
    "arena_votes": 9788,
    "license_type": "Proprietary",
    "creator": null,
    "intelligence_score": null,
    "coding_score": null,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 1273.36,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": null,
    "tau2": null,
    "lcr": null,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": null,
    "gpqa": null,
    "scicode": null,
    "ifbench": null,
    "aime25": null,
    "critpt": null,
    "mmmu_pro": null,
    "livecodebench": null,
    "sources": [
      "LMSYS Chatbot Arena"
    ],
    "source_count": 1,
    "data_tier": 3,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.894161+00:00",
        "confidence": 1.0,
        "raw_name": "glm-4-0520",
        "raw_scores": {
          "arena_elo": 1273.36,
          "arena_votes": 9788
        }
      }
    ],
    "benchmark_breakdown": {
      "LMSYS Chatbot Arena": {
        "arena_elo": 1273.36
      }
    },
    "confidence_score": 1.0,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "reka-flash-20240904",
    "canonical_name": "reka-flash-20240904",
    "model_name": "reka-flash-20240904",
    "aliases": [
      "reka-flash-20240904"
    ],
    "provider": "Reka AI",
    "context_window": null,
    "open_source": null,
    "arena_votes": 7537,
    "license_type": "Proprietary",
    "creator": null,
    "intelligence_score": null,
    "coding_score": null,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 1272.12,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": null,
    "tau2": null,
    "lcr": null,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": null,
    "gpqa": null,
    "scicode": null,
    "ifbench": null,
    "aime25": null,
    "critpt": null,
    "mmmu_pro": null,
    "livecodebench": null,
    "sources": [
      "LMSYS Chatbot Arena"
    ],
    "source_count": 1,
    "data_tier": 3,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.894173+00:00",
        "confidence": 1.0,
        "raw_name": "reka-flash-20240904",
        "raw_scores": {
          "arena_elo": 1272.12,
          "arena_votes": 7537
        }
      }
    ],
    "benchmark_breakdown": {
      "LMSYS Chatbot Arena": {
        "arena_elo": 1272.12
      }
    },
    "confidence_score": 1.0,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "qwen25-coder-32b-instruct",
    "canonical_name": "qwen2.5-coder-32b-instruct",
    "model_name": "qwen2.5-coder-32b-instruct",
    "aliases": [
      "qwen2.5-coder-32b-instruct"
    ],
    "provider": "Alibaba",
    "context_window": null,
    "open_source": null,
    "arena_votes": 5430,
    "license_type": "Apache 2.0",
    "creator": null,
    "intelligence_score": null,
    "coding_score": null,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 1270.57,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": null,
    "tau2": null,
    "lcr": null,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": null,
    "gpqa": null,
    "scicode": null,
    "ifbench": null,
    "aime25": null,
    "critpt": null,
    "mmmu_pro": null,
    "livecodebench": null,
    "sources": [
      "LMSYS Chatbot Arena"
    ],
    "source_count": 1,
    "data_tier": 3,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.894204+00:00",
        "confidence": 1.0,
        "raw_name": "qwen2.5-coder-32b-instruct",
        "raw_scores": {
          "arena_elo": 1270.57,
          "arena_votes": 5430
        }
      }
    ],
    "benchmark_breakdown": {
      "LMSYS Chatbot Arena": {
        "arena_elo": 1270.57
      }
    },
    "confidence_score": 1.0,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "c4ai-aya-expanse-32b",
    "canonical_name": "c4ai-aya-expanse-32b",
    "model_name": "c4ai-aya-expanse-32b",
    "aliases": [
      "c4ai-aya-expanse-32b"
    ],
    "provider": "Cohere",
    "context_window": null,
    "open_source": null,
    "arena_votes": 27123,
    "license_type": "CC-BY-NC-4.0",
    "creator": null,
    "intelligence_score": null,
    "coding_score": null,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 1267.03,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": null,
    "tau2": null,
    "lcr": null,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": null,
    "gpqa": null,
    "scicode": null,
    "ifbench": null,
    "aime25": null,
    "critpt": null,
    "mmmu_pro": null,
    "livecodebench": null,
    "sources": [
      "LMSYS Chatbot Arena"
    ],
    "source_count": 1,
    "data_tier": 3,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.894224+00:00",
        "confidence": 1.0,
        "raw_name": "c4ai-aya-expanse-32b",
        "raw_scores": {
          "arena_elo": 1267.03,
          "arena_votes": 27123
        }
      }
    ],
    "benchmark_breakdown": {
      "LMSYS Chatbot Arena": {
        "arena_elo": 1267.03
      }
    },
    "confidence_score": 1.0,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "gemma-2-9b-it",
    "canonical_name": "gemma-2-9b-it",
    "model_name": "gemma-2-9b-it",
    "aliases": [
      "gemma-2-9b-it"
    ],
    "provider": "Google",
    "context_window": null,
    "open_source": null,
    "arena_votes": 54615,
    "license_type": "Gemma license",
    "creator": null,
    "intelligence_score": null,
    "coding_score": null,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 1265.46,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": null,
    "tau2": null,
    "lcr": null,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": null,
    "gpqa": null,
    "scicode": null,
    "ifbench": null,
    "aime25": null,
    "critpt": null,
    "mmmu_pro": null,
    "livecodebench": null,
    "sources": [
      "LMSYS Chatbot Arena"
    ],
    "source_count": 1,
    "data_tier": 3,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.894237+00:00",
        "confidence": 1.0,
        "raw_name": "gemma-2-9b-it",
        "raw_scores": {
          "arena_elo": 1265.46,
          "arena_votes": 54615
        }
      }
    ],
    "benchmark_breakdown": {
      "LMSYS Chatbot Arena": {
        "arena_elo": 1265.46
      }
    },
    "confidence_score": 1.0,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "command-r-plus",
    "canonical_name": "command-r-plus",
    "model_name": "command-r-plus",
    "aliases": [
      "command-r-plus"
    ],
    "provider": "Cohere",
    "context_window": null,
    "open_source": null,
    "arena_votes": 77556,
    "license_type": "CC-BY-NC-4.0",
    "creator": null,
    "intelligence_score": null,
    "coding_score": null,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 1261.9,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": null,
    "tau2": null,
    "lcr": null,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": null,
    "gpqa": null,
    "scicode": null,
    "ifbench": null,
    "aime25": null,
    "critpt": null,
    "mmmu_pro": null,
    "livecodebench": null,
    "sources": [
      "LMSYS Chatbot Arena"
    ],
    "source_count": 1,
    "data_tier": 3,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.894262+00:00",
        "confidence": 1.0,
        "raw_name": "command-r-plus",
        "raw_scores": {
          "arena_elo": 1261.9,
          "arena_votes": 77556
        }
      }
    ],
    "benchmark_breakdown": {
      "LMSYS Chatbot Arena": {
        "arena_elo": 1261.9
      }
    },
    "confidence_score": 1.0,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "qwen2-72b-instruct",
    "canonical_name": "qwen2-72b-instruct",
    "model_name": "qwen2-72b-instruct",
    "aliases": [
      "qwen2-72b-instruct"
    ],
    "provider": "Alibaba",
    "context_window": null,
    "open_source": null,
    "arena_votes": 37325,
    "license_type": "Qianwen LICENSE",
    "creator": null,
    "intelligence_score": null,
    "coding_score": null,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 1261.79,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": null,
    "tau2": null,
    "lcr": null,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": null,
    "gpqa": null,
    "scicode": null,
    "ifbench": null,
    "aime25": null,
    "critpt": null,
    "mmmu_pro": null,
    "livecodebench": null,
    "sources": [
      "LMSYS Chatbot Arena"
    ],
    "source_count": 1,
    "data_tier": 3,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.894275+00:00",
        "confidence": 1.0,
        "raw_name": "qwen2-72b-instruct",
        "raw_scores": {
          "arena_elo": 1261.79,
          "arena_votes": 37325
        }
      }
    ],
    "benchmark_breakdown": {
      "LMSYS Chatbot Arena": {
        "arena_elo": 1261.79
      }
    },
    "confidence_score": 1.0,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "claude-3-haiku-20240307",
    "canonical_name": "claude-3-haiku-20240307",
    "model_name": "claude-3-haiku-20240307",
    "aliases": [
      "claude-3-haiku-20240307"
    ],
    "provider": "Anthropic",
    "context_window": null,
    "open_source": null,
    "arena_votes": 117705,
    "license_type": "Proprietary",
    "creator": null,
    "intelligence_score": null,
    "coding_score": null,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 1261.08,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": null,
    "tau2": null,
    "lcr": null,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": null,
    "gpqa": null,
    "scicode": null,
    "ifbench": null,
    "aime25": null,
    "critpt": null,
    "mmmu_pro": null,
    "livecodebench": null,
    "sources": [
      "LMSYS Chatbot Arena"
    ],
    "source_count": 1,
    "data_tier": 3,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.894291+00:00",
        "confidence": 1.0,
        "raw_name": "claude-3-haiku-20240307",
        "raw_scores": {
          "arena_elo": 1261.08,
          "arena_votes": 117705
        }
      }
    ],
    "benchmark_breakdown": {
      "LMSYS Chatbot Arena": {
        "arena_elo": 1261.08
      }
    },
    "confidence_score": 1.0,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "gemini-15-flash-8b-001",
    "canonical_name": "gemini-1.5-flash-8b-001",
    "model_name": "gemini-1.5-flash-8b-001",
    "aliases": [
      "gemini-1.5-flash-8b-001"
    ],
    "provider": "Google",
    "context_window": null,
    "open_source": null,
    "arena_votes": 35556,
    "license_type": "Proprietary",
    "creator": null,
    "intelligence_score": null,
    "coding_score": null,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 1258.67,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": null,
    "tau2": null,
    "lcr": null,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": null,
    "gpqa": null,
    "scicode": null,
    "ifbench": null,
    "aime25": null,
    "critpt": null,
    "mmmu_pro": null,
    "livecodebench": null,
    "sources": [
      "LMSYS Chatbot Arena"
    ],
    "source_count": 1,
    "data_tier": 3,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.894316+00:00",
        "confidence": 1.0,
        "raw_name": "gemini-1.5-flash-8b-001",
        "raw_scores": {
          "arena_elo": 1258.67,
          "arena_votes": 35556
        }
      }
    ],
    "benchmark_breakdown": {
      "LMSYS Chatbot Arena": {
        "arena_elo": 1258.67
      }
    },
    "confidence_score": 1.0,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "olmo-2-0325-32b-instruct",
    "canonical_name": "olmo-2-0325-32b-instruct",
    "model_name": "olmo-2-0325-32b-instruct",
    "aliases": [
      "olmo-2-0325-32b-instruct"
    ],
    "provider": "Ai2",
    "context_window": null,
    "open_source": null,
    "arena_votes": 3335,
    "license_type": "Apache-2.0",
    "creator": null,
    "intelligence_score": null,
    "coding_score": null,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 1251.98,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": null,
    "tau2": null,
    "lcr": null,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": null,
    "gpqa": null,
    "scicode": null,
    "ifbench": null,
    "aime25": null,
    "critpt": null,
    "mmmu_pro": null,
    "livecodebench": null,
    "sources": [
      "LMSYS Chatbot Arena"
    ],
    "source_count": 1,
    "data_tier": 3,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.894341+00:00",
        "confidence": 1.0,
        "raw_name": "olmo-2-0325-32b-instruct",
        "raw_scores": {
          "arena_elo": 1251.98,
          "arena_votes": 3335
        }
      }
    ],
    "benchmark_breakdown": {
      "LMSYS Chatbot Arena": {
        "arena_elo": 1251.98
      }
    },
    "confidence_score": 1.0,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "command-r-08-2024",
    "canonical_name": "command-r-08-2024",
    "model_name": "command-r-08-2024",
    "aliases": [
      "command-r-08-2024"
    ],
    "provider": "Cohere",
    "context_window": null,
    "open_source": null,
    "arena_votes": 10141,
    "license_type": "CC-BY-NC-4.0",
    "creator": null,
    "intelligence_score": null,
    "coding_score": null,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 1250.21,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": null,
    "tau2": null,
    "lcr": null,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": null,
    "gpqa": null,
    "scicode": null,
    "ifbench": null,
    "aime25": null,
    "critpt": null,
    "mmmu_pro": null,
    "livecodebench": null,
    "sources": [
      "LMSYS Chatbot Arena"
    ],
    "source_count": 1,
    "data_tier": 3,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.894354+00:00",
        "confidence": 1.0,
        "raw_name": "command-r-08-2024",
        "raw_scores": {
          "arena_elo": 1250.21,
          "arena_votes": 10141
        }
      }
    ],
    "benchmark_breakdown": {
      "LMSYS Chatbot Arena": {
        "arena_elo": 1250.21
      }
    },
    "confidence_score": 1.0,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "mistral-large-2402",
    "canonical_name": "mistral-large-2402",
    "model_name": "mistral-large-2402",
    "aliases": [
      "mistral-large-2402"
    ],
    "provider": "Mistral",
    "context_window": null,
    "open_source": null,
    "arena_votes": 62437,
    "license_type": "Proprietary",
    "creator": null,
    "intelligence_score": null,
    "coding_score": null,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 1242.41,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": null,
    "tau2": null,
    "lcr": null,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": null,
    "gpqa": null,
    "scicode": null,
    "ifbench": null,
    "aime25": null,
    "critpt": null,
    "mmmu_pro": null,
    "livecodebench": null,
    "sources": [
      "LMSYS Chatbot Arena"
    ],
    "source_count": 1,
    "data_tier": 3,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.894366+00:00",
        "confidence": 1.0,
        "raw_name": "mistral-large-2402",
        "raw_scores": {
          "arena_elo": 1242.41,
          "arena_votes": 62437
        }
      }
    ],
    "benchmark_breakdown": {
      "LMSYS Chatbot Arena": {
        "arena_elo": 1242.41
      }
    },
    "confidence_score": 1.0,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "ministral-8b-2410",
    "canonical_name": "ministral-8b-2410",
    "model_name": "ministral-8b-2410",
    "aliases": [
      "ministral-8b-2410"
    ],
    "provider": "Mistral",
    "context_window": null,
    "open_source": null,
    "arena_votes": 4780,
    "license_type": "MRL",
    "creator": null,
    "intelligence_score": null,
    "coding_score": null,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 1237.03,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": null,
    "tau2": null,
    "lcr": null,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": null,
    "gpqa": null,
    "scicode": null,
    "ifbench": null,
    "aime25": null,
    "critpt": null,
    "mmmu_pro": null,
    "livecodebench": null,
    "sources": [
      "LMSYS Chatbot Arena"
    ],
    "source_count": 1,
    "data_tier": 3,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.894403+00:00",
        "confidence": 1.0,
        "raw_name": "ministral-8b-2410",
        "raw_scores": {
          "arena_elo": 1237.03,
          "arena_votes": 4780
        }
      }
    ],
    "benchmark_breakdown": {
      "LMSYS Chatbot Arena": {
        "arena_elo": 1237.03
      }
    },
    "confidence_score": 1.0,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "gemini-pro-dev-api",
    "canonical_name": "gemini-pro-dev-api",
    "model_name": "gemini-pro-dev-api",
    "aliases": [
      "gemini-pro-dev-api"
    ],
    "provider": "Google",
    "context_window": null,
    "open_source": null,
    "arena_votes": 18352,
    "license_type": "Proprietary",
    "creator": null,
    "intelligence_score": null,
    "coding_score": null,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 1235.0,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": null,
    "tau2": null,
    "lcr": null,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": null,
    "gpqa": null,
    "scicode": null,
    "ifbench": null,
    "aime25": null,
    "critpt": null,
    "mmmu_pro": null,
    "livecodebench": null,
    "sources": [
      "LMSYS Chatbot Arena"
    ],
    "source_count": 1,
    "data_tier": 3,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.894415+00:00",
        "confidence": 1.0,
        "raw_name": "gemini-pro-dev-api",
        "raw_scores": {
          "arena_elo": 1235.0,
          "arena_votes": 18352
        }
      }
    ],
    "benchmark_breakdown": {
      "LMSYS Chatbot Arena": {
        "arena_elo": 1235.0
      }
    },
    "confidence_score": 1.0,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "qwen15-110b-chat",
    "canonical_name": "qwen1.5-110b-chat",
    "model_name": "qwen1.5-110b-chat",
    "aliases": [
      "qwen1.5-110b-chat"
    ],
    "provider": "Alibaba",
    "context_window": null,
    "open_source": null,
    "arena_votes": 26191,
    "license_type": "Qianwen LICENSE",
    "creator": null,
    "intelligence_score": null,
    "coding_score": null,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 1233.96,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": null,
    "tau2": null,
    "lcr": null,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": null,
    "gpqa": null,
    "scicode": null,
    "ifbench": null,
    "aime25": null,
    "critpt": null,
    "mmmu_pro": null,
    "livecodebench": null,
    "sources": [
      "LMSYS Chatbot Arena"
    ],
    "source_count": 1,
    "data_tier": 3,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.894428+00:00",
        "confidence": 1.0,
        "raw_name": "qwen1.5-110b-chat",
        "raw_scores": {
          "arena_elo": 1233.96,
          "arena_votes": 26191
        }
      }
    ],
    "benchmark_breakdown": {
      "LMSYS Chatbot Arena": {
        "arena_elo": 1233.96
      }
    },
    "confidence_score": 1.0,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "hunyuan-standard-256k",
    "canonical_name": "hunyuan-standard-256k",
    "model_name": "hunyuan-standard-256k",
    "aliases": [
      "hunyuan-standard-256k"
    ],
    "provider": "Tencent",
    "context_window": null,
    "open_source": null,
    "arena_votes": 2729,
    "license_type": "Proprietary",
    "creator": null,
    "intelligence_score": null,
    "coding_score": null,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 1233.36,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": null,
    "tau2": null,
    "lcr": null,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": null,
    "gpqa": null,
    "scicode": null,
    "ifbench": null,
    "aime25": null,
    "critpt": null,
    "mmmu_pro": null,
    "livecodebench": null,
    "sources": [
      "LMSYS Chatbot Arena"
    ],
    "source_count": 1,
    "data_tier": 3,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.894440+00:00",
        "confidence": 1.0,
        "raw_name": "hunyuan-standard-256k",
        "raw_scores": {
          "arena_elo": 1233.36,
          "arena_votes": 2729
        }
      }
    ],
    "benchmark_breakdown": {
      "LMSYS Chatbot Arena": {
        "arena_elo": 1233.36
      }
    },
    "confidence_score": 1.0,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "reka-flash-21b-20240226-online",
    "canonical_name": "reka-flash-21b-20240226-online",
    "model_name": "reka-flash-21b-20240226-online",
    "aliases": [
      "reka-flash-21b-20240226-online"
    ],
    "provider": "Reka AI",
    "context_window": null,
    "open_source": null,
    "arena_votes": 15451,
    "license_type": "Proprietary",
    "creator": null,
    "intelligence_score": null,
    "coding_score": null,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 1233.27,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": null,
    "tau2": null,
    "lcr": null,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": null,
    "gpqa": null,
    "scicode": null,
    "ifbench": null,
    "aime25": null,
    "critpt": null,
    "mmmu_pro": null,
    "livecodebench": null,
    "sources": [
      "LMSYS Chatbot Arena"
    ],
    "source_count": 1,
    "data_tier": 3,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.894452+00:00",
        "confidence": 1.0,
        "raw_name": "reka-flash-21b-20240226-online",
        "raw_scores": {
          "arena_elo": 1233.27,
          "arena_votes": 15451
        }
      }
    ],
    "benchmark_breakdown": {
      "LMSYS Chatbot Arena": {
        "arena_elo": 1233.27
      }
    },
    "confidence_score": 1.0,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "qwen15-72b-chat",
    "canonical_name": "qwen1.5-72b-chat",
    "model_name": "qwen1.5-72b-chat",
    "aliases": [
      "qwen1.5-72b-chat"
    ],
    "provider": "Alibaba",
    "context_window": null,
    "open_source": null,
    "arena_votes": 39296,
    "license_type": "Qianwen LICENSE",
    "creator": null,
    "intelligence_score": null,
    "coding_score": null,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 1233.14,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": null,
    "tau2": null,
    "lcr": null,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": null,
    "gpqa": null,
    "scicode": null,
    "ifbench": null,
    "aime25": null,
    "critpt": null,
    "mmmu_pro": null,
    "livecodebench": null,
    "sources": [
      "LMSYS Chatbot Arena"
    ],
    "source_count": 1,
    "data_tier": 3,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.894465+00:00",
        "confidence": 1.0,
        "raw_name": "qwen1.5-72b-chat",
        "raw_scores": {
          "arena_elo": 1233.14,
          "arena_votes": 39296
        }
      }
    ],
    "benchmark_breakdown": {
      "LMSYS Chatbot Arena": {
        "arena_elo": 1233.14
      }
    },
    "confidence_score": 1.0,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "mixtral-8x22b-instruct-v01",
    "canonical_name": "mixtral-8x22b-instruct-v0.1",
    "model_name": "mixtral-8x22b-instruct-v0.1",
    "aliases": [
      "mixtral-8x22b-instruct-v0.1"
    ],
    "provider": "Mistral",
    "context_window": null,
    "open_source": null,
    "arena_votes": 51417,
    "license_type": "Apache 2.0",
    "creator": null,
    "intelligence_score": null,
    "coding_score": null,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 1229.48,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": null,
    "tau2": null,
    "lcr": null,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": null,
    "gpqa": null,
    "scicode": null,
    "ifbench": null,
    "aime25": null,
    "critpt": null,
    "mmmu_pro": null,
    "livecodebench": null,
    "sources": [
      "LMSYS Chatbot Arena"
    ],
    "source_count": 1,
    "data_tier": 3,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.894477+00:00",
        "confidence": 1.0,
        "raw_name": "mixtral-8x22b-instruct-v0.1",
        "raw_scores": {
          "arena_elo": 1229.48,
          "arena_votes": 51417
        }
      }
    ],
    "benchmark_breakdown": {
      "LMSYS Chatbot Arena": {
        "arena_elo": 1229.48
      }
    },
    "confidence_score": 1.0,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "reka-flash-21b-20240226",
    "canonical_name": "reka-flash-21b-20240226",
    "model_name": "reka-flash-21b-20240226",
    "aliases": [
      "reka-flash-21b-20240226"
    ],
    "provider": "Reka AI",
    "context_window": null,
    "open_source": null,
    "arena_votes": 24806,
    "license_type": "Proprietary",
    "creator": null,
    "intelligence_score": null,
    "coding_score": null,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 1226.51,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": null,
    "tau2": null,
    "lcr": null,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": null,
    "gpqa": null,
    "scicode": null,
    "ifbench": null,
    "aime25": null,
    "critpt": null,
    "mmmu_pro": null,
    "livecodebench": null,
    "sources": [
      "LMSYS Chatbot Arena"
    ],
    "source_count": 1,
    "data_tier": 3,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.894503+00:00",
        "confidence": 1.0,
        "raw_name": "reka-flash-21b-20240226",
        "raw_scores": {
          "arena_elo": 1226.51,
          "arena_votes": 24806
        }
      }
    ],
    "benchmark_breakdown": {
      "LMSYS Chatbot Arena": {
        "arena_elo": 1226.51
      }
    },
    "confidence_score": 1.0,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "gpt-35-turbo-0125",
    "canonical_name": "gpt-3.5-turbo-0125",
    "model_name": "gpt-3.5-turbo-0125",
    "aliases": [
      "gpt-3.5-turbo-0125"
    ],
    "provider": "OpenAI",
    "context_window": null,
    "open_source": null,
    "arena_votes": 66191,
    "license_type": "Proprietary",
    "creator": null,
    "intelligence_score": null,
    "coding_score": null,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 1223.96,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": null,
    "tau2": null,
    "lcr": null,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": null,
    "gpqa": null,
    "scicode": null,
    "ifbench": null,
    "aime25": null,
    "critpt": null,
    "mmmu_pro": null,
    "livecodebench": null,
    "sources": [
      "LMSYS Chatbot Arena"
    ],
    "source_count": 1,
    "data_tier": 3,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.894515+00:00",
        "confidence": 1.0,
        "raw_name": "gpt-3.5-turbo-0125",
        "raw_scores": {
          "arena_elo": 1223.96,
          "arena_votes": 66191
        }
      }
    ],
    "benchmark_breakdown": {
      "LMSYS Chatbot Arena": {
        "arena_elo": 1223.96
      }
    },
    "confidence_score": 1.0,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "llama-3-8b-instruct",
    "canonical_name": "llama-3-8b-instruct",
    "model_name": "llama-3-8b-instruct",
    "aliases": [
      "llama-3-8b-instruct"
    ],
    "provider": "Meta",
    "context_window": null,
    "open_source": null,
    "arena_votes": 104636,
    "license_type": "Llama 3 Community",
    "creator": null,
    "intelligence_score": null,
    "coding_score": null,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 1223.12,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": null,
    "tau2": null,
    "lcr": null,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": null,
    "gpqa": null,
    "scicode": null,
    "ifbench": null,
    "aime25": null,
    "critpt": null,
    "mmmu_pro": null,
    "livecodebench": null,
    "sources": [
      "LMSYS Chatbot Arena"
    ],
    "source_count": 1,
    "data_tier": 3,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.894528+00:00",
        "confidence": 1.0,
        "raw_name": "llama-3-8b-instruct",
        "raw_scores": {
          "arena_elo": 1223.12,
          "arena_votes": 104636
        }
      }
    ],
    "benchmark_breakdown": {
      "LMSYS Chatbot Arena": {
        "arena_elo": 1223.12
      }
    },
    "confidence_score": 1.0,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "c4ai-aya-expanse-8b",
    "canonical_name": "c4ai-aya-expanse-8b",
    "model_name": "c4ai-aya-expanse-8b",
    "aliases": [
      "c4ai-aya-expanse-8b"
    ],
    "provider": "Cohere",
    "context_window": null,
    "open_source": null,
    "arena_votes": 9827,
    "license_type": "CC-BY-NC-4.0",
    "creator": null,
    "intelligence_score": null,
    "coding_score": null,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 1223.0,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": null,
    "tau2": null,
    "lcr": null,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": null,
    "gpqa": null,
    "scicode": null,
    "ifbench": null,
    "aime25": null,
    "critpt": null,
    "mmmu_pro": null,
    "livecodebench": null,
    "sources": [
      "LMSYS Chatbot Arena"
    ],
    "source_count": 1,
    "data_tier": 3,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.894540+00:00",
        "confidence": 1.0,
        "raw_name": "c4ai-aya-expanse-8b",
        "raw_scores": {
          "arena_elo": 1223.0,
          "arena_votes": 9827
        }
      }
    ],
    "benchmark_breakdown": {
      "LMSYS Chatbot Arena": {
        "arena_elo": 1223.0
      }
    },
    "confidence_score": 1.0,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "gemini-pro",
    "canonical_name": "gemini-pro",
    "model_name": "gemini-pro",
    "aliases": [
      "gemini-pro"
    ],
    "provider": "Google",
    "context_window": null,
    "open_source": null,
    "arena_votes": 6390,
    "license_type": "Proprietary",
    "creator": null,
    "intelligence_score": null,
    "coding_score": null,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 1221.71,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": null,
    "tau2": null,
    "lcr": null,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": null,
    "gpqa": null,
    "scicode": null,
    "ifbench": null,
    "aime25": null,
    "critpt": null,
    "mmmu_pro": null,
    "livecodebench": null,
    "sources": [
      "LMSYS Chatbot Arena"
    ],
    "source_count": 1,
    "data_tier": 3,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.894565+00:00",
        "confidence": 1.0,
        "raw_name": "gemini-pro",
        "raw_scores": {
          "arena_elo": 1221.71,
          "arena_votes": 6390
        }
      }
    ],
    "benchmark_breakdown": {
      "LMSYS Chatbot Arena": {
        "arena_elo": 1221.71
      }
    },
    "confidence_score": 1.0,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "llama-31-tulu-3-8b",
    "canonical_name": "llama-3.1-tulu-3-8b",
    "model_name": "llama-3.1-tulu-3-8b",
    "aliases": [
      "llama-3.1-tulu-3-8b"
    ],
    "provider": "Ai2",
    "context_window": null,
    "open_source": null,
    "arena_votes": 2895,
    "license_type": "Llama 3.1",
    "creator": null,
    "intelligence_score": null,
    "coding_score": null,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 1220.55,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": null,
    "tau2": null,
    "lcr": null,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": null,
    "gpqa": null,
    "scicode": null,
    "ifbench": null,
    "aime25": null,
    "critpt": null,
    "mmmu_pro": null,
    "livecodebench": null,
    "sources": [
      "LMSYS Chatbot Arena"
    ],
    "source_count": 1,
    "data_tier": 3,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.894577+00:00",
        "confidence": 1.0,
        "raw_name": "llama-3.1-tulu-3-8b",
        "raw_scores": {
          "arena_elo": 1220.55,
          "arena_votes": 2895
        }
      }
    ],
    "benchmark_breakdown": {
      "LMSYS Chatbot Arena": {
        "arena_elo": 1220.55
      }
    },
    "confidence_score": 1.0,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "yi-15-34b-chat",
    "canonical_name": "yi-1.5-34b-chat",
    "model_name": "yi-1.5-34b-chat",
    "aliases": [
      "yi-1.5-34b-chat"
    ],
    "provider": "01 AI",
    "context_window": null,
    "open_source": null,
    "arena_votes": 24142,
    "license_type": "Apache-2.0",
    "creator": null,
    "intelligence_score": null,
    "coding_score": null,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 1213.33,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": null,
    "tau2": null,
    "lcr": null,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": null,
    "gpqa": null,
    "scicode": null,
    "ifbench": null,
    "aime25": null,
    "critpt": null,
    "mmmu_pro": null,
    "livecodebench": null,
    "sources": [
      "LMSYS Chatbot Arena"
    ],
    "source_count": 1,
    "data_tier": 3,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.894590+00:00",
        "confidence": 1.0,
        "raw_name": "yi-1.5-34b-chat",
        "raw_scores": {
          "arena_elo": 1213.33,
          "arena_votes": 24142
        }
      }
    ],
    "benchmark_breakdown": {
      "LMSYS Chatbot Arena": {
        "arena_elo": 1213.33
      }
    },
    "confidence_score": 1.0,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "zephyr-orpo-141b-a35b-v01",
    "canonical_name": "zephyr-orpo-141b-A35b-v0.1",
    "model_name": "zephyr-orpo-141b-A35b-v0.1",
    "aliases": [
      "zephyr-orpo-141b-A35b-v0.1"
    ],
    "provider": "HuggingFace",
    "context_window": null,
    "open_source": null,
    "arena_votes": 4653,
    "license_type": "Apache 2.0",
    "creator": null,
    "intelligence_score": null,
    "coding_score": null,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 1212.74,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": null,
    "tau2": null,
    "lcr": null,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": null,
    "gpqa": null,
    "scicode": null,
    "ifbench": null,
    "aime25": null,
    "critpt": null,
    "mmmu_pro": null,
    "livecodebench": null,
    "sources": [
      "LMSYS Chatbot Arena"
    ],
    "source_count": 1,
    "data_tier": 3,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.894602+00:00",
        "confidence": 1.0,
        "raw_name": "zephyr-orpo-141b-A35b-v0.1",
        "raw_scores": {
          "arena_elo": 1212.74,
          "arena_votes": 4653
        }
      }
    ],
    "benchmark_breakdown": {
      "LMSYS Chatbot Arena": {
        "arena_elo": 1212.74
      }
    },
    "confidence_score": 1.0,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "llama-31-8b-instruct",
    "canonical_name": "llama-3.1-8b-instruct",
    "model_name": "llama-3.1-8b-instruct",
    "aliases": [
      "llama-3.1-8b-instruct"
    ],
    "provider": "Meta",
    "context_window": null,
    "open_source": null,
    "arena_votes": 49605,
    "license_type": "Llama 3.1 Community",
    "creator": null,
    "intelligence_score": null,
    "coding_score": null,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 1211.47,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": null,
    "tau2": null,
    "lcr": null,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": null,
    "gpqa": null,
    "scicode": null,
    "ifbench": null,
    "aime25": null,
    "critpt": null,
    "mmmu_pro": null,
    "livecodebench": null,
    "sources": [
      "LMSYS Chatbot Arena"
    ],
    "source_count": 1,
    "data_tier": 3,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.894615+00:00",
        "confidence": 1.0,
        "raw_name": "llama-3.1-8b-instruct",
        "raw_scores": {
          "arena_elo": 1211.47,
          "arena_votes": 49605
        }
      }
    ],
    "benchmark_breakdown": {
      "LMSYS Chatbot Arena": {
        "arena_elo": 1211.47
      }
    },
    "confidence_score": 1.0,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "granite-31-8b-instruct",
    "canonical_name": "granite-3.1-8b-instruct",
    "model_name": "granite-3.1-8b-instruct",
    "aliases": [
      "granite-3.1-8b-instruct"
    ],
    "provider": "IBM",
    "context_window": null,
    "open_source": null,
    "arena_votes": 3092,
    "license_type": "Apache 2.0",
    "creator": null,
    "intelligence_score": null,
    "coding_score": null,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 1208.56,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": null,
    "tau2": null,
    "lcr": null,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": null,
    "gpqa": null,
    "scicode": null,
    "ifbench": null,
    "aime25": null,
    "critpt": null,
    "mmmu_pro": null,
    "livecodebench": null,
    "sources": [
      "LMSYS Chatbot Arena"
    ],
    "source_count": 1,
    "data_tier": 3,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.894627+00:00",
        "confidence": 1.0,
        "raw_name": "granite-3.1-8b-instruct",
        "raw_scores": {
          "arena_elo": 1208.56,
          "arena_votes": 3092
        }
      }
    ],
    "benchmark_breakdown": {
      "LMSYS Chatbot Arena": {
        "arena_elo": 1208.56
      }
    },
    "confidence_score": 1.0,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "qwen15-32b-chat",
    "canonical_name": "qwen1.5-32b-chat",
    "model_name": "qwen1.5-32b-chat",
    "aliases": [
      "qwen1.5-32b-chat"
    ],
    "provider": "Alibaba",
    "context_window": null,
    "open_source": null,
    "arena_votes": 21744,
    "license_type": "Qianwen LICENSE",
    "creator": null,
    "intelligence_score": null,
    "coding_score": null,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 1203.87,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": null,
    "tau2": null,
    "lcr": null,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": null,
    "gpqa": null,
    "scicode": null,
    "ifbench": null,
    "aime25": null,
    "critpt": null,
    "mmmu_pro": null,
    "livecodebench": null,
    "sources": [
      "LMSYS Chatbot Arena"
    ],
    "source_count": 1,
    "data_tier": 3,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.894639+00:00",
        "confidence": 1.0,
        "raw_name": "qwen1.5-32b-chat",
        "raw_scores": {
          "arena_elo": 1203.87,
          "arena_votes": 21744
        }
      }
    ],
    "benchmark_breakdown": {
      "LMSYS Chatbot Arena": {
        "arena_elo": 1203.87
      }
    },
    "confidence_score": 1.0,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "gpt-35-turbo-1106",
    "canonical_name": "gpt-3.5-turbo-1106",
    "model_name": "gpt-3.5-turbo-1106",
    "aliases": [
      "gpt-3.5-turbo-1106"
    ],
    "provider": "OpenAI",
    "context_window": null,
    "open_source": null,
    "arena_votes": 16616,
    "license_type": "Proprietary",
    "creator": null,
    "intelligence_score": null,
    "coding_score": null,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 1202.4,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": null,
    "tau2": null,
    "lcr": null,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": null,
    "gpqa": null,
    "scicode": null,
    "ifbench": null,
    "aime25": null,
    "critpt": null,
    "mmmu_pro": null,
    "livecodebench": null,
    "sources": [
      "LMSYS Chatbot Arena"
    ],
    "source_count": 1,
    "data_tier": 3,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.894652+00:00",
        "confidence": 1.0,
        "raw_name": "gpt-3.5-turbo-1106",
        "raw_scores": {
          "arena_elo": 1202.4,
          "arena_votes": 16616
        }
      }
    ],
    "benchmark_breakdown": {
      "LMSYS Chatbot Arena": {
        "arena_elo": 1202.4
      }
    },
    "confidence_score": 1.0,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "gemma-2-2b-it",
    "canonical_name": "gemma-2-2b-it",
    "model_name": "gemma-2-2b-it",
    "aliases": [
      "gemma-2-2b-it"
    ],
    "provider": "Google",
    "context_window": null,
    "open_source": null,
    "arena_votes": 46618,
    "license_type": "Gemma license",
    "creator": null,
    "intelligence_score": null,
    "coding_score": null,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 1198.7,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": null,
    "tau2": null,
    "lcr": null,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": null,
    "gpqa": null,
    "scicode": null,
    "ifbench": null,
    "aime25": null,
    "critpt": null,
    "mmmu_pro": null,
    "livecodebench": null,
    "sources": [
      "LMSYS Chatbot Arena"
    ],
    "source_count": 1,
    "data_tier": 3,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.894664+00:00",
        "confidence": 1.0,
        "raw_name": "gemma-2-2b-it",
        "raw_scores": {
          "arena_elo": 1198.7,
          "arena_votes": 46618
        }
      }
    ],
    "benchmark_breakdown": {
      "LMSYS Chatbot Arena": {
        "arena_elo": 1198.7
      }
    },
    "confidence_score": 1.0,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "phi-3-medium-4k-instruct",
    "canonical_name": "phi-3-medium-4k-instruct",
    "model_name": "phi-3-medium-4k-instruct",
    "aliases": [
      "phi-3-medium-4k-instruct"
    ],
    "provider": "Microsoft",
    "context_window": null,
    "open_source": null,
    "arena_votes": 25055,
    "license_type": "MIT",
    "creator": null,
    "intelligence_score": null,
    "coding_score": null,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 1197.96,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": null,
    "tau2": null,
    "lcr": null,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": null,
    "gpqa": null,
    "scicode": null,
    "ifbench": null,
    "aime25": null,
    "critpt": null,
    "mmmu_pro": null,
    "livecodebench": null,
    "sources": [
      "LMSYS Chatbot Arena"
    ],
    "source_count": 1,
    "data_tier": 3,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.894677+00:00",
        "confidence": 1.0,
        "raw_name": "phi-3-medium-4k-instruct",
        "raw_scores": {
          "arena_elo": 1197.96,
          "arena_votes": 25055
        }
      }
    ],
    "benchmark_breakdown": {
      "LMSYS Chatbot Arena": {
        "arena_elo": 1197.96
      }
    },
    "confidence_score": 1.0,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "mixtral-8x7b-instruct-v01",
    "canonical_name": "mixtral-8x7b-instruct-v0.1",
    "model_name": "mixtral-8x7b-instruct-v0.1",
    "aliases": [
      "mixtral-8x7b-instruct-v0.1"
    ],
    "provider": "Mistral",
    "context_window": null,
    "open_source": null,
    "arena_votes": 73505,
    "license_type": "Apache 2.0",
    "creator": null,
    "intelligence_score": null,
    "coding_score": null,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 1197.12,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": null,
    "tau2": null,
    "lcr": null,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": null,
    "gpqa": null,
    "scicode": null,
    "ifbench": null,
    "aime25": null,
    "critpt": null,
    "mmmu_pro": null,
    "livecodebench": null,
    "sources": [
      "LMSYS Chatbot Arena"
    ],
    "source_count": 1,
    "data_tier": 3,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.894689+00:00",
        "confidence": 1.0,
        "raw_name": "mixtral-8x7b-instruct-v0.1",
        "raw_scores": {
          "arena_elo": 1197.12,
          "arena_votes": 73505
        }
      }
    ],
    "benchmark_breakdown": {
      "LMSYS Chatbot Arena": {
        "arena_elo": 1197.12
      }
    },
    "confidence_score": 1.0,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "dbrx-instruct-preview",
    "canonical_name": "dbrx-instruct-preview",
    "model_name": "dbrx-instruct-preview",
    "aliases": [
      "dbrx-instruct-preview"
    ],
    "provider": "Databricks",
    "context_window": null,
    "open_source": null,
    "arena_votes": 32196,
    "license_type": "DBRX LICENSE",
    "creator": null,
    "intelligence_score": null,
    "coding_score": null,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 1194.93,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": null,
    "tau2": null,
    "lcr": null,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": null,
    "gpqa": null,
    "scicode": null,
    "ifbench": null,
    "aime25": null,
    "critpt": null,
    "mmmu_pro": null,
    "livecodebench": null,
    "sources": [
      "LMSYS Chatbot Arena"
    ],
    "source_count": 1,
    "data_tier": 3,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.894702+00:00",
        "confidence": 1.0,
        "raw_name": "dbrx-instruct-preview",
        "raw_scores": {
          "arena_elo": 1194.93,
          "arena_votes": 32196
        }
      }
    ],
    "benchmark_breakdown": {
      "LMSYS Chatbot Arena": {
        "arena_elo": 1194.93
      }
    },
    "confidence_score": 1.0,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "internlm2_5-20b-chat",
    "canonical_name": "internlm2_5-20b-chat",
    "model_name": "internlm2_5-20b-chat",
    "aliases": [
      "internlm2_5-20b-chat"
    ],
    "provider": "InternLM",
    "context_window": null,
    "open_source": null,
    "arena_votes": 9902,
    "license_type": "Other",
    "creator": null,
    "intelligence_score": null,
    "coding_score": null,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 1191.28,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": null,
    "tau2": null,
    "lcr": null,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": null,
    "gpqa": null,
    "scicode": null,
    "ifbench": null,
    "aime25": null,
    "critpt": null,
    "mmmu_pro": null,
    "livecodebench": null,
    "sources": [
      "LMSYS Chatbot Arena"
    ],
    "source_count": 1,
    "data_tier": 3,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.894714+00:00",
        "confidence": 1.0,
        "raw_name": "internlm2_5-20b-chat",
        "raw_scores": {
          "arena_elo": 1191.28,
          "arena_votes": 9902
        }
      }
    ],
    "benchmark_breakdown": {
      "LMSYS Chatbot Arena": {
        "arena_elo": 1191.28
      }
    },
    "confidence_score": 1.0,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "qwen15-14b-chat",
    "canonical_name": "qwen1.5-14b-chat",
    "model_name": "qwen1.5-14b-chat",
    "aliases": [
      "qwen1.5-14b-chat"
    ],
    "provider": "Alibaba",
    "context_window": null,
    "open_source": null,
    "arena_votes": 17841,
    "license_type": "Qianwen LICENSE",
    "creator": null,
    "intelligence_score": null,
    "coding_score": null,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 1190.83,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": null,
    "tau2": null,
    "lcr": null,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": null,
    "gpqa": null,
    "scicode": null,
    "ifbench": null,
    "aime25": null,
    "critpt": null,
    "mmmu_pro": null,
    "livecodebench": null,
    "sources": [
      "LMSYS Chatbot Arena"
    ],
    "source_count": 1,
    "data_tier": 3,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.894727+00:00",
        "confidence": 1.0,
        "raw_name": "qwen1.5-14b-chat",
        "raw_scores": {
          "arena_elo": 1190.83,
          "arena_votes": 17841
        }
      }
    ],
    "benchmark_breakdown": {
      "LMSYS Chatbot Arena": {
        "arena_elo": 1190.83
      }
    },
    "confidence_score": 1.0,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "wizardlm-70b",
    "canonical_name": "wizardlm-70b",
    "model_name": "wizardlm-70b",
    "aliases": [
      "wizardlm-70b"
    ],
    "provider": "Microsoft",
    "context_window": null,
    "open_source": null,
    "arena_votes": 8214,
    "license_type": "Llama 2 Community",
    "creator": null,
    "intelligence_score": null,
    "coding_score": null,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 1184.64,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": null,
    "tau2": null,
    "lcr": null,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": null,
    "gpqa": null,
    "scicode": null,
    "ifbench": null,
    "aime25": null,
    "critpt": null,
    "mmmu_pro": null,
    "livecodebench": null,
    "sources": [
      "LMSYS Chatbot Arena"
    ],
    "source_count": 1,
    "data_tier": 3,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.894739+00:00",
        "confidence": 1.0,
        "raw_name": "wizardlm-70b",
        "raw_scores": {
          "arena_elo": 1184.64,
          "arena_votes": 8214
        }
      }
    ],
    "benchmark_breakdown": {
      "LMSYS Chatbot Arena": {
        "arena_elo": 1184.64
      }
    },
    "confidence_score": 1.0,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "yi-34b-chat",
    "canonical_name": "yi-34b-chat",
    "model_name": "yi-34b-chat",
    "aliases": [
      "yi-34b-chat"
    ],
    "provider": "01 AI",
    "context_window": null,
    "open_source": null,
    "arena_votes": 15483,
    "license_type": "Yi License",
    "creator": null,
    "intelligence_score": null,
    "coding_score": null,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 1183.75,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": null,
    "tau2": null,
    "lcr": null,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": null,
    "gpqa": null,
    "scicode": null,
    "ifbench": null,
    "aime25": null,
    "critpt": null,
    "mmmu_pro": null,
    "livecodebench": null,
    "sources": [
      "LMSYS Chatbot Arena"
    ],
    "source_count": 1,
    "data_tier": 3,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.894764+00:00",
        "confidence": 1.0,
        "raw_name": "yi-34b-chat",
        "raw_scores": {
          "arena_elo": 1183.75,
          "arena_votes": 15483
        }
      }
    ],
    "benchmark_breakdown": {
      "LMSYS Chatbot Arena": {
        "arena_elo": 1183.75
      }
    },
    "confidence_score": 1.0,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "openchat-35-0106",
    "canonical_name": "openchat-3.5-0106",
    "model_name": "openchat-3.5-0106",
    "aliases": [
      "openchat-3.5-0106"
    ],
    "provider": "OpenChat",
    "context_window": null,
    "open_source": null,
    "arena_votes": 12636,
    "license_type": "Apache-2.0",
    "creator": null,
    "intelligence_score": null,
    "coding_score": null,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 1182.27,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": null,
    "tau2": null,
    "lcr": null,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": null,
    "gpqa": null,
    "scicode": null,
    "ifbench": null,
    "aime25": null,
    "critpt": null,
    "mmmu_pro": null,
    "livecodebench": null,
    "sources": [
      "LMSYS Chatbot Arena"
    ],
    "source_count": 1,
    "data_tier": 3,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.894777+00:00",
        "confidence": 1.0,
        "raw_name": "openchat-3.5-0106",
        "raw_scores": {
          "arena_elo": 1182.27,
          "arena_votes": 12636
        }
      }
    ],
    "benchmark_breakdown": {
      "LMSYS Chatbot Arena": {
        "arena_elo": 1182.27
      }
    },
    "confidence_score": 1.0,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "granite-30-8b-instruct",
    "canonical_name": "granite-3.0-8b-instruct",
    "model_name": "granite-3.0-8b-instruct",
    "aliases": [
      "granite-3.0-8b-instruct"
    ],
    "provider": "IBM",
    "context_window": null,
    "open_source": null,
    "arena_votes": 6643,
    "license_type": "Apache 2.0",
    "creator": null,
    "intelligence_score": null,
    "coding_score": null,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 1181.92,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": null,
    "tau2": null,
    "lcr": null,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": null,
    "gpqa": null,
    "scicode": null,
    "ifbench": null,
    "aime25": null,
    "critpt": null,
    "mmmu_pro": null,
    "livecodebench": null,
    "sources": [
      "LMSYS Chatbot Arena"
    ],
    "source_count": 1,
    "data_tier": 3,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.894802+00:00",
        "confidence": 1.0,
        "raw_name": "granite-3.0-8b-instruct",
        "raw_scores": {
          "arena_elo": 1181.92,
          "arena_votes": 6643
        }
      }
    ],
    "benchmark_breakdown": {
      "LMSYS Chatbot Arena": {
        "arena_elo": 1181.92
      }
    },
    "confidence_score": 1.0,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "gemma-11-7b-it",
    "canonical_name": "gemma-1.1-7b-it",
    "model_name": "gemma-1.1-7b-it",
    "aliases": [
      "gemma-1.1-7b-it"
    ],
    "provider": "Google",
    "context_window": null,
    "open_source": null,
    "arena_votes": 23893,
    "license_type": "Gemma license",
    "creator": null,
    "intelligence_score": null,
    "coding_score": null,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 1180.13,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": null,
    "tau2": null,
    "lcr": null,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": null,
    "gpqa": null,
    "scicode": null,
    "ifbench": null,
    "aime25": null,
    "critpt": null,
    "mmmu_pro": null,
    "livecodebench": null,
    "sources": [
      "LMSYS Chatbot Arena"
    ],
    "source_count": 1,
    "data_tier": 3,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.894814+00:00",
        "confidence": 1.0,
        "raw_name": "gemma-1.1-7b-it",
        "raw_scores": {
          "arena_elo": 1180.13,
          "arena_votes": 23893
        }
      }
    ],
    "benchmark_breakdown": {
      "LMSYS Chatbot Arena": {
        "arena_elo": 1180.13
      }
    },
    "confidence_score": 1.0,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "snowflake-arctic-instruct",
    "canonical_name": "snowflake-arctic-instruct",
    "model_name": "snowflake-arctic-instruct",
    "aliases": [
      "snowflake-arctic-instruct"
    ],
    "provider": "Snowflake",
    "context_window": null,
    "open_source": null,
    "arena_votes": 32836,
    "license_type": "Apache 2.0",
    "creator": null,
    "intelligence_score": null,
    "coding_score": null,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 1179.46,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": null,
    "tau2": null,
    "lcr": null,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": null,
    "gpqa": null,
    "scicode": null,
    "ifbench": null,
    "aime25": null,
    "critpt": null,
    "mmmu_pro": null,
    "livecodebench": null,
    "sources": [
      "LMSYS Chatbot Arena"
    ],
    "source_count": 1,
    "data_tier": 3,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.894826+00:00",
        "confidence": 1.0,
        "raw_name": "snowflake-arctic-instruct",
        "raw_scores": {
          "arena_elo": 1179.46,
          "arena_votes": 32836
        }
      }
    ],
    "benchmark_breakdown": {
      "LMSYS Chatbot Arena": {
        "arena_elo": 1179.46
      }
    },
    "confidence_score": 1.0,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "granite-31-2b-instruct",
    "canonical_name": "granite-3.1-2b-instruct",
    "model_name": "granite-3.1-2b-instruct",
    "aliases": [
      "granite-3.1-2b-instruct"
    ],
    "provider": "IBM",
    "context_window": null,
    "open_source": null,
    "arena_votes": 3191,
    "license_type": "Apache 2.0",
    "creator": null,
    "intelligence_score": null,
    "coding_score": null,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 1179.3,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": null,
    "tau2": null,
    "lcr": null,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": null,
    "gpqa": null,
    "scicode": null,
    "ifbench": null,
    "aime25": null,
    "critpt": null,
    "mmmu_pro": null,
    "livecodebench": null,
    "sources": [
      "LMSYS Chatbot Arena"
    ],
    "source_count": 1,
    "data_tier": 3,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.894840+00:00",
        "confidence": 1.0,
        "raw_name": "granite-3.1-2b-instruct",
        "raw_scores": {
          "arena_elo": 1179.3,
          "arena_votes": 3191
        }
      }
    ],
    "benchmark_breakdown": {
      "LMSYS Chatbot Arena": {
        "arena_elo": 1179.3
      }
    },
    "confidence_score": 1.0,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "tulu-2-dpo-70b",
    "canonical_name": "tulu-2-dpo-70b",
    "model_name": "tulu-2-dpo-70b",
    "aliases": [
      "tulu-2-dpo-70b"
    ],
    "provider": "AllenAI/UW",
    "context_window": null,
    "open_source": null,
    "arena_votes": 6534,
    "license_type": "AI2 ImpACT Low-risk",
    "creator": null,
    "intelligence_score": null,
    "coding_score": null,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 1178.01,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": null,
    "tau2": null,
    "lcr": null,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": null,
    "gpqa": null,
    "scicode": null,
    "ifbench": null,
    "aime25": null,
    "critpt": null,
    "mmmu_pro": null,
    "livecodebench": null,
    "sources": [
      "LMSYS Chatbot Arena"
    ],
    "source_count": 1,
    "data_tier": 3,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.894852+00:00",
        "confidence": 1.0,
        "raw_name": "tulu-2-dpo-70b",
        "raw_scores": {
          "arena_elo": 1178.01,
          "arena_votes": 6534
        }
      }
    ],
    "benchmark_breakdown": {
      "LMSYS Chatbot Arena": {
        "arena_elo": 1178.01
      }
    },
    "confidence_score": 1.0,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "openhermes-25-mistral-7b",
    "canonical_name": "openhermes-2.5-mistral-7b",
    "model_name": "openhermes-2.5-mistral-7b",
    "aliases": [
      "openhermes-2.5-mistral-7b"
    ],
    "provider": "NousResearch",
    "context_window": null,
    "open_source": null,
    "arena_votes": 5006,
    "license_type": "Apache-2.0",
    "creator": null,
    "intelligence_score": null,
    "coding_score": null,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 1175.12,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": null,
    "tau2": null,
    "lcr": null,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": null,
    "gpqa": null,
    "scicode": null,
    "ifbench": null,
    "aime25": null,
    "critpt": null,
    "mmmu_pro": null,
    "livecodebench": null,
    "sources": [
      "LMSYS Chatbot Arena"
    ],
    "source_count": 1,
    "data_tier": 3,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.894865+00:00",
        "confidence": 1.0,
        "raw_name": "openhermes-2.5-mistral-7b",
        "raw_scores": {
          "arena_elo": 1175.12,
          "arena_votes": 5006
        }
      }
    ],
    "benchmark_breakdown": {
      "LMSYS Chatbot Arena": {
        "arena_elo": 1175.12
      }
    },
    "confidence_score": 1.0,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "vicuna-33b",
    "canonical_name": "vicuna-33b",
    "model_name": "vicuna-33b",
    "aliases": [
      "vicuna-33b"
    ],
    "provider": "LMSYS",
    "context_window": null,
    "open_source": null,
    "arena_votes": 22479,
    "license_type": "Non-commercial",
    "creator": null,
    "intelligence_score": null,
    "coding_score": null,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 1172.78,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": null,
    "tau2": null,
    "lcr": null,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": null,
    "gpqa": null,
    "scicode": null,
    "ifbench": null,
    "aime25": null,
    "critpt": null,
    "mmmu_pro": null,
    "livecodebench": null,
    "sources": [
      "LMSYS Chatbot Arena"
    ],
    "source_count": 1,
    "data_tier": 3,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.894877+00:00",
        "confidence": 1.0,
        "raw_name": "vicuna-33b",
        "raw_scores": {
          "arena_elo": 1172.78,
          "arena_votes": 22479
        }
      }
    ],
    "benchmark_breakdown": {
      "LMSYS Chatbot Arena": {
        "arena_elo": 1172.78
      }
    },
    "confidence_score": 1.0,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "starling-lm-7b-beta",
    "canonical_name": "starling-lm-7b-beta",
    "model_name": "starling-lm-7b-beta",
    "aliases": [
      "starling-lm-7b-beta"
    ],
    "provider": "Nexusflow",
    "context_window": null,
    "open_source": null,
    "arena_votes": 16057,
    "license_type": "Apache-2.0",
    "creator": null,
    "intelligence_score": null,
    "coding_score": null,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 1171.59,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": null,
    "tau2": null,
    "lcr": null,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": null,
    "gpqa": null,
    "scicode": null,
    "ifbench": null,
    "aime25": null,
    "critpt": null,
    "mmmu_pro": null,
    "livecodebench": null,
    "sources": [
      "LMSYS Chatbot Arena"
    ],
    "source_count": 1,
    "data_tier": 3,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.894890+00:00",
        "confidence": 1.0,
        "raw_name": "starling-lm-7b-beta",
        "raw_scores": {
          "arena_elo": 1171.59,
          "arena_votes": 16057
        }
      }
    ],
    "benchmark_breakdown": {
      "LMSYS Chatbot Arena": {
        "arena_elo": 1171.59
      }
    },
    "confidence_score": 1.0,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "phi-3-small-8k-instruct",
    "canonical_name": "phi-3-small-8k-instruct",
    "model_name": "phi-3-small-8k-instruct",
    "aliases": [
      "phi-3-small-8k-instruct"
    ],
    "provider": "Microsoft",
    "context_window": null,
    "open_source": null,
    "arena_votes": 17763,
    "license_type": "MIT",
    "creator": null,
    "intelligence_score": null,
    "coding_score": null,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 1171.16,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": null,
    "tau2": null,
    "lcr": null,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": null,
    "gpqa": null,
    "scicode": null,
    "ifbench": null,
    "aime25": null,
    "critpt": null,
    "mmmu_pro": null,
    "livecodebench": null,
    "sources": [
      "LMSYS Chatbot Arena"
    ],
    "source_count": 1,
    "data_tier": 3,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.894902+00:00",
        "confidence": 1.0,
        "raw_name": "phi-3-small-8k-instruct",
        "raw_scores": {
          "arena_elo": 1171.16,
          "arena_votes": 17763
        }
      }
    ],
    "benchmark_breakdown": {
      "LMSYS Chatbot Arena": {
        "arena_elo": 1171.16
      }
    },
    "confidence_score": 1.0,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "llama-2-70b-chat",
    "canonical_name": "llama-2-70b-chat",
    "model_name": "llama-2-70b-chat",
    "aliases": [
      "llama-2-70b-chat"
    ],
    "provider": "Meta",
    "context_window": null,
    "open_source": null,
    "arena_votes": 38491,
    "license_type": "Llama 2 Community",
    "creator": null,
    "intelligence_score": null,
    "coding_score": null,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 1170.79,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": null,
    "tau2": null,
    "lcr": null,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": null,
    "gpqa": null,
    "scicode": null,
    "ifbench": null,
    "aime25": null,
    "critpt": null,
    "mmmu_pro": null,
    "livecodebench": null,
    "sources": [
      "LMSYS Chatbot Arena"
    ],
    "source_count": 1,
    "data_tier": 3,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.894914+00:00",
        "confidence": 1.0,
        "raw_name": "llama-2-70b-chat",
        "raw_scores": {
          "arena_elo": 1170.79,
          "arena_votes": 38491
        }
      }
    ],
    "benchmark_breakdown": {
      "LMSYS Chatbot Arena": {
        "arena_elo": 1170.79
      }
    },
    "confidence_score": 1.0,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "starling-lm-7b-alpha",
    "canonical_name": "starling-lm-7b-alpha",
    "model_name": "starling-lm-7b-alpha",
    "aliases": [
      "starling-lm-7b-alpha"
    ],
    "provider": "UC Berkeley",
    "context_window": null,
    "open_source": null,
    "arena_votes": 10224,
    "license_type": "CC-BY-NC-4.0",
    "creator": null,
    "intelligence_score": null,
    "coding_score": null,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 1167.52,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": null,
    "tau2": null,
    "lcr": null,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": null,
    "gpqa": null,
    "scicode": null,
    "ifbench": null,
    "aime25": null,
    "critpt": null,
    "mmmu_pro": null,
    "livecodebench": null,
    "sources": [
      "LMSYS Chatbot Arena"
    ],
    "source_count": 1,
    "data_tier": 3,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.894926+00:00",
        "confidence": 1.0,
        "raw_name": "starling-lm-7b-alpha",
        "raw_scores": {
          "arena_elo": 1167.52,
          "arena_votes": 10224
        }
      }
    ],
    "benchmark_breakdown": {
      "LMSYS Chatbot Arena": {
        "arena_elo": 1167.52
      }
    },
    "confidence_score": 1.0,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "llama-32-3b-instruct",
    "canonical_name": "llama-3.2-3b-instruct",
    "model_name": "llama-3.2-3b-instruct",
    "aliases": [
      "llama-3.2-3b-instruct"
    ],
    "provider": "Meta",
    "context_window": null,
    "open_source": null,
    "arena_votes": 7936,
    "license_type": "Llama 3.2",
    "creator": null,
    "intelligence_score": null,
    "coding_score": null,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 1166.61,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": null,
    "tau2": null,
    "lcr": null,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": null,
    "gpqa": null,
    "scicode": null,
    "ifbench": null,
    "aime25": null,
    "critpt": null,
    "mmmu_pro": null,
    "livecodebench": null,
    "sources": [
      "LMSYS Chatbot Arena"
    ],
    "source_count": 1,
    "data_tier": 3,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.894939+00:00",
        "confidence": 1.0,
        "raw_name": "llama-3.2-3b-instruct",
        "raw_scores": {
          "arena_elo": 1166.61,
          "arena_votes": 7936
        }
      }
    ],
    "benchmark_breakdown": {
      "LMSYS Chatbot Arena": {
        "arena_elo": 1166.61
      }
    },
    "confidence_score": 1.0,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "nous-hermes-2-mixtral-8x7b-dpo",
    "canonical_name": "nous-hermes-2-mixtral-8x7b-dpo",
    "model_name": "nous-hermes-2-mixtral-8x7b-dpo",
    "aliases": [
      "nous-hermes-2-mixtral-8x7b-dpo"
    ],
    "provider": "NousResearch",
    "context_window": null,
    "open_source": null,
    "arena_votes": 3776,
    "license_type": "Apache-2.0",
    "creator": null,
    "intelligence_score": null,
    "coding_score": null,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 1164.66,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": null,
    "tau2": null,
    "lcr": null,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": null,
    "gpqa": null,
    "scicode": null,
    "ifbench": null,
    "aime25": null,
    "critpt": null,
    "mmmu_pro": null,
    "livecodebench": null,
    "sources": [
      "LMSYS Chatbot Arena"
    ],
    "source_count": 1,
    "data_tier": 3,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.894952+00:00",
        "confidence": 1.0,
        "raw_name": "nous-hermes-2-mixtral-8x7b-dpo",
        "raw_scores": {
          "arena_elo": 1164.66,
          "arena_votes": 3776
        }
      }
    ],
    "benchmark_breakdown": {
      "LMSYS Chatbot Arena": {
        "arena_elo": 1164.66
      }
    },
    "confidence_score": 1.0,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "granite-30-2b-instruct",
    "canonical_name": "granite-3.0-2b-instruct",
    "model_name": "granite-3.0-2b-instruct",
    "aliases": [
      "granite-3.0-2b-instruct"
    ],
    "provider": "IBM",
    "context_window": null,
    "open_source": null,
    "arena_votes": 6837,
    "license_type": "Apache 2.0",
    "creator": null,
    "intelligence_score": null,
    "coding_score": null,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 1155.93,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": null,
    "tau2": null,
    "lcr": null,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": null,
    "gpqa": null,
    "scicode": null,
    "ifbench": null,
    "aime25": null,
    "critpt": null,
    "mmmu_pro": null,
    "livecodebench": null,
    "sources": [
      "LMSYS Chatbot Arena"
    ],
    "source_count": 1,
    "data_tier": 3,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.894977+00:00",
        "confidence": 1.0,
        "raw_name": "granite-3.0-2b-instruct",
        "raw_scores": {
          "arena_elo": 1155.93,
          "arena_votes": 6837
        }
      }
    ],
    "benchmark_breakdown": {
      "LMSYS Chatbot Arena": {
        "arena_elo": 1155.93
      }
    },
    "confidence_score": 1.0,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "llama2-70b-steerlm-chat",
    "canonical_name": "llama2-70b-steerlm-chat",
    "model_name": "llama2-70b-steerlm-chat",
    "aliases": [
      "llama2-70b-steerlm-chat"
    ],
    "provider": "Nvidia",
    "context_window": null,
    "open_source": null,
    "arena_votes": 3584,
    "license_type": "Llama 2 Community",
    "creator": null,
    "intelligence_score": null,
    "coding_score": null,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 1155.29,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": null,
    "tau2": null,
    "lcr": null,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": null,
    "gpqa": null,
    "scicode": null,
    "ifbench": null,
    "aime25": null,
    "critpt": null,
    "mmmu_pro": null,
    "livecodebench": null,
    "sources": [
      "LMSYS Chatbot Arena"
    ],
    "source_count": 1,
    "data_tier": 3,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.894989+00:00",
        "confidence": 1.0,
        "raw_name": "llama2-70b-steerlm-chat",
        "raw_scores": {
          "arena_elo": 1155.29,
          "arena_votes": 3584
        }
      }
    ],
    "benchmark_breakdown": {
      "LMSYS Chatbot Arena": {
        "arena_elo": 1155.29
      }
    },
    "confidence_score": 1.0,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "solar-107b-instruct-v10",
    "canonical_name": "solar-10.7b-instruct-v1.0",
    "model_name": "solar-10.7b-instruct-v1.0",
    "aliases": [
      "solar-10.7b-instruct-v1.0"
    ],
    "provider": "Upstage AI",
    "context_window": null,
    "open_source": null,
    "arena_votes": 4155,
    "license_type": "CC-BY-NC-4.0",
    "creator": null,
    "intelligence_score": null,
    "coding_score": null,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 1152.34,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": null,
    "tau2": null,
    "lcr": null,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": null,
    "gpqa": null,
    "scicode": null,
    "ifbench": null,
    "aime25": null,
    "critpt": null,
    "mmmu_pro": null,
    "livecodebench": null,
    "sources": [
      "LMSYS Chatbot Arena"
    ],
    "source_count": 1,
    "data_tier": 3,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.895001+00:00",
        "confidence": 1.0,
        "raw_name": "solar-10.7b-instruct-v1.0",
        "raw_scores": {
          "arena_elo": 1152.34,
          "arena_votes": 4155
        }
      }
    ],
    "benchmark_breakdown": {
      "LMSYS Chatbot Arena": {
        "arena_elo": 1152.34
      }
    },
    "confidence_score": 1.0,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "dolphin-221-mistral-7b",
    "canonical_name": "dolphin-2.2.1-mistral-7b",
    "model_name": "dolphin-2.2.1-mistral-7b",
    "aliases": [
      "dolphin-2.2.1-mistral-7b"
    ],
    "provider": "Cognitive Computations",
    "context_window": null,
    "open_source": null,
    "arena_votes": 1679,
    "license_type": "Apache-2.0",
    "creator": null,
    "intelligence_score": null,
    "coding_score": null,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 1151.95,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": null,
    "tau2": null,
    "lcr": null,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": null,
    "gpqa": null,
    "scicode": null,
    "ifbench": null,
    "aime25": null,
    "critpt": null,
    "mmmu_pro": null,
    "livecodebench": null,
    "sources": [
      "LMSYS Chatbot Arena"
    ],
    "source_count": 1,
    "data_tier": 3,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.895014+00:00",
        "confidence": 1.0,
        "raw_name": "dolphin-2.2.1-mistral-7b",
        "raw_scores": {
          "arena_elo": 1151.95,
          "arena_votes": 1679
        }
      }
    ],
    "benchmark_breakdown": {
      "LMSYS Chatbot Arena": {
        "arena_elo": 1151.95
      }
    },
    "confidence_score": 1.0,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "mpt-30b-chat",
    "canonical_name": "mpt-30b-chat",
    "model_name": "mpt-30b-chat",
    "aliases": [
      "mpt-30b-chat"
    ],
    "provider": "MosaicML",
    "context_window": null,
    "open_source": null,
    "arena_votes": 2571,
    "license_type": "CC-BY-NC-SA-4.0",
    "creator": null,
    "intelligence_score": null,
    "coding_score": null,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 1150.09,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": null,
    "tau2": null,
    "lcr": null,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": null,
    "gpqa": null,
    "scicode": null,
    "ifbench": null,
    "aime25": null,
    "critpt": null,
    "mmmu_pro": null,
    "livecodebench": null,
    "sources": [
      "LMSYS Chatbot Arena"
    ],
    "source_count": 1,
    "data_tier": 3,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.895026+00:00",
        "confidence": 1.0,
        "raw_name": "mpt-30b-chat",
        "raw_scores": {
          "arena_elo": 1150.09,
          "arena_votes": 2571
        }
      }
    ],
    "benchmark_breakdown": {
      "LMSYS Chatbot Arena": {
        "arena_elo": 1150.09
      }
    },
    "confidence_score": 1.0,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "mistral-7b-instruct-v02",
    "canonical_name": "mistral-7b-instruct-v0.2",
    "model_name": "mistral-7b-instruct-v0.2",
    "aliases": [
      "mistral-7b-instruct-v0.2"
    ],
    "provider": "Mistral",
    "context_window": null,
    "open_source": null,
    "arena_votes": 19402,
    "license_type": "Apache-2.0",
    "creator": null,
    "intelligence_score": null,
    "coding_score": null,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 1149.61,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": null,
    "tau2": null,
    "lcr": null,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": null,
    "gpqa": null,
    "scicode": null,
    "ifbench": null,
    "aime25": null,
    "critpt": null,
    "mmmu_pro": null,
    "livecodebench": null,
    "sources": [
      "LMSYS Chatbot Arena"
    ],
    "source_count": 1,
    "data_tier": 3,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.895038+00:00",
        "confidence": 1.0,
        "raw_name": "mistral-7b-instruct-v0.2",
        "raw_scores": {
          "arena_elo": 1149.61,
          "arena_votes": 19402
        }
      }
    ],
    "benchmark_breakdown": {
      "LMSYS Chatbot Arena": {
        "arena_elo": 1149.61
      }
    },
    "confidence_score": 1.0,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "wizardlm-13b",
    "canonical_name": "wizardlm-13b",
    "model_name": "wizardlm-13b",
    "aliases": [
      "wizardlm-13b"
    ],
    "provider": "Microsoft",
    "context_window": null,
    "open_source": null,
    "arena_votes": 7046,
    "license_type": "Llama 2 Community",
    "creator": null,
    "intelligence_score": null,
    "coding_score": null,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 1149.12,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": null,
    "tau2": null,
    "lcr": null,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": null,
    "gpqa": null,
    "scicode": null,
    "ifbench": null,
    "aime25": null,
    "critpt": null,
    "mmmu_pro": null,
    "livecodebench": null,
    "sources": [
      "LMSYS Chatbot Arena"
    ],
    "source_count": 1,
    "data_tier": 3,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.895051+00:00",
        "confidence": 1.0,
        "raw_name": "wizardlm-13b",
        "raw_scores": {
          "arena_elo": 1149.12,
          "arena_votes": 7046
        }
      }
    ],
    "benchmark_breakdown": {
      "LMSYS Chatbot Arena": {
        "arena_elo": 1149.12
      }
    },
    "confidence_score": 1.0,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "falcon-180b-chat",
    "canonical_name": "falcon-180b-chat",
    "model_name": "falcon-180b-chat",
    "aliases": [
      "falcon-180b-chat"
    ],
    "provider": "TII",
    "context_window": null,
    "open_source": null,
    "arena_votes": 1295,
    "license_type": "Falcon-180B TII License",
    "creator": null,
    "intelligence_score": null,
    "coding_score": null,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 1146.96,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": null,
    "tau2": null,
    "lcr": null,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": null,
    "gpqa": null,
    "scicode": null,
    "ifbench": null,
    "aime25": null,
    "critpt": null,
    "mmmu_pro": null,
    "livecodebench": null,
    "sources": [
      "LMSYS Chatbot Arena"
    ],
    "source_count": 1,
    "data_tier": 3,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.895063+00:00",
        "confidence": 1.0,
        "raw_name": "falcon-180b-chat",
        "raw_scores": {
          "arena_elo": 1146.96,
          "arena_votes": 1295
        }
      }
    ],
    "benchmark_breakdown": {
      "LMSYS Chatbot Arena": {
        "arena_elo": 1146.96
      }
    },
    "confidence_score": 1.0,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "qwen15-7b-chat",
    "canonical_name": "qwen1.5-7b-chat",
    "model_name": "qwen1.5-7b-chat",
    "aliases": [
      "qwen1.5-7b-chat"
    ],
    "provider": "Alibaba",
    "context_window": null,
    "open_source": null,
    "arena_votes": 4735,
    "license_type": "Qianwen LICENSE",
    "creator": null,
    "intelligence_score": null,
    "coding_score": null,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 1143.86,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": null,
    "tau2": null,
    "lcr": null,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": null,
    "gpqa": null,
    "scicode": null,
    "ifbench": null,
    "aime25": null,
    "critpt": null,
    "mmmu_pro": null,
    "livecodebench": null,
    "sources": [
      "LMSYS Chatbot Arena"
    ],
    "source_count": 1,
    "data_tier": 3,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.895075+00:00",
        "confidence": 1.0,
        "raw_name": "qwen1.5-7b-chat",
        "raw_scores": {
          "arena_elo": 1143.86,
          "arena_votes": 4735
        }
      }
    ],
    "benchmark_breakdown": {
      "LMSYS Chatbot Arena": {
        "arena_elo": 1143.86
      }
    },
    "confidence_score": 1.0,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "phi-3-mini-4k-instruct-june-2024",
    "canonical_name": "phi-3-mini-4k-instruct-june-2024",
    "model_name": "phi-3-mini-4k-instruct-june-2024",
    "aliases": [
      "phi-3-mini-4k-instruct-june-2024"
    ],
    "provider": "Microsoft",
    "context_window": null,
    "open_source": null,
    "arena_votes": 12296,
    "license_type": "MIT",
    "creator": null,
    "intelligence_score": null,
    "coding_score": null,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 1142.99,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": null,
    "tau2": null,
    "lcr": null,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": null,
    "gpqa": null,
    "scicode": null,
    "ifbench": null,
    "aime25": null,
    "critpt": null,
    "mmmu_pro": null,
    "livecodebench": null,
    "sources": [
      "LMSYS Chatbot Arena"
    ],
    "source_count": 1,
    "data_tier": 3,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.895088+00:00",
        "confidence": 1.0,
        "raw_name": "phi-3-mini-4k-instruct-june-2024",
        "raw_scores": {
          "arena_elo": 1142.99,
          "arena_votes": 12296
        }
      }
    ],
    "benchmark_breakdown": {
      "LMSYS Chatbot Arena": {
        "arena_elo": 1142.99
      }
    },
    "confidence_score": 1.0,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "llama-2-13b-chat",
    "canonical_name": "llama-2-13b-chat",
    "model_name": "llama-2-13b-chat",
    "aliases": [
      "llama-2-13b-chat"
    ],
    "provider": "Meta",
    "context_window": null,
    "open_source": null,
    "arena_votes": 19171,
    "license_type": "Llama 2 Community",
    "creator": null,
    "intelligence_score": null,
    "coding_score": null,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 1141.46,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": null,
    "tau2": null,
    "lcr": null,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": null,
    "gpqa": null,
    "scicode": null,
    "ifbench": null,
    "aime25": null,
    "critpt": null,
    "mmmu_pro": null,
    "livecodebench": null,
    "sources": [
      "LMSYS Chatbot Arena"
    ],
    "source_count": 1,
    "data_tier": 3,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.895100+00:00",
        "confidence": 1.0,
        "raw_name": "llama-2-13b-chat",
        "raw_scores": {
          "arena_elo": 1141.46,
          "arena_votes": 19171
        }
      }
    ],
    "benchmark_breakdown": {
      "LMSYS Chatbot Arena": {
        "arena_elo": 1141.46
      }
    },
    "confidence_score": 1.0,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "vicuna-13b",
    "canonical_name": "vicuna-13b",
    "model_name": "vicuna-13b",
    "aliases": [
      "vicuna-13b"
    ],
    "provider": "LMSYS",
    "context_window": null,
    "open_source": null,
    "arena_votes": 19366,
    "license_type": "Llama 2 Community",
    "creator": null,
    "intelligence_score": null,
    "coding_score": null,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 1140.86,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": null,
    "tau2": null,
    "lcr": null,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": null,
    "gpqa": null,
    "scicode": null,
    "ifbench": null,
    "aime25": null,
    "critpt": null,
    "mmmu_pro": null,
    "livecodebench": null,
    "sources": [
      "LMSYS Chatbot Arena"
    ],
    "source_count": 1,
    "data_tier": 3,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.895113+00:00",
        "confidence": 1.0,
        "raw_name": "vicuna-13b",
        "raw_scores": {
          "arena_elo": 1140.86,
          "arena_votes": 19366
        }
      }
    ],
    "benchmark_breakdown": {
      "LMSYS Chatbot Arena": {
        "arena_elo": 1140.86
      }
    },
    "confidence_score": 1.0,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "qwen-14b-chat",
    "canonical_name": "qwen-14b-chat",
    "model_name": "qwen-14b-chat",
    "aliases": [
      "qwen-14b-chat"
    ],
    "provider": "Alibaba",
    "context_window": null,
    "open_source": null,
    "arena_votes": 4964,
    "license_type": "Qianwen LICENSE",
    "creator": null,
    "intelligence_score": null,
    "coding_score": null,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 1138.51,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": null,
    "tau2": null,
    "lcr": null,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": null,
    "gpqa": null,
    "scicode": null,
    "ifbench": null,
    "aime25": null,
    "critpt": null,
    "mmmu_pro": null,
    "livecodebench": null,
    "sources": [
      "LMSYS Chatbot Arena"
    ],
    "source_count": 1,
    "data_tier": 3,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.895125+00:00",
        "confidence": 1.0,
        "raw_name": "qwen-14b-chat",
        "raw_scores": {
          "arena_elo": 1138.51,
          "arena_votes": 4964
        }
      }
    ],
    "benchmark_breakdown": {
      "LMSYS Chatbot Arena": {
        "arena_elo": 1138.51
      }
    },
    "confidence_score": 1.0,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "codellama-34b-instruct",
    "canonical_name": "codellama-34b-instruct",
    "model_name": "codellama-34b-instruct",
    "aliases": [
      "codellama-34b-instruct"
    ],
    "provider": "Meta",
    "context_window": null,
    "open_source": null,
    "arena_votes": 7363,
    "license_type": "Llama 2 Community",
    "creator": null,
    "intelligence_score": null,
    "coding_score": null,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 1136.55,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": null,
    "tau2": null,
    "lcr": null,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": null,
    "gpqa": null,
    "scicode": null,
    "ifbench": null,
    "aime25": null,
    "critpt": null,
    "mmmu_pro": null,
    "livecodebench": null,
    "sources": [
      "LMSYS Chatbot Arena"
    ],
    "source_count": 1,
    "data_tier": 3,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.895150+00:00",
        "confidence": 1.0,
        "raw_name": "codellama-34b-instruct",
        "raw_scores": {
          "arena_elo": 1136.55,
          "arena_votes": 7363
        }
      }
    ],
    "benchmark_breakdown": {
      "LMSYS Chatbot Arena": {
        "arena_elo": 1136.55
      }
    },
    "confidence_score": 1.0,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "gemma-7b-it",
    "canonical_name": "gemma-7b-it",
    "model_name": "gemma-7b-it",
    "aliases": [
      "gemma-7b-it"
    ],
    "provider": "Google",
    "context_window": null,
    "open_source": null,
    "arena_votes": 8925,
    "license_type": "Gemma license",
    "creator": null,
    "intelligence_score": null,
    "coding_score": null,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 1135.91,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": null,
    "tau2": null,
    "lcr": null,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": null,
    "gpqa": null,
    "scicode": null,
    "ifbench": null,
    "aime25": null,
    "critpt": null,
    "mmmu_pro": null,
    "livecodebench": null,
    "sources": [
      "LMSYS Chatbot Arena"
    ],
    "source_count": 1,
    "data_tier": 3,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.895162+00:00",
        "confidence": 1.0,
        "raw_name": "gemma-7b-it",
        "raw_scores": {
          "arena_elo": 1135.91,
          "arena_votes": 8925
        }
      }
    ],
    "benchmark_breakdown": {
      "LMSYS Chatbot Arena": {
        "arena_elo": 1135.91
      }
    },
    "confidence_score": 1.0,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "zephyr-7b-beta",
    "canonical_name": "zephyr-7b-beta",
    "model_name": "zephyr-7b-beta",
    "aliases": [
      "zephyr-7b-beta"
    ],
    "provider": "HuggingFace",
    "context_window": null,
    "open_source": null,
    "arena_votes": 11116,
    "license_type": "MIT",
    "creator": null,
    "intelligence_score": null,
    "coding_score": null,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 1131.04,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": null,
    "tau2": null,
    "lcr": null,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": null,
    "gpqa": null,
    "scicode": null,
    "ifbench": null,
    "aime25": null,
    "critpt": null,
    "mmmu_pro": null,
    "livecodebench": null,
    "sources": [
      "LMSYS Chatbot Arena"
    ],
    "source_count": 1,
    "data_tier": 3,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.895174+00:00",
        "confidence": 1.0,
        "raw_name": "zephyr-7b-beta",
        "raw_scores": {
          "arena_elo": 1131.04,
          "arena_votes": 11116
        }
      }
    ],
    "benchmark_breakdown": {
      "LMSYS Chatbot Arena": {
        "arena_elo": 1131.04
      }
    },
    "confidence_score": 1.0,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "phi-3-mini-128k-instruct",
    "canonical_name": "phi-3-mini-128k-instruct",
    "model_name": "phi-3-mini-128k-instruct",
    "aliases": [
      "phi-3-mini-128k-instruct"
    ],
    "provider": "Microsoft",
    "context_window": null,
    "open_source": null,
    "arena_votes": 20691,
    "license_type": "MIT",
    "creator": null,
    "intelligence_score": null,
    "coding_score": null,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 1129.15,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": null,
    "tau2": null,
    "lcr": null,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": null,
    "gpqa": null,
    "scicode": null,
    "ifbench": null,
    "aime25": null,
    "critpt": null,
    "mmmu_pro": null,
    "livecodebench": null,
    "sources": [
      "LMSYS Chatbot Arena"
    ],
    "source_count": 1,
    "data_tier": 3,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.895197+00:00",
        "confidence": 1.0,
        "raw_name": "phi-3-mini-128k-instruct",
        "raw_scores": {
          "arena_elo": 1129.15,
          "arena_votes": 20691
        }
      }
    ],
    "benchmark_breakdown": {
      "LMSYS Chatbot Arena": {
        "arena_elo": 1129.15
      }
    },
    "confidence_score": 1.0,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "phi-3-mini-4k-instruct",
    "canonical_name": "phi-3-mini-4k-instruct",
    "model_name": "phi-3-mini-4k-instruct",
    "aliases": [
      "phi-3-mini-4k-instruct"
    ],
    "provider": "Microsoft",
    "context_window": null,
    "open_source": null,
    "arena_votes": 20115,
    "license_type": "MIT",
    "creator": null,
    "intelligence_score": null,
    "coding_score": null,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 1128.34,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": null,
    "tau2": null,
    "lcr": null,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": null,
    "gpqa": null,
    "scicode": null,
    "ifbench": null,
    "aime25": null,
    "critpt": null,
    "mmmu_pro": null,
    "livecodebench": null,
    "sources": [
      "LMSYS Chatbot Arena"
    ],
    "source_count": 1,
    "data_tier": 3,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.895211+00:00",
        "confidence": 1.0,
        "raw_name": "phi-3-mini-4k-instruct",
        "raw_scores": {
          "arena_elo": 1128.34,
          "arena_votes": 20115
        }
      }
    ],
    "benchmark_breakdown": {
      "LMSYS Chatbot Arena": {
        "arena_elo": 1128.34
      }
    },
    "confidence_score": 1.0,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "guanaco-33b",
    "canonical_name": "guanaco-33b",
    "model_name": "guanaco-33b",
    "aliases": [
      "guanaco-33b"
    ],
    "provider": "UW",
    "context_window": null,
    "open_source": null,
    "arena_votes": 2921,
    "license_type": "Non-commercial",
    "creator": null,
    "intelligence_score": null,
    "coding_score": null,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 1127.26,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": null,
    "tau2": null,
    "lcr": null,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": null,
    "gpqa": null,
    "scicode": null,
    "ifbench": null,
    "aime25": null,
    "critpt": null,
    "mmmu_pro": null,
    "livecodebench": null,
    "sources": [
      "LMSYS Chatbot Arena"
    ],
    "source_count": 1,
    "data_tier": 3,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.895223+00:00",
        "confidence": 1.0,
        "raw_name": "guanaco-33b",
        "raw_scores": {
          "arena_elo": 1127.26,
          "arena_votes": 2921
        }
      }
    ],
    "benchmark_breakdown": {
      "LMSYS Chatbot Arena": {
        "arena_elo": 1127.26
      }
    },
    "confidence_score": 1.0,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "zephyr-7b-alpha",
    "canonical_name": "zephyr-7b-alpha",
    "model_name": "zephyr-7b-alpha",
    "aliases": [
      "zephyr-7b-alpha"
    ],
    "provider": "HuggingFace",
    "context_window": null,
    "open_source": null,
    "arena_votes": 1785,
    "license_type": "MIT",
    "creator": null,
    "intelligence_score": null,
    "coding_score": null,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 1126.87,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": null,
    "tau2": null,
    "lcr": null,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": null,
    "gpqa": null,
    "scicode": null,
    "ifbench": null,
    "aime25": null,
    "critpt": null,
    "mmmu_pro": null,
    "livecodebench": null,
    "sources": [
      "LMSYS Chatbot Arena"
    ],
    "source_count": 1,
    "data_tier": 3,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.895236+00:00",
        "confidence": 1.0,
        "raw_name": "zephyr-7b-alpha",
        "raw_scores": {
          "arena_elo": 1126.87,
          "arena_votes": 1785
        }
      }
    ],
    "benchmark_breakdown": {
      "LMSYS Chatbot Arena": {
        "arena_elo": 1126.87
      }
    },
    "confidence_score": 1.0,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "stripedhyena-nous-7b",
    "canonical_name": "stripedhyena-nous-7b",
    "model_name": "stripedhyena-nous-7b",
    "aliases": [
      "stripedhyena-nous-7b"
    ],
    "provider": "Together AI",
    "context_window": null,
    "open_source": null,
    "arena_votes": 5184,
    "license_type": "Apache 2.0",
    "creator": null,
    "intelligence_score": null,
    "coding_score": null,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 1120.9,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": null,
    "tau2": null,
    "lcr": null,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": null,
    "gpqa": null,
    "scicode": null,
    "ifbench": null,
    "aime25": null,
    "critpt": null,
    "mmmu_pro": null,
    "livecodebench": null,
    "sources": [
      "LMSYS Chatbot Arena"
    ],
    "source_count": 1,
    "data_tier": 3,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.895248+00:00",
        "confidence": 1.0,
        "raw_name": "stripedhyena-nous-7b",
        "raw_scores": {
          "arena_elo": 1120.9,
          "arena_votes": 5184
        }
      }
    ],
    "benchmark_breakdown": {
      "LMSYS Chatbot Arena": {
        "arena_elo": 1120.9
      }
    },
    "confidence_score": 1.0,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "codellama-70b-instruct",
    "canonical_name": "codellama-70b-instruct",
    "model_name": "codellama-70b-instruct",
    "aliases": [
      "codellama-70b-instruct"
    ],
    "provider": "Meta",
    "context_window": null,
    "open_source": null,
    "arena_votes": 1143,
    "license_type": "Llama 2 Community",
    "creator": null,
    "intelligence_score": null,
    "coding_score": null,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 1118.94,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": null,
    "tau2": null,
    "lcr": null,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": null,
    "gpqa": null,
    "scicode": null,
    "ifbench": null,
    "aime25": null,
    "critpt": null,
    "mmmu_pro": null,
    "livecodebench": null,
    "sources": [
      "LMSYS Chatbot Arena"
    ],
    "source_count": 1,
    "data_tier": 3,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.895260+00:00",
        "confidence": 1.0,
        "raw_name": "codellama-70b-instruct",
        "raw_scores": {
          "arena_elo": 1118.94,
          "arena_votes": 1143
        }
      }
    ],
    "benchmark_breakdown": {
      "LMSYS Chatbot Arena": {
        "arena_elo": 1118.94
      }
    },
    "confidence_score": 1.0,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "vicuna-7b",
    "canonical_name": "vicuna-7b",
    "model_name": "vicuna-7b",
    "aliases": [
      "vicuna-7b"
    ],
    "provider": "LMSYS",
    "context_window": null,
    "open_source": null,
    "arena_votes": 6923,
    "license_type": "Llama 2 Community",
    "creator": null,
    "intelligence_score": null,
    "coding_score": null,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 1114.57,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": null,
    "tau2": null,
    "lcr": null,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": null,
    "gpqa": null,
    "scicode": null,
    "ifbench": null,
    "aime25": null,
    "critpt": null,
    "mmmu_pro": null,
    "livecodebench": null,
    "sources": [
      "LMSYS Chatbot Arena"
    ],
    "source_count": 1,
    "data_tier": 3,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.895273+00:00",
        "confidence": 1.0,
        "raw_name": "vicuna-7b",
        "raw_scores": {
          "arena_elo": 1114.57,
          "arena_votes": 6923
        }
      }
    ],
    "benchmark_breakdown": {
      "LMSYS Chatbot Arena": {
        "arena_elo": 1114.57
      }
    },
    "confidence_score": 1.0,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "smollm2-17b-instruct",
    "canonical_name": "smollm2-1.7b-instruct",
    "model_name": "smollm2-1.7b-instruct",
    "aliases": [
      "smollm2-1.7b-instruct"
    ],
    "provider": "HuggingFace",
    "context_window": null,
    "open_source": null,
    "arena_votes": 2201,
    "license_type": "Apache 2.0",
    "creator": null,
    "intelligence_score": null,
    "coding_score": null,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 1114.26,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": null,
    "tau2": null,
    "lcr": null,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": null,
    "gpqa": null,
    "scicode": null,
    "ifbench": null,
    "aime25": null,
    "critpt": null,
    "mmmu_pro": null,
    "livecodebench": null,
    "sources": [
      "LMSYS Chatbot Arena"
    ],
    "source_count": 1,
    "data_tier": 3,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.895285+00:00",
        "confidence": 1.0,
        "raw_name": "smollm2-1.7b-instruct",
        "raw_scores": {
          "arena_elo": 1114.26,
          "arena_votes": 2201
        }
      }
    ],
    "benchmark_breakdown": {
      "LMSYS Chatbot Arena": {
        "arena_elo": 1114.26
      }
    },
    "confidence_score": 1.0,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "gemma-11-2b-it",
    "canonical_name": "gemma-1.1-2b-it",
    "model_name": "gemma-1.1-2b-it",
    "aliases": [
      "gemma-1.1-2b-it"
    ],
    "provider": "Google",
    "context_window": null,
    "open_source": null,
    "arena_votes": 10853,
    "license_type": "Gemma license",
    "creator": null,
    "intelligence_score": null,
    "coding_score": null,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 1114.08,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": null,
    "tau2": null,
    "lcr": null,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": null,
    "gpqa": null,
    "scicode": null,
    "ifbench": null,
    "aime25": null,
    "critpt": null,
    "mmmu_pro": null,
    "livecodebench": null,
    "sources": [
      "LMSYS Chatbot Arena"
    ],
    "source_count": 1,
    "data_tier": 3,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.895297+00:00",
        "confidence": 1.0,
        "raw_name": "gemma-1.1-2b-it",
        "raw_scores": {
          "arena_elo": 1114.08,
          "arena_votes": 10853
        }
      }
    ],
    "benchmark_breakdown": {
      "LMSYS Chatbot Arena": {
        "arena_elo": 1114.08
      }
    },
    "confidence_score": 1.0,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "llama-32-1b-instruct",
    "canonical_name": "llama-3.2-1b-instruct",
    "model_name": "llama-3.2-1b-instruct",
    "aliases": [
      "llama-3.2-1b-instruct"
    ],
    "provider": "Meta",
    "context_window": null,
    "open_source": null,
    "arena_votes": 8045,
    "license_type": "Llama 3.2",
    "creator": null,
    "intelligence_score": null,
    "coding_score": null,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 1111.19,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": null,
    "tau2": null,
    "lcr": null,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": null,
    "gpqa": null,
    "scicode": null,
    "ifbench": null,
    "aime25": null,
    "critpt": null,
    "mmmu_pro": null,
    "livecodebench": null,
    "sources": [
      "LMSYS Chatbot Arena"
    ],
    "source_count": 1,
    "data_tier": 3,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.895309+00:00",
        "confidence": 1.0,
        "raw_name": "llama-3.2-1b-instruct",
        "raw_scores": {
          "arena_elo": 1111.19,
          "arena_votes": 8045
        }
      }
    ],
    "benchmark_breakdown": {
      "LMSYS Chatbot Arena": {
        "arena_elo": 1111.19
      }
    },
    "confidence_score": 1.0,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "llama-2-7b-chat",
    "canonical_name": "llama-2-7b-chat",
    "model_name": "llama-2-7b-chat",
    "aliases": [
      "llama-2-7b-chat"
    ],
    "provider": "Meta",
    "context_window": null,
    "open_source": null,
    "arena_votes": 14148,
    "license_type": "Llama 2 Community",
    "creator": null,
    "intelligence_score": null,
    "coding_score": null,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 1108.14,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": null,
    "tau2": null,
    "lcr": null,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": null,
    "gpqa": null,
    "scicode": null,
    "ifbench": null,
    "aime25": null,
    "critpt": null,
    "mmmu_pro": null,
    "livecodebench": null,
    "sources": [
      "LMSYS Chatbot Arena"
    ],
    "source_count": 1,
    "data_tier": 3,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.895333+00:00",
        "confidence": 1.0,
        "raw_name": "llama-2-7b-chat",
        "raw_scores": {
          "arena_elo": 1108.14,
          "arena_votes": 14148
        }
      }
    ],
    "benchmark_breakdown": {
      "LMSYS Chatbot Arena": {
        "arena_elo": 1108.14
      }
    },
    "confidence_score": 1.0,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "gemma-2b-it",
    "canonical_name": "gemma-2b-it",
    "model_name": "gemma-2b-it",
    "aliases": [
      "gemma-2b-it"
    ],
    "provider": "Google",
    "context_window": null,
    "open_source": null,
    "arena_votes": 4779,
    "license_type": "Gemma license",
    "creator": null,
    "intelligence_score": null,
    "coding_score": null,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 1091.54,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": null,
    "tau2": null,
    "lcr": null,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": null,
    "gpqa": null,
    "scicode": null,
    "ifbench": null,
    "aime25": null,
    "critpt": null,
    "mmmu_pro": null,
    "livecodebench": null,
    "sources": [
      "LMSYS Chatbot Arena"
    ],
    "source_count": 1,
    "data_tier": 3,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.895346+00:00",
        "confidence": 1.0,
        "raw_name": "gemma-2b-it",
        "raw_scores": {
          "arena_elo": 1091.54,
          "arena_votes": 4779
        }
      }
    ],
    "benchmark_breakdown": {
      "LMSYS Chatbot Arena": {
        "arena_elo": 1091.54
      }
    },
    "confidence_score": 1.0,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "qwen15-4b-chat",
    "canonical_name": "qwen1.5-4b-chat",
    "model_name": "qwen1.5-4b-chat",
    "aliases": [
      "qwen1.5-4b-chat"
    ],
    "provider": "Alibaba",
    "context_window": null,
    "open_source": null,
    "arena_votes": 7598,
    "license_type": "Qianwen LICENSE",
    "creator": null,
    "intelligence_score": null,
    "coding_score": null,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 1090.06,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": null,
    "tau2": null,
    "lcr": null,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": null,
    "gpqa": null,
    "scicode": null,
    "ifbench": null,
    "aime25": null,
    "critpt": null,
    "mmmu_pro": null,
    "livecodebench": null,
    "sources": [
      "LMSYS Chatbot Arena"
    ],
    "source_count": 1,
    "data_tier": 3,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.895358+00:00",
        "confidence": 1.0,
        "raw_name": "qwen1.5-4b-chat",
        "raw_scores": {
          "arena_elo": 1090.06,
          "arena_votes": 7598
        }
      }
    ],
    "benchmark_breakdown": {
      "LMSYS Chatbot Arena": {
        "arena_elo": 1090.06
      }
    },
    "confidence_score": 1.0,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "olmo-7b-instruct",
    "canonical_name": "olmo-7b-instruct",
    "model_name": "olmo-7b-instruct",
    "aliases": [
      "olmo-7b-instruct"
    ],
    "provider": "Ai2",
    "context_window": null,
    "open_source": null,
    "arena_votes": 6329,
    "license_type": "Apache-2.0",
    "creator": null,
    "intelligence_score": null,
    "coding_score": null,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 1074.57,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": null,
    "tau2": null,
    "lcr": null,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": null,
    "gpqa": null,
    "scicode": null,
    "ifbench": null,
    "aime25": null,
    "critpt": null,
    "mmmu_pro": null,
    "livecodebench": null,
    "sources": [
      "LMSYS Chatbot Arena"
    ],
    "source_count": 1,
    "data_tier": 3,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.895370+00:00",
        "confidence": 1.0,
        "raw_name": "olmo-7b-instruct",
        "raw_scores": {
          "arena_elo": 1074.57,
          "arena_votes": 6329
        }
      }
    ],
    "benchmark_breakdown": {
      "LMSYS Chatbot Arena": {
        "arena_elo": 1074.57
      }
    },
    "confidence_score": 1.0,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "koala-13b",
    "canonical_name": "koala-13b",
    "model_name": "koala-13b",
    "aliases": [
      "koala-13b"
    ],
    "provider": "UC Berkeley",
    "context_window": null,
    "open_source": null,
    "arena_votes": 6964,
    "license_type": "Non-commercial",
    "creator": null,
    "intelligence_score": null,
    "coding_score": null,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 1070.35,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": null,
    "tau2": null,
    "lcr": null,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": null,
    "gpqa": null,
    "scicode": null,
    "ifbench": null,
    "aime25": null,
    "critpt": null,
    "mmmu_pro": null,
    "livecodebench": null,
    "sources": [
      "LMSYS Chatbot Arena"
    ],
    "source_count": 1,
    "data_tier": 3,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.895383+00:00",
        "confidence": 1.0,
        "raw_name": "koala-13b",
        "raw_scores": {
          "arena_elo": 1070.35,
          "arena_votes": 6964
        }
      }
    ],
    "benchmark_breakdown": {
      "LMSYS Chatbot Arena": {
        "arena_elo": 1070.35
      }
    },
    "confidence_score": 1.0,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "alpaca-13b",
    "canonical_name": "alpaca-13b",
    "model_name": "alpaca-13b",
    "aliases": [
      "alpaca-13b"
    ],
    "provider": "Stanford",
    "context_window": null,
    "open_source": null,
    "arena_votes": 5745,
    "license_type": "Non-commercial",
    "creator": null,
    "intelligence_score": null,
    "coding_score": null,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 1067.28,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": null,
    "tau2": null,
    "lcr": null,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": null,
    "gpqa": null,
    "scicode": null,
    "ifbench": null,
    "aime25": null,
    "critpt": null,
    "mmmu_pro": null,
    "livecodebench": null,
    "sources": [
      "LMSYS Chatbot Arena"
    ],
    "source_count": 1,
    "data_tier": 3,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.895395+00:00",
        "confidence": 1.0,
        "raw_name": "alpaca-13b",
        "raw_scores": {
          "arena_elo": 1067.28,
          "arena_votes": 5745
        }
      }
    ],
    "benchmark_breakdown": {
      "LMSYS Chatbot Arena": {
        "arena_elo": 1067.28
      }
    },
    "confidence_score": 1.0,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "gpt4all-13b-snoozy",
    "canonical_name": "gpt4all-13b-snoozy",
    "model_name": "gpt4all-13b-snoozy",
    "aliases": [
      "gpt4all-13b-snoozy"
    ],
    "provider": "Nomic AI",
    "context_window": null,
    "open_source": null,
    "arena_votes": 1743,
    "license_type": "Non-commercial",
    "creator": null,
    "intelligence_score": null,
    "coding_score": null,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 1065.85,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": null,
    "tau2": null,
    "lcr": null,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": null,
    "gpqa": null,
    "scicode": null,
    "ifbench": null,
    "aime25": null,
    "critpt": null,
    "mmmu_pro": null,
    "livecodebench": null,
    "sources": [
      "LMSYS Chatbot Arena"
    ],
    "source_count": 1,
    "data_tier": 3,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.895407+00:00",
        "confidence": 1.0,
        "raw_name": "gpt4all-13b-snoozy",
        "raw_scores": {
          "arena_elo": 1065.85,
          "arena_votes": 1743
        }
      }
    ],
    "benchmark_breakdown": {
      "LMSYS Chatbot Arena": {
        "arena_elo": 1065.85
      }
    },
    "confidence_score": 1.0,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "mpt-7b-chat",
    "canonical_name": "mpt-7b-chat",
    "model_name": "mpt-7b-chat",
    "aliases": [
      "mpt-7b-chat"
    ],
    "provider": "MosaicML",
    "context_window": null,
    "open_source": null,
    "arena_votes": 3925,
    "license_type": "CC-BY-NC-SA-4.0",
    "creator": null,
    "intelligence_score": null,
    "coding_score": null,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 1061.73,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": null,
    "tau2": null,
    "lcr": null,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": null,
    "gpqa": null,
    "scicode": null,
    "ifbench": null,
    "aime25": null,
    "critpt": null,
    "mmmu_pro": null,
    "livecodebench": null,
    "sources": [
      "LMSYS Chatbot Arena"
    ],
    "source_count": 1,
    "data_tier": 3,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.895424+00:00",
        "confidence": 1.0,
        "raw_name": "mpt-7b-chat",
        "raw_scores": {
          "arena_elo": 1061.73,
          "arena_votes": 3925
        }
      }
    ],
    "benchmark_breakdown": {
      "LMSYS Chatbot Arena": {
        "arena_elo": 1061.73
      }
    },
    "confidence_score": 1.0,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "chatglm3-6b",
    "canonical_name": "chatglm3-6b",
    "model_name": "chatglm3-6b",
    "aliases": [
      "chatglm3-6b"
    ],
    "provider": "Tsinghua",
    "context_window": null,
    "open_source": null,
    "arena_votes": 4658,
    "license_type": "Apache-2.0",
    "creator": null,
    "intelligence_score": null,
    "coding_score": null,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 1055.97,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": null,
    "tau2": null,
    "lcr": null,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": null,
    "gpqa": null,
    "scicode": null,
    "ifbench": null,
    "aime25": null,
    "critpt": null,
    "mmmu_pro": null,
    "livecodebench": null,
    "sources": [
      "LMSYS Chatbot Arena"
    ],
    "source_count": 1,
    "data_tier": 3,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.895436+00:00",
        "confidence": 1.0,
        "raw_name": "chatglm3-6b",
        "raw_scores": {
          "arena_elo": 1055.97,
          "arena_votes": 4658
        }
      }
    ],
    "benchmark_breakdown": {
      "LMSYS Chatbot Arena": {
        "arena_elo": 1055.97
      }
    },
    "confidence_score": 1.0,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "rwkv-4-raven-14b",
    "canonical_name": "RWKV-4-Raven-14B",
    "model_name": "RWKV-4-Raven-14B",
    "aliases": [
      "RWKV-4-Raven-14B"
    ],
    "provider": "RWKV",
    "context_window": null,
    "open_source": null,
    "arena_votes": 4845,
    "license_type": "Apache 2.0",
    "creator": null,
    "intelligence_score": null,
    "coding_score": null,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 1041.2,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": null,
    "tau2": null,
    "lcr": null,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": null,
    "gpqa": null,
    "scicode": null,
    "ifbench": null,
    "aime25": null,
    "critpt": null,
    "mmmu_pro": null,
    "livecodebench": null,
    "sources": [
      "LMSYS Chatbot Arena"
    ],
    "source_count": 1,
    "data_tier": 3,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.895448+00:00",
        "confidence": 1.0,
        "raw_name": "RWKV-4-Raven-14B",
        "raw_scores": {
          "arena_elo": 1041.2,
          "arena_votes": 4845
        }
      }
    ],
    "benchmark_breakdown": {
      "LMSYS Chatbot Arena": {
        "arena_elo": 1041.2
      }
    },
    "confidence_score": 1.0,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "chatglm2-6b",
    "canonical_name": "chatglm2-6b",
    "model_name": "chatglm2-6b",
    "aliases": [
      "chatglm2-6b"
    ],
    "provider": "Tsinghua",
    "context_window": null,
    "open_source": null,
    "arena_votes": 2657,
    "license_type": "Apache-2.0",
    "creator": null,
    "intelligence_score": null,
    "coding_score": null,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 1024.11,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": null,
    "tau2": null,
    "lcr": null,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": null,
    "gpqa": null,
    "scicode": null,
    "ifbench": null,
    "aime25": null,
    "critpt": null,
    "mmmu_pro": null,
    "livecodebench": null,
    "sources": [
      "LMSYS Chatbot Arena"
    ],
    "source_count": 1,
    "data_tier": 3,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.895461+00:00",
        "confidence": 1.0,
        "raw_name": "chatglm2-6b",
        "raw_scores": {
          "arena_elo": 1024.11,
          "arena_votes": 2657
        }
      }
    ],
    "benchmark_breakdown": {
      "LMSYS Chatbot Arena": {
        "arena_elo": 1024.11
      }
    },
    "confidence_score": 1.0,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "oasst-pythia-12b",
    "canonical_name": "oasst-pythia-12b",
    "model_name": "oasst-pythia-12b",
    "aliases": [
      "oasst-pythia-12b"
    ],
    "provider": "OpenAssistant",
    "context_window": null,
    "open_source": null,
    "arena_votes": 6311,
    "license_type": "Apache 2.0",
    "creator": null,
    "intelligence_score": null,
    "coding_score": null,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 1022.0,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": null,
    "tau2": null,
    "lcr": null,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": null,
    "gpqa": null,
    "scicode": null,
    "ifbench": null,
    "aime25": null,
    "critpt": null,
    "mmmu_pro": null,
    "livecodebench": null,
    "sources": [
      "LMSYS Chatbot Arena"
    ],
    "source_count": 1,
    "data_tier": 3,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.895473+00:00",
        "confidence": 1.0,
        "raw_name": "oasst-pythia-12b",
        "raw_scores": {
          "arena_elo": 1022.0,
          "arena_votes": 6311
        }
      }
    ],
    "benchmark_breakdown": {
      "LMSYS Chatbot Arena": {
        "arena_elo": 1022.0
      }
    },
    "confidence_score": 1.0,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "chatglm-6b",
    "canonical_name": "chatglm-6b",
    "model_name": "chatglm-6b",
    "aliases": [
      "chatglm-6b"
    ],
    "provider": "Tsinghua",
    "context_window": null,
    "open_source": null,
    "arena_votes": 4914,
    "license_type": "Non-commercial",
    "creator": null,
    "intelligence_score": null,
    "coding_score": null,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 995.492,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": null,
    "tau2": null,
    "lcr": null,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": null,
    "gpqa": null,
    "scicode": null,
    "ifbench": null,
    "aime25": null,
    "critpt": null,
    "mmmu_pro": null,
    "livecodebench": null,
    "sources": [
      "LMSYS Chatbot Arena"
    ],
    "source_count": 1,
    "data_tier": 3,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.895486+00:00",
        "confidence": 1.0,
        "raw_name": "chatglm-6b",
        "raw_scores": {
          "arena_elo": 995.492,
          "arena_votes": 4914
        }
      }
    ],
    "benchmark_breakdown": {
      "LMSYS Chatbot Arena": {
        "arena_elo": 995.492
      }
    },
    "confidence_score": 1.0,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "fastchat-t5-3b",
    "canonical_name": "fastchat-t5-3b",
    "model_name": "fastchat-t5-3b",
    "aliases": [
      "fastchat-t5-3b"
    ],
    "provider": "LMSYS",
    "context_window": null,
    "open_source": null,
    "arena_votes": 4203,
    "license_type": "Apache 2.0",
    "creator": null,
    "intelligence_score": null,
    "coding_score": null,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 991.198,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": null,
    "tau2": null,
    "lcr": null,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": null,
    "gpqa": null,
    "scicode": null,
    "ifbench": null,
    "aime25": null,
    "critpt": null,
    "mmmu_pro": null,
    "livecodebench": null,
    "sources": [
      "LMSYS Chatbot Arena"
    ],
    "source_count": 1,
    "data_tier": 3,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.895498+00:00",
        "confidence": 1.0,
        "raw_name": "fastchat-t5-3b",
        "raw_scores": {
          "arena_elo": 991.198,
          "arena_votes": 4203
        }
      }
    ],
    "benchmark_breakdown": {
      "LMSYS Chatbot Arena": {
        "arena_elo": 991.198
      }
    },
    "confidence_score": 1.0,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "dolly-v2-12b",
    "canonical_name": "dolly-v2-12b",
    "model_name": "dolly-v2-12b",
    "aliases": [
      "dolly-v2-12b"
    ],
    "provider": "Databricks",
    "context_window": null,
    "open_source": null,
    "arena_votes": 3412,
    "license_type": "MIT",
    "creator": null,
    "intelligence_score": null,
    "coding_score": null,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 979.94,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": null,
    "tau2": null,
    "lcr": null,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": null,
    "gpqa": null,
    "scicode": null,
    "ifbench": null,
    "aime25": null,
    "critpt": null,
    "mmmu_pro": null,
    "livecodebench": null,
    "sources": [
      "LMSYS Chatbot Arena"
    ],
    "source_count": 1,
    "data_tier": 3,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.895510+00:00",
        "confidence": 1.0,
        "raw_name": "dolly-v2-12b",
        "raw_scores": {
          "arena_elo": 979.94,
          "arena_votes": 3412
        }
      }
    ],
    "benchmark_breakdown": {
      "LMSYS Chatbot Arena": {
        "arena_elo": 979.94
      }
    },
    "confidence_score": 1.0,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "llama-13b",
    "canonical_name": "llama-13b",
    "model_name": "llama-13b",
    "aliases": [
      "llama-13b"
    ],
    "provider": "Meta",
    "context_window": null,
    "open_source": null,
    "arena_votes": 2391,
    "license_type": "Non-commercial",
    "creator": null,
    "intelligence_score": null,
    "coding_score": null,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 972.002,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": null,
    "tau2": null,
    "lcr": null,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": null,
    "gpqa": null,
    "scicode": null,
    "ifbench": null,
    "aime25": null,
    "critpt": null,
    "mmmu_pro": null,
    "livecodebench": null,
    "sources": [
      "LMSYS Chatbot Arena"
    ],
    "source_count": 1,
    "data_tier": 3,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.895523+00:00",
        "confidence": 1.0,
        "raw_name": "llama-13b",
        "raw_scores": {
          "arena_elo": 972.002,
          "arena_votes": 2391
        }
      }
    ],
    "benchmark_breakdown": {
      "LMSYS Chatbot Arena": {
        "arena_elo": 972.002
      }
    },
    "confidence_score": 1.0,
    "snapshot_date": "2026-02-27"
  },
  {
    "model_id": null,
    "model_slug": "stablelm-tuned-alpha-7b",
    "canonical_name": "stablelm-tuned-alpha-7b",
    "model_name": "stablelm-tuned-alpha-7b",
    "aliases": [
      "stablelm-tuned-alpha-7b"
    ],
    "provider": "Stability AI",
    "context_window": null,
    "open_source": null,
    "arena_votes": 3287,
    "license_type": "CC-BY-NC-SA-4.0",
    "creator": null,
    "intelligence_score": null,
    "coding_score": null,
    "reasoning_score": null,
    "multimodal_score": null,
    "arena_elo": 952.506,
    "input_cost_per_1m": null,
    "output_cost_per_1m": null,
    "blended_cost_per_1m": null,
    "latency_seconds": null,
    "latency_first_token": null,
    "latency_p5": null,
    "latency_p25": null,
    "latency_p75": null,
    "latency_p95": null,
    "total_response_time": null,
    "reasoning_time": null,
    "tokens_per_second": null,
    "speed_p5": null,
    "speed_p25": null,
    "speed_p75": null,
    "speed_p95": null,
    "gdpval": null,
    "terminalbench_hard": null,
    "tau2": null,
    "lcr": null,
    "omniscience": null,
    "omniscience_hallucination": null,
    "hle": null,
    "gpqa": null,
    "scicode": null,
    "ifbench": null,
    "aime25": null,
    "critpt": null,
    "mmmu_pro": null,
    "livecodebench": null,
    "sources": [
      "LMSYS Chatbot Arena"
    ],
    "source_count": 1,
    "data_tier": 3,
    "aggregation_method": "weighted_mean",
    "provenance": [
      {
        "source": "LMSYS Chatbot Arena",
        "scraped_at": "2026-02-27T11:07:39.895535+00:00",
        "confidence": 1.0,
        "raw_name": "stablelm-tuned-alpha-7b",
        "raw_scores": {
          "arena_elo": 952.506,
          "arena_votes": 3287
        }
      }
    ],
    "benchmark_breakdown": {
      "LMSYS Chatbot Arena": {
        "arena_elo": 952.506
      }
    },
    "confidence_score": 1.0,
    "snapshot_date": "2026-02-27"
  }
]