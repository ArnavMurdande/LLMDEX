{"timestamp": 1771962983.584918, "mentions": [{"source": "Reddit", "text": "Rasbery Pi 5 16 GB 9k context running byteshape devstral and goose ai agent coder framework. by extending timeout. roo code kilo code on rasbery pi next? # ByteShape Devstral Time Out Increased scripts for Raspberry Pi 5 16GB running Goose Ai Agent Coder Framework\n\nI got goose to run on rasbary pi 5 16gb with devstral a vision model at 12k context 98 minute response time. 53 minutes 9k context I think.\n\nWhat SYSTEM prompt would you use to stylise your assistant agent coder?\n\nWhat would you ask y", "score": 0, "created": 1771899343.0, "url": "https://reddit.com/r/LocalLLaMA/comments/1rd223u/rasbery_pi_5_16_gb_9k_context_running_byteshape/"}, {"source": "Reddit", "text": "ByteShape Devstral Time Out Increased scripts for Raspberry Pi 5 16GB running Goose Ai Agent Coder Framework I got goose to run on rasbary pi 5 16gb with devstral a vision model at 12k context 98 minute response time. 53 minutes 9k context I think. \n\nWhat SYSTEM prompt would you use to stylise your assistant agent coder? \n\nWhat would you ask your agent to code?\n\nGood for hikes a set and forget gadget. Also accessible. \n\n# server:\n\n[](https://github.com/josheeg/Game-Note/blob/main/README.md#serve", "score": 2, "created": 1771899174.0, "url": "https://reddit.com/r/ByteShape/comments/1rd1zs1/byteshape_devstral_time_out_increased_scripts_for/"}, {"source": "Reddit", "text": "Model Aliases (23.02.2026) Findings reflect the state as of 2026-02-23\n\n\n## Model Aliases (base model -&gt; aliases)\n\n| Base Model | Aliases |\n| :--- | :--- |\n| mistral-small-2506 | mistral-small-latest |\n| mistral-small-2501 | (deprecated 2026-02-28, replacement: mistral-small-latest) |\n| mistral-large-2512 | mistral-large-latest |\n| mistral-large-2411 | **no aliases, isolated model** |\n| mistral-medium-2508 | mistral-medium-latest, mistral-medium, mistral-vibe-cli-with-tools |\n| mistral-medium", "score": 8, "created": 1771835105.0, "url": "https://reddit.com/r/MistralAI/comments/1rcbxp5/model_aliases_23022026/"}, {"source": "Reddit", "text": "Mistral API quota and rate limits pools analysis for Free Tier plan (20.02.2026) The goal of research is to map which models share quota pools and rate limits on the Mistral Free Tier, and document the actual limits returned via response headers.\n\nFindings reflect the state as of 2026-02-23\n\nModels not probed (quota and rate limits status unknown): \n- `codestral-embed`\n- `mistral-moderation-2411`\n- `mistral-ocr-*`\n- `labs-devstral-small-2512`\n- `labs-mistral-small-creative`\n- `voxtral-*`\n\n**Impo", "score": 36, "created": 1771824053.0, "url": "https://reddit.com/r/MistralAI/comments/1rc8rwf/mistral_api_quota_and_rate_limits_pools_analysis/"}, {"source": "Reddit", "text": "Which local-sized models would you like to see in the next Brokk Power Ranking? So far I've got devstral 2 123B, nemotron 3, and qwen 3 coder next of the recent releases. Anything else you think might beat these?", "score": 0, "created": 1771794386.0, "url": "https://reddit.com/r/LocalLLaMA/comments/1rbxnrm/which_localsized_models_would_you_like_to_see_in/"}, {"source": "Reddit", "text": "Interesting Observation from a Simple Multi-Agent Experiment with 10 Different Models This is an update to [my earlier post this week.](https://www.reddit.com/r/LocalLLaMA/comments/1r7d9xb/can_your_local_setup_complete_this_simple_multi/)\n\nTLDR: I ran a small personal experiment to autonomously summarize 10 transcripts using a multi-agent workflow on Codex.\n\nThe following sub-100B models failed to complete this simple task reliably:\n\n* qwen3-coder-next\n* glm-4.7-flash\n* Devstral-Small-2\n* gpt-os", "score": 4, "created": 1771659543.0, "url": "https://reddit.com/r/LocalLLaMA/comments/1ral48v/interesting_observation_from_a_simple_multiagent/"}, {"source": "Reddit", "text": "Mistral sucks Mistral kinda sucks tbh. They're getting crushed by Chinese and US models right now.\n\nWhen it comes to coding, I always pick Kimi K2.5 or GLM 5 over Devstral since they're just better and more cost efficient. Devstral is probably the weirdest coding model I've ever used. I usually create comprehensive plans for what I want to change in the codebase, but Devstral requires so much handholding it becomes useless. It needs way more detailed instructions than any other model, to the poi", "score": 0, "created": 1771653851.0, "url": "https://reddit.com/r/MistralAI/comments/1rajgq1/mistral_sucks/"}, {"source": "Reddit", "text": "Qwen3 coder next oddly usable at aggressive quantization Hi guys,\n\nI've been  testing the 30b range models but i've been a little disappointed by them (qwen 30b, devstral 2, nemotron etc) as they need a lot of guidance and almost all of them can't correct some mistake they made no matter what.\n\nThen i tried to use qwen next coder at q2 because i don't have enough ram for q4. Oddly enough it does not say nonsense, even better, he one shot some html front page and can correct some mistake by himse", "score": 84, "created": 1771630861.0, "url": "https://reddit.com/r/LocalLLaMA/comments/1rabg6o/qwen3_coder_next_oddly_usable_at_aggressive/"}, {"source": "Reddit", "text": "Devstral Small 2 24B + Qwen3 Coder 30B Quants for All (And for every hardware, even the Pi) Hey r/LocalLLM, we\u2019re ByteShape.\n\nWe create **device-optimized GGUF quants,** and we also **measure them properly** so you can see the TPS vs quality tradeoff and pick what makes sense for your setup.\n\nOur core technology, ShapeLearn, instead of hand-picking quant formats for the models, leverages the fine-tuning process to **learn the best datatype per tensor** and lands on better **TPS-quality trade-off", "score": 143, "created": 1771599414.0, "url": "https://reddit.com/r/LocalLLM/comments/1r9xifw/devstral_small_2_24b_qwen3_coder_30b_quants_for/"}, {"source": "Reddit", "text": "Qwen3 Coder Next 8FP in the process of converting the entire Flutter documentation for 12 hours now with just 3 sentence prompt with 64K max tokens at around 102GB memory (out of 128GB)... A remarkable LLM -- we really have a winner.\n\n(Most of the models below were NVFP4)\n\nGPT OSS 120B can't do this (though it's a bit outdated now)  \nGLM 4.7 Flash can't do this  \nSERA 32B tokens too slow  \nDevstral 2 Small can't do this  \nSEED OSS freezes while thinking  \nNemotron 3 Nano can't do this  \n\n(Unsure", "score": 123, "created": 1771548886.0, "url": "https://reddit.com/r/LocalLLaMA/comments/1r9h3g8/qwen3_coder_next_8fp_in_the_process_of_converting/"}, {"source": "Reddit", "text": "Mind-Blown by 1-Bit Quantized Qwen3-Coder-Next-UD-TQ1_0 on Just 24GB VRAM - Why Isn't This Getting More Hype? # Mind-Blown by 1-Bit Quantized Qwen3-Coder-Next-UD-TQ1_0 on Just 24GB VRAM \u2013 Why Isn't This Getting More Hype?\n\nI've been tinkering with local LLMs for coding tasks, and like many of you, I'm always hunting for models that perform well without melting my GPU. With only 24GB VRAM to work with, I've cycled through the usual suspects in the Q4-Q8 range, but nothing quite hit the mark. They", "score": 7, "created": 1771535798.0, "url": "https://reddit.com/r/LocalLLaMA/comments/1r9borh/mindblown_by_1bit_quantized_qwen3codernextudtq1_0/"}, {"source": "Reddit", "text": "Local Openclaw = template problem? I've been experimenting with openclaw with local models so far. \n\nMost stable and reliable so far were qwen and ministral/devstral.\n\nBoth suffered of lost connection and freezing after complex or sequential tasks.  Eventually, minimized that using unsloth/ministral with qwens prompt template. \n\nI had the most excellent result with a wacky template made by another openclaw 120b doing a back and forth with me trying to improve it. Could find medications reliably ", "score": 2, "created": 1771529720.0, "url": "https://reddit.com/r/openclaw/comments/1r98z1g/local_openclaw_template_problem/"}, {"source": "Reddit", "text": "Devstral Small 2 24B + Qwen3 Coder 30B: Coders for Every Hardware (Yes, Even the Pi) We're back at it with another GGUF quants release, this time focused on coder models and multimodal. We use our technology to find the optimal datatypes per layer to squeeze as much performance out of these models while compromising the least amount of accuracy.\n\n**TL;DR**\n\n* **Devstral**\u00a0is the hero on\u00a0**RTX 40/50 series**. Also: it has a\u00a0**quality cliff \\~2.30 bpw,**\u00a0but ShapeLearn avoids faceplanting there.\n*", "score": 8, "created": 1771523453.0, "url": "https://reddit.com/r/ByteShape/comments/1r96237/devstral_small_2_24b_qwen3_coder_30b_coders_for/"}, {"source": "Reddit", "text": "GLM-5 is officially on NVIDIA NIM, and you can now use it to power Claude Code for FREE \ud83d\ude80 NVIDIA just added `z-ai/glm5` to their NIM inventory, and I've updated `free-claude-code` to support it fully. You can now run Anthropic's Claude Code CLI using GLM-5 (or any number of open models) as the backend engine, completely free.\n\n**What is this?** `free-claude-code` is a lightweight proxy that converts Claude Code's Anthropic API requests into other provider formats. It started with NVIDIA NIM (fre", "score": 36, "created": 1771461013.0, "url": "https://reddit.com/r/LLMDevs/comments/1r8kgld/glm5_is_officially_on_nvidia_nim_and_you_can_now/"}, {"source": "Reddit", "text": "News discussion: Qwen 3.5 MXFP4 quants officially confirmed According to a recent thread on r/LocalLLaMA, Junyang Lin has confirmed that **Qwen 3.5** models will be receiving **MXFP4** (Microscaling Formats) quantization support.\n\nThis is a significant technical development for local tool performance. MXFP4 is designed to offer higher fidelity than standard integer quantization at similar compression levels. It aims to mitigate the \"perplexity cliff\" often seen when crunching large models down t", "score": 1, "created": 1771453083.0, "url": "https://reddit.com/r/AIToolsPerformance/comments/1r8h81p/news_discussion_qwen_35_mxfp4_quants_officially/"}, {"source": "Reddit", "text": "LLMs grading other LLMs 2 A year ago I made a [meta-eval here on the sub](https://www.reddit.com/r/LocalLLaMA/comments/1j1npv1/llms_grading_other_llms/), asking LLMs to grade a few criterias about other LLMs. \n\nTime for the part 2.\n\nThe premise is very simple, the model is asked a few ego-baiting questions and other models are then asked to rank it. The scores in the pivot table are normalised.\n\nYou can find [all the data on HuggingFace](https://huggingface.co/datasets/av-codes/cringebench) for ", "score": 231, "created": 1771429644.0, "url": "https://reddit.com/r/LocalLLaMA/comments/1r86i3o/llms_grading_other_llms_2/"}, {"source": "Reddit", "text": "Is GPT-5 Nano actually usable for coding, or is it just a glorified summarizer? I\u2019ve been looking at the pricing for the new \"Nano\" class models like **GPT-5 Nano** ($0.05/M) and **Nemotron Nano 9B V2** ($0.04/M). On paper, they look like a dream for high-volume tasks, but I\u2019m struggling to find a real place for them in my dev workflow.\n\nI tried using **Nemotron Nano** to write basic unit tests for a series of CRUD operations. Out of 10 tests, it hallucinated the import paths in 4 of them and co", "score": 1, "created": 1771424433.0, "url": "https://reddit.com/r/AIToolsPerformance/comments/1r848rh/is_gpt5_nano_actually_usable_for_coding_or_is_it/"}, {"source": "Reddit", "text": "Trouble accessing the local file system when using a local LLM Hi all, \n\nI've got a problem.  I get a local LLM running with Ollama and Claude Code on Windows 11 pro.  I can ask it questions and it percolates, but anytime I ask it to work with my files it fails.  How it fails depends on what model I am running, but always with a long delay and then an error about\n\n  \n\"I'm sorry, but I don't have access to the tools needed to determine the current working directory. If you need help with somethin", "score": 3, "created": 1771422179.0, "url": "https://reddit.com/r/ClaudeCode/comments/1r83bu2/trouble_accessing_the_local_file_system_when/"}, {"source": "Reddit", "text": "Q: How do I use Eagle3 to make MLX go faster? This is one of those dumb question worth asking. There are like half a dozen models that seems to be very portable and yet not necessary \"fast as lightning\" like linear attention models. I wanted to see if Eagle3 would support them, but a lot of the Eagle3 models in HuggingFace is made for vLLM/SGLang instead! What else can I do to make models go even faster other than quantization?\n\n* Qwen3-Coder-30B-A3B\n* Qwen3-32B\n* GLM-4.7 Flash\n* Devstral-Small-", "score": 1, "created": 1771391192.0, "url": "https://reddit.com/r/LocalLLaMA/comments/1r7u212/q_how_do_i_use_eagle3_to_make_mlx_go_faster/"}, {"source": "Reddit", "text": "Devstral 2 or whatever feels appropriate to run on server with 24 VRAM and 256 GB RAM Hello there!\n\nI'm thinking about turning my server from hobbyist machine for generating images via ComfyUI (Stable Diffusion) into DevOps assistant (coding and agentic local LLM for software engineering) with focus on troubleshooting Java, Kotlin and Go code, along with troubleshooting via cli tools like kubectl, aws-cli, and good ol' Bash.\n\nI have:\n\n* Intel Xeon W-2275 @ 3.30GHz (14 cores, 28 threads)\n* NVIDIA", "score": 1, "created": 1771364482.0, "url": "https://reddit.com/r/LocalLLaMA/comments/1r7jq4j/devstral_2_or_whatever_feels_appropriate_to_run/"}, {"source": "Reddit", "text": "Can Your Local Setup Complete This Simple Multi Agent Challenge? TLTR: I couldn't get qwen3-coder-next, glm-4.7-flash, Devstral-Small-2, and gpt-oss-20b to complete a simple multi-agent task below: summarizing 10 transcripts, about 4K tokens per file.\n\nIf your local setup can complete this challenge end to end autonomously (AKA YOLO mode) with no intervention, I would appreciate hearing your setup and how you are using.\n\nhttps://github.com/chigkim/collaborative-agent\n\n---\n\nUpdate: My Suspicion s", "score": 0, "created": 1771350780.0, "url": "https://reddit.com/r/LocalLLaMA/comments/1r7d9xb/can_your_local_setup_complete_this_simple_multi/"}, {"source": "Reddit", "text": "vibe tends to stop without concluding answers or suggesting the next steps It is (almost) always the case  \n  \nEspecially when I am in \"Planification\" mode (so not actively writing on files), he will update his own todo, read and write, tick 2 or 3 boxes out of 5 tasks and stop right at the middle. \n\nSometimes even weider he will tick all tasks but stop right after, no conclusion report on what has been done, some suggestion, further question, have to ask him twice to \"wake him up\". Anyone with ", "score": 2, "created": 1771345633.0, "url": "https://reddit.com/r/MistralVibe/comments/1r7aqjb/vibe_tends_to_stop_without_concluding_answers_or/"}, {"source": "Reddit", "text": "The only model that works is gpt-oss Hello,\n\nI set up a local machine in my network that runs ollama a couple of weeks ago. I have in addition set up OpenCode as a coding agent and connected it to the ollama server in my network.\n\nI was hoping to check out some agentic programming with the differnt models; qwen2.5-coder, devstral and such. But for some reason none of them work. However, gpt-oss does work! I can prompt it in OpenCode and I get the result I want. I also have some success with ralp", "score": 0, "created": 1771281147.0, "url": "https://reddit.com/r/LocalLLaMA/comments/1r6o7va/the_only_model_that_works_is_gptoss/"}, {"source": "Reddit", "text": "Are 20-100B models enough for Good Coding? The reason I'm asking this question because some folks(including me) are in self-doubt little bit. Maybe because after seeing threads about comparison with Online models(More than Trillions of parameters).\n\nOf course, we can't expect same coding performance &amp; output from these 20-100B models.\n\nSome didn't even utilize full potential of these local models. I think only 1/3 of folks hit the turbo with these models.\n\nPersonally I never tried Agentic co", "score": 77, "created": 1771270712.0, "url": "https://reddit.com/r/LocalLLaMA/comments/1r6jklq/are_20100b_models_enough_for_good_coding/"}, {"source": "Reddit", "text": "Any idea when Successors of current DGX Spark &amp; Strix Halo gonna arrive? For inference, Current version is suitable &amp; enough only up to 100B MOE models. \n\nFor big/large MOE models &amp; medium/big Dense models, it's not suitable as those devices have only 128GB unified RAM &amp; around 300 GB/s bandwidth.\n\nIt would be great to have upgraded versions with 512GB/1TB variant + 1-2 TB/s bandwidth so it's possible to use 150-300B MOE models &amp; 20-100B Dense models with good t/s.\n\nBelow are", "score": 6, "created": 1771257631.0, "url": "https://reddit.com/r/LocalLLaMA/comments/1r6dge2/any_idea_when_successors_of_current_dgx_spark/"}, {"source": "Reddit", "text": "How to integrate local ollama into vs code? [I added http:\\/\\/localhost:11434 to \\\\\"Language Models\\\\\" as \\\\\"AGX\\\\\", enabled select models and nothing...](https://preview.redd.it/bqtub1ngjvjg1.png?width=1009&amp;format=png&amp;auto=webp&amp;s=552d2c8e994b5959881f5d11660e9534762c83fc)\n\n", "score": 0, "created": 1771255398.0, "url": "https://reddit.com/r/vscode/comments/1r6cg6r/how_to_integrate_local_ollama_into_vs_code/"}, {"source": "Reddit", "text": "Janitor with Rout.My proxy? Hi all,\n\nIve tested JanAI with a few proxy sites.\n\nAs you know, ones like OpenRouter and LiteRouter, among others, have something like 30 messages or less free per day.\n\nI went around a long list that some lovely so compiled for JanAI proxies and joined the Rout.My discord to get on their free tier \n\nI have so far loved deekseek, devstral/mistral..so wanted to see what other models do.\n\nQwen was questionable and responses were meh.\n\nRout.My has gpt models, so far very", "score": 0, "created": 1771247901.0, "url": "https://reddit.com/r/JanitorAI_Refuges/comments/1r69e7q/janitor_with_routmy_proxy/"}, {"source": "Reddit", "text": "No more Devstral I found out this gem, awesomly fast and accurate model for subagents, that follow instructions and now its removed?... Is there any chance you return back mistralai/Devstral-2-123B-Instruct-2512-TEE", "score": 6, "created": 1771205616.0, "url": "https://reddit.com/r/chutesAI/comments/1r5w9vw/no_more_devstral/"}, {"source": "Reddit", "text": "Meta's AIRS-Bench reveals why no single agent pattern wins If you're building multi-agent systems, you've probably observe that your agent crushes simple tasks but fumbles on complex ones, or vice versa.\n\nGithub :\u00a0[https://github.com/facebookresearch/airs-bench](https://github.com/facebookresearch/airs-bench)\n\nMeta's AIRS-Bench research reveals why it happens. Meta tested AI agents on 20 real machine learning research problems using three different reasoning patterns.\n\n1. The first was ReAct, a ", "score": 1, "created": 1771193994.0, "url": "https://reddit.com/r/EngineeringGTM/comments/1r5rw6g/metas_airsbench_reveals_why_no_single_agent/"}, {"source": "Reddit", "text": "Why Devstral Small 2 is \"comfy\" but MiniMax M2.5 is actually SOTA for local agents I see the Devstral Small 2 fans, but let's look at the benchmarks. MiniMax M2.5 is hitting 80.2% on SWE-Bench Verified. That's not just \"good,\" it's SOTA. It's a 10B active parameter model that functions as a Real World Coworker for $1 an hour. Mistral is fine for basic local chat, but for complex, multi-step agentic workflows, MiniMax is simply more stable. Read their RL technical blog - they've solved the tool-c", "score": 0, "created": 1771083937.0, "url": "https://reddit.com/r/MistralAI/comments/1r4nvdq/why_devstral_small_2_is_comfy_but_minimax_m25_is/"}, {"source": "Reddit", "text": "Google Gemini 3.1 Pro Preview Soon? GOOGLE MIGHT BE PREPARING GEMINI 3.1 PRO PREVIEW FOR RELEASE! \n\nThe same reference has been spotted on the Artificial Analysys Arena earlier. \n\nSource: x -&gt; testingcatalog/status/2021718211662614927\n\nx -&gt; synthwavedd/status/2021707113177747545", "score": 214, "created": 1770852436.0, "url": "https://reddit.com/r/singularity/comments/1r2d9o6/google_gemini_31_pro_preview_soon/"}, {"source": "Reddit", "text": "Claude code router with local LLMs? Hey so I am playing around with using a local LLM like gemma 27b or qwen coder or even devstral. I got it setup and was able to use them through claude code.\n\nusing llama.cpp on my desktop with a 3090 ti and then running claude code on my macbook.\n\nHowever when I tried to do something with files, I got one response saying it can't access my files? I thought claude code handles the reading part. Am I doing something wrong here?\n\nAren't these models supposed to ", "score": 2, "created": 1770817427.0, "url": "https://reddit.com/r/LocalLLaMA/comments/1r1xqjp/claude_code_router_with_local_llms/"}, {"source": "Reddit", "text": "Looking for suggestions for a local LLM to use with open code or claude code. Hi I am fairly new to this, so please excuse my naivety.\n\nMy device specs are:\n\nNVIDIA 4060ti 16GB VRAM\n32 GB DDR5 RAM\nIntel i5-13600K\n\nSo far I have tried gpt-oss-20b, GLM-4.7 Flash, Devstral Small 2-24B.\n\nGpt-oss works okay with opencode and is fast enough on my device, but sometimes gets into these loops where it fails to run a command and then keeps generating tokens.\n\nDevstral Small 2-24B runs a bit slow to make i", "score": 6, "created": 1770775271.0, "url": "https://reddit.com/r/LocalLLaMA/comments/1r1kfg7/looking_for_suggestions_for_a_local_llm_to_use/"}, {"source": "Reddit", "text": "PSA on llama.cpp \u2014spec-type ngram-mod (use LF not CRLF, 35x speedup) TLDR; if using llama-server with \u2014spec-type ngram-mod, and pasting/uploading/sending text files, make sure the files use LF instead of CRLF.\n\nWhen I would copy a file from vscode and paste into the native llama-server webui with ngram speculative decoding enabled, there was no speed boost for file editing responses. I would only get a speed boost on the models second response (if I asked it to make a minor change to its first r", "score": 49, "created": 1770774519.0, "url": "https://reddit.com/r/LocalLLaMA/comments/1r1k5gn/psa_on_llamacpp_spectype_ngrammod_use_lf_not_crlf/"}, {"source": "Reddit", "text": "I benchmarked the newest 40 AI models (Feb 2026) Everyone is talking about the viral Kimi k2.5 and Claude Opus 4.6 right now. But while the world was watching the giants, I spent the last week benchmarking 40 of the newest models on the market to see what's actually happening with Price vs. Performance.\n\n**The TL;DR:** The market has split into two extremes. \"Mid-range\" models are now a waste of money. You should either be in \"God Mode\" or \"Flash Mode.\"\n\n**Here is the hard data from Week 7:**\n\nh", "score": 0, "created": 1770738508.0, "url": "https://reddit.com/r/LocalLLaMA/comments/1r14bqk/i_benchmarked_the_newest_40_ai_models_feb_2026/"}, {"source": "Reddit", "text": "Plenty of medium size(20-80B) models in last 3 months. How those works for you? We got plenty of medium size(20-80B) models in last 3 months before upcoming models. These models are good even for 24/32GB VRAM + RAM @ Q4/Q5 with decent context.\n\n* Devstral-Small-2-24B-Instruct-2512\n* Olmo-3.1-32B\n* GLM-4.7-Flash\n* Nemotron-Nano-30B\n* Qwen3-Coder-Next &amp; Qwen3-Next-80B\n* Kimi-Linear-48B-A3B\n\nI think most issues(including FA issue) haven been fixed for GLM-4.7-Flash.\n\nBoth Qwen3-Next models went", "score": 43, "created": 1770736517.0, "url": "https://reddit.com/r/LocalLLaMA/comments/1r13ffw/plenty_of_medium_size2080b_models_in_last_3/"}, {"source": "Reddit", "text": "The best agent for Nextjs is Codex 5.3 with a 90% success rate, Codex 5.3 xhigh is 10 points higher than Claude Opus 4.6. and double what Codex 5.2 scored [https://nextjs.org/evals](https://nextjs.org/evals)", "score": 72, "created": 1770709565.0, "url": "https://reddit.com/r/codex/comments/1r0uo57/the_best_agent_for_nextjs_is_codex_53/"}, {"source": "Reddit", "text": "Cursor + Chutes = unlimited AI coding power \ud83d\udd25 Cursor + Chutes = unlimited AI coding power \ud83d\udd25\n\nStop paying $20/month for one model locked behind rate limits.\n\nWith Chutes, you get 60+ open source models inside Cursor, DeepSeek V3.2, GLM-4.7, Kimi K2.5, and more.\n\nSetup takes 2 minutes. Watch the video!\n\n  \nWhy this matters:  \nCursor's built-in models hit rate limits fast. When you're deep in a coding session, the last thing you need is a \"slow down\" message.  \nChutes gives you OpenAI-compatible en", "score": 10, "created": 1770648807.0, "url": "https://reddit.com/r/chutesAI/comments/1r068z3/cursor_chutes_unlimited_ai_coding_power/"}, {"source": "Reddit", "text": "Which best coding open source model compatible with agent mode ? We are a small team of dev (2 and half \\^\\^) with a simple Github Copilot pro plan. We are not full vibe coders (10y to 30y exp devs) but it helps a lot to improve our productivity with our low budgets (gov organisation \\^\\^).\n\nWe have in addition a server with a RTX 6000 96Gb Gpu to experiment some N8N workflows.\n\nWe tried to put a Devstral 2 model on it, and it works. But we tried to put it on our Github Copilot account and it on", "score": 2, "created": 1770622084.0, "url": "https://reddit.com/r/GithubCopilot/comments/1qzxw4n/which_best_coding_open_source_model_compatible/"}, {"source": "Reddit", "text": "Is GPT-5.1-Codex-Max worth the 18x price premium over Devstral 2? I\u2019ve been looking at the latest pricing for **GPT-5.1-Codex-Max** ($1.25/M) and comparing it to the performance I'm getting from **Devstral 2 2512** ($0.05/M). With **Qwen3.5** support finally merged into `llama.cpp` today, the barrier for high-tier local coding assistance has basically vanished.\n\nI ran a benchmark on a complex React refactor involving nested state and custom hooks:\nbash\n# Testing local Qwen3.5 Coder 30B \n./llama-", "score": 3, "created": 1770618064.0, "url": "https://reddit.com/r/AIToolsPerformance/comments/1qzwqil/is_gpt51codexmax_worth_the_18x_price_premium_over/"}, {"source": "Reddit", "text": "Trying claude with ollama is going... weird? ", "score": 12, "created": 1770596596.0, "url": "https://reddit.com/r/vibecoding/comments/1qzpito/trying_claude_with_ollama_is_going_weird/"}, {"source": "Reddit", "text": "Local LLMs - Good General &amp; Coding models Frequently mentioned General &amp; Coding models in LLM subs(sorted by size):\n\n* GPT-OSS-20B\n* Devstral-Small-2-24B-Instruct-2512\n* Qwen3-30B-A3B\n* Qwen3-30B-Coder\n* Nemotron-3-Nano-30B-A3B\n* Qwen3-32B\n* GLM-4.7-Flash\n* Seed-OSS-36B\n* Kimi-Linear-48B-A3B\n* Qwen3-Next-80B-A3B\n* Qwen3-Coder-Next\n* GLM-4.5-Air\n* GPT-OSS-120B\n* Devstral-2-123B-Instruct-2512\n* Step-3.5-Flash\n* MiniMax-M2.1, 2\n* Qwen3-235B-A22B\n* GLM-5, 4.5, 4.6, 4.7\n* Qwen3-480B-Coder\n* D", "score": 3, "created": 1770566733.0, "url": "https://reddit.com/r/u_pmttyji/comments/1qzcrgj/local_llms_good_general_coding_models/"}, {"source": "Reddit", "text": "Gas Town with Qwen 3 Mayor on Ollama, Claude With 64GB M4 Max MacBook pro, I got Gas Town finally setup to run using Qwen 3, 70b and Devstral for polecats, etc.\n\nQwen 2.5 was not even executing gt hook \n\n\nSo, what are you guys doing with local AI for agent orchestration \nLocal open openclaw like setup is interesting as well.\n\n\nI am not impressed with my setup, so now the fun \ud83d\ude0a begins \n\nAre you guys using lammacpp or what on macos?\nI need to free up more ram, as well as overall sluggish ness", "score": 10, "created": 1770563606.0, "url": "https://reddit.com/r/LocalLLM/comments/1qzberq/gas_town_with_qwen_3_mayor_on_ollama_claude/"}, {"source": "Reddit", "text": "New computer arrived... JAN is still super slow. Hi all,\n\nJust received my new laptop: Thinkpad P1 Gen 8 with 64GB RAM an Intel(R) Core(TM) Ultra 9 285H processor and a RTX PRO 2000 BLACKWELL NVIDIA GPU.\n\nDownloaded JAN (latest version).\n\nEnabled the GPU in the Settings &gt;&gt; Hardware.\n\nInstalled the DEVSTRAL-Small-2507-GGUF model and asked it a question.\n\nAnd I started getting words at a pace of 1 word per second max... and the GPU seemed not to be in use...\n\nIs there something else that is ", "score": 0, "created": 1770561943.0, "url": "https://reddit.com/r/LocalLLaMA/comments/1qzaps9/new_computer_arrived_jan_is_still_super_slow/"}, {"source": "Reddit", "text": "What models are you running on RTX 3060 12GB in 2026? Hey everyone!\n\nI'm running a single RTX 3060 12GB with llama.cpp (no offloading tricks, just --n-gpu-layers -1) and I'm quite happy with my current trio, but I'd love to hear what other people are using on similar hardware in early 2026.\n\nMy current setup (exact commands I use):\n\n1. \\*\\*Magnum-v4 9B Q5\\_K\\_M\\*\\*\n2. \u2192 Great for general knowledge, culture/history/socio-econ, immersive narration/RP, uncensored cybersecurity/pentest, storytelling", "score": 30, "created": 1770551109.0, "url": "https://reddit.com/r/LocalLLaMA/comments/1qz6w36/what_models_are_you_running_on_rtx_3060_12gb_in/"}, {"source": "Reddit", "text": "Qwen3 Coder Next as first \"usable\" coding model &lt; 60 GB for me I've tried lots of \"small\" models &lt; 60 GB in the past. GLM 4.5 Air, GLM 4.7 Flash, GPT OSS 20B and 120B, Magistral, Devstral, Apriel Thinker, previous Qwen coders, Seed OSS, QwQ, DeepCoder, DeepSeekCoder, etc. So what's different with Qwen3 Coder Next in OpenCode or in Roo Code with VSCodium?\n\n* **Speed**: The reasoning models would often yet not always produce rather good results. However, now and then they'd enter reasoning l", "score": 381, "created": 1770547439.0, "url": "https://reddit.com/r/LocalLLaMA/comments/1qz5uww/qwen3_coder_next_as_first_usable_coding_model_60/"}, {"source": "Reddit", "text": "News reaction: GPT-5 Mini launch and the gpt-oss-120b price war OpenAI just stealth-dropped **GPT-5 Mini** on OpenRouter, and the specs are wild: a 400,000 token window for just $0.25/M. It\u2019s clearly a direct response to the recent context window wars. Even more interesting is **GPT-5.1-Codex**\u2014at $1.25/M, it\u2019s pricey, but the logic depth for complex refactoring is a noticeable step up from the previous o-series.\n\nOn the local front, the `llama.cpp` community is seeing some insane benchmarks wit", "score": 1, "created": 1770546027.0, "url": "https://reddit.com/r/AIToolsPerformance/comments/1qz5gt2/news_reaction_gpt5_mini_launch_and_the_gptoss120b/"}, {"source": "Reddit", "text": "Benchmarking total wait time instead of pp/tg I find pp512/tg128 numbers not very useful for judging real-world performance. I've had setups that looked acceptable on paper but turned out to be too slow in real use.\n\nSo I started benchmarking total time to process realistic context sizes (1k to 64k tokens) + generation (always 500 tokens), which I think better represents what actually matters: how long do I need to wait?\n\nAutomated the whole process and put results on a website. Attached a scree", "score": 59, "created": 1770484955.0, "url": "https://reddit.com/r/LocalLLaMA/comments/1qyjm0l/benchmarking_total_wait_time_instead_of_pptg/"}, {"source": "Reddit", "text": "Europe can still be competitive in AI I\u2019m sharing my thoughts and an initiative here\u2014I hope it doesn\u2019t come across as spam, but I\u2019d love to spark a discussion.\n\nI often hear that Europe is hopelessly behind in AI (and not just in AI, by the way). While it\u2019s undeniable that there\u2019s a gap, Mistral\u2019s Devstral 2 model scores 72.2 on SWE-Bench verified, matching the performance of Claude Opus 4, which was released 7 months earlier.\n\nWe could argue that Mistral is only 7 months behind, despite:\n\n* Bud", "score": 17, "created": 1770456212.0, "url": "https://reddit.com/r/EU_Economics/comments/1qy9f20/europe_can_still_be_competitive_in_ai/"}, {"source": "Reddit", "text": "Europe can still be competitive in AI - Mistral should take a part in it I\u2019m sharing my thoughts and an initiative here\u2014I hope it doesn\u2019t come across as spam, but I\u2019d love to spark a discussion.\n\nI often hear that Europe is hopelessly behind in AI (and not just in AI, by the way). While it\u2019s undeniable that there\u2019s a gap, Mistral\u2019s Devstral 2 model scores 72.2 on SWE-Bench verified, matching the performance of Claude Opus 4, which was released 7 months earlier.\n\nWe could argue that Mistral is on", "score": 210, "created": 1770402418.0, "url": "https://reddit.com/r/MistralAI/comments/1qxpmrw/europe_can_still_be_competitive_in_ai_mistral/"}, {"source": "Reddit", "text": "Anthropic releases opus 4.6 Anthropic just released their latest opus model.\n\nWhere is mistral? Devstral is nice and all but far far behind the competition. Give us something even similar to sonnet 4.5 and I\u2019ll switch happily but devstral is unusable right now.\n\nEDIT: When I refer to devstral I am comparing Vibe CLI (with devstral running in the cloud) VS claude code. It seems people think I am comparing opus to me devstral running in a local computer.", "score": 0, "created": 1770324517.0, "url": "https://reddit.com/r/MistralAI/comments/1qwx4gw/anthropic_releases_opus_46/"}, {"source": "Reddit", "text": "Devstral 2 vs Gemini 2.5 Pro: Benchmark results for Python refactoring at scale I spent the afternoon running a head-to-head benchmark on several massive legacy Python repos to see which model handles repo-level refactoring without breaking the bank. I focused on **Devstral 2 2512**, **Gemini 2.5 Pro Preview**, and **Olmo 3 7B Instruct**.\n\n**The Setup**\nI used a custom script to feed each model a 50k token context containing multiple inter-dependent files. The goal was to migrate synchronous dat", "score": 1, "created": 1770315627.0, "url": "https://reddit.com/r/AIToolsPerformance/comments/1qwt1e8/devstral_2_vs_gemini_25_pro_benchmark_results_for/"}, {"source": "Reddit", "text": "We created a Tool Calling Guide for LLMs! We made a guide on how to do tool calling with local LLMs.\n\nLearn how to use open models like Qwen3-Coder-Next and GLM-4.7-Flash for function calling.\n\nHas hands-on examples for: story writing, Python execution, terminal tool calls, maths and more.\n\nGuide: [https://unsloth.ai/docs/basics/tool-calling-guide-for-local-llms](https://unsloth.ai/docs/basics/tool-calling-guide-for-local-llms)\n\nLet us know if you have any feedback! :)", "score": 247, "created": 1770307239.0, "url": "https://reddit.com/r/unsloth/comments/1qwp508/we_created_a_tool_calling_guide_for_llms/"}, {"source": "Reddit", "text": "Leaderboard benchmarks for Open Agentic Models I have always heard the word agentic AI, and AI agent harness scaffold \u2026etc\n\nAnd to me this was About hooking up a chat agent with an environment (terminal Python \u2026etc) and letting it take action (agent)\n\nI believe the first to do so was BabyAGI harness \n\nHowever recently I started to notice that benchmarks like MMLU score for an example, don\u2019t even matter the slightest bit for such tasks as compared to my own experience with each model \n\nI think be", "score": 2, "created": 1770277412.0, "url": "https://reddit.com/r/LocalLLaMA/comments/1qwf4il/leaderboard_benchmarks_for_open_agentic_models/"}, {"source": "Reddit", "text": "Local filesystem access with Mistral Le Chat - possible? Is there any way to set up Mistral Le Chat to access folders in my local filesystem through an MCP server? Mistral doesn't have a desktop app, so that doesn't seem to be an option. Currently the only way I can use any Mistral models with my local filesystem is by using Vibe-CLI, but that only offers a couple of models (devstral-2 and one other). ", "score": 3, "created": 1770244249.0, "url": "https://reddit.com/r/MistralAI/comments/1qw38nr/local_filesystem_access_with_mistral_le_chat/"}, {"source": "Reddit", "text": "Open-weight models dominate JSON parsing benchmark \u2014 Gemma 3 27B takes first, raw code inside The Multivac runs daily peer evaluations where models judge each other blind. Today's coding challenge: build a production JSON path parser.\n\n**Top 5 (all open-weight):**\n\n|Model|Score|License|\n|:-|:-|:-|\n|Gemma 3 27B|9.15|Gemma Terms|\n|Devstral Small|8.86|Apache 2.0|\n|Llama 3.1 70B|8.16|Llama 3.1|\n|Phi-4 14B|8.02|MIT|\n|Granite 4.0 Micro|7.44|Apache 2.0|\n\nNo proprietary models in this eval (SLM pool onl", "score": 2, "created": 1770174004.0, "url": "https://reddit.com/r/OpenSourceeAI/comments/1qvcv7v/openweight_models_dominate_json_parsing_benchmark/"}, {"source": "Reddit", "text": "10 SLMs tried to write a JSON parser. 3 of them generated zero code. Here's the raw outputs. I run peer evaluations of language models (The Multivac). Today: 10 small models (&lt;48B params) vs a production-ready JSON parsing challenge.\n\n**The Challenge:**\n\n    - Path syntax: \"user.profile.settings.theme\"\n    - Array indices: \"users[0].name\"  \n    - Handle missing keys gracefully\n    - Detect circular references\n    - Type hints + docstrings\n\n**Results:**\n\nhttps://preview.redd.it/l7dhj0cw7ehg1.j", "score": 1, "created": 1770173950.0, "url": "https://reddit.com/r/LocalLLM/comments/1qvcug8/10_slms_tried_to_write_a_json_parser_3_of_them/"}, {"source": "Reddit", "text": "Gemma 3 27B just mass-murdered the JSON parsing challenge \u2014 full raw code outputs inside Running daily peer evaluations of language models (The Multivac). Today's coding challenge had some interesting results for the local crowd.\n\n**The Task:** Build a production-ready JSON path parser with:\n\n* Dot notation (`user.profile.settings.theme`)\n* Array indices (`users[0].name`)\n* Graceful missing key handling (return None, don't crash)\n* Circular reference detection\n* Type hints + docstrings\n\n**Final ", "score": 0, "created": 1770173878.0, "url": "https://reddit.com/r/LocalLLaMA/comments/1qvcthc/gemma_3_27b_just_massmurdered_the_json_parsing/"}, {"source": "Reddit", "text": "How can I classify the downloaded llms? Hi, how can I find out what I can and can't do with these models? The icons help a little, but of course, would I have to go through the documentation for each one individually? When I ask the models in the chat what they can do, almost all of them say the same thing. Or is it better to rely on benchmarks? It would be great if it were possible to add notes or personal comments in a section of LMStudio or similar programs.", "score": 0, "created": 1770166622.0, "url": "https://reddit.com/r/LocalLLaMA/comments/1qva32b/how_can_i_classify_the_downloaded_llms/"}, {"source": "Reddit", "text": "Ecosia uses GPT-4.1 REVEALED (GPT-4.1 Mini / Nano) **TL;DR:**\n\n**Ecosia, a European alternative to Google, uses OpenAI's cheaper, less capable AI models, which are comparable to European Mistral's cheaper, more capable AI models.**  \n  \n**Ecosia AI Full System Prompt, revealed via prompt injection.**\n\nWe all heard Ecosia is using Open AI for its search summaries \"overviews\" and Ecosia AI Search /Chat.  \nBut because Ecosia wasn't transparent about the details. We didn't know which model (s) it us", "score": 253, "created": 1770158962.0, "url": "https://reddit.com/r/BuyFromEU/comments/1qv6yyk/ecosia_uses_gpt41_revealed_gpt41_mini_nano/"}, {"source": "Reddit", "text": "New combination of LM Studio + Claude Code very frustrating Cheers everyone :)\n\nFirst things first: I do like testing some stuff, but ultimately I do prefer simple solutions. Hence, I use Windows (boooo, yes yes ;)) , 128 GB RAM and 16 GB VRAM (RTX 4080) with LM Studio and VS Code.\n\nSo far, I have used Cline or Kilo or Roo Code as VS Code extensions, all of them combined with varying models (Qwen3, OSS 120B, Devstral Small 2, GLM 4.7 flash, ... you know it, it's the communities' favorites) hoste", "score": 8, "created": 1770063935.0, "url": "https://reddit.com/r/LocalLLM/comments/1qu6jwe/new_combination_of_lm_studio_claude_code_very/"}, {"source": "Reddit", "text": "All Major LLM Releases from 2025 - Today (Source:Lex Fridman State of Ai in 2026 Video) ", "score": 104, "created": 1770056666.0, "url": "https://reddit.com/r/singularity/comments/1qu33gu/all_major_llm_releases_from_2025_today_sourcelex/"}, {"source": "Reddit", "text": "Kalynt \u2013 Privacy-first AI IDE with local LLMs , serverless P2P and more... Hey r/LocalLLaMA,\n\nI've been working on **Kalynt**, an open-core AI IDE that prioritizes local inference and privacy. After lurking here and learning from your optimization discussions, I wanted to share what I built.\n\n**The Problem I'm Solving:**\n\nTools like Cursor and GitHub Copilot require constant cloud connectivity and send your code to external servers. I wanted an IDE where:\n\n* Code never leaves your machine unless", "score": 0, "created": 1770039196.0, "url": "https://reddit.com/r/LocalLLaMA/comments/1qtv57o/kalynt_privacyfirst_ai_ide_with_local_llms/"}, {"source": "Reddit", "text": "Built an orchestration layer for Claude - Agents spawning agents - built an orchestration layer where AI can create and coordinate sibling instances, coordinate work, Desktop UI to monitor everything https://preview.redd.it/wht28777nzgg1.png?width=1919&amp;format=png&amp;auto=webp&amp;s=f54a5c5f3f28ea7dedb6547d3bc77397af7e6831\n\n[https://www.youtube.com/watch?v=2\\_zsmgBUsuE](https://www.youtube.com/watch?v=2_zsmgBUsuE)\n\nBuilt a platform specifically for orchestrating Claude agents.\n\n\\*\\*How it wo", "score": 1, "created": 1769997644.0, "url": "https://reddit.com/r/Anthropic/comments/1qthyud/built_an_orchestration_layer_for_claude_agents/"}, {"source": "Reddit", "text": "Mistral Vibe vs Claude Code vs OpenAI Codex vs Opencode/others? Best coding model for 92GB? I've dipped my toe in the water with Mistral Vibe, using LM Studio and Devstral Small for inference. I've had pretty good success refactoring a small python project, and a few other small tasks.\n\nOverall, it seems to work well on my MacBook w/ 92GB RAM, although I've encountered issues when it gets near or above 100k tokens of context. Sometimes it stops working entirely with no errors indicated in LM Stu", "score": 27, "created": 1769992698.0, "url": "https://reddit.com/r/LocalLLaMA/comments/1qtg3sm/mistral_vibe_vs_claude_code_vs_openai_codex_vs/"}, {"source": "Reddit", "text": "I love Mistral This is my second post in a long time praising mistral \n\nSo earlier I praised how they train objective models that services Le Mistral \n\nNow I am doing this again, but as I am running and switching between many models for local agentic tasks (using an agent scaffold and and MCP to perform basic static malware analysis tasks for cybersecurity that is essentially copy pasting to and from an LLM model in an automated way!) \n\nI tried many things \n\nFirst \u201cfrontier\u201d (local frontier for ", "score": 87, "created": 1769988702.0, "url": "https://reddit.com/r/MistralAI/comments/1qteixn/i_love_mistral/"}, {"source": "Reddit", "text": "Built an orchestration layer for Claude Code - spawn agents, coordinate work, see everything https://preview.redd.it/oboyoyijczgg1.png?width=1919&amp;format=png&amp;auto=webp&amp;s=8cd6047b0755254537e4d642b086a2448a53d7ba\n\n  \n\n\nhttps://preview.redd.it/a7i6co2ubzgg1.png?width=1535&amp;format=png&amp;auto=webp&amp;s=f62311099aa3a86b7786a603874632e27a0e3475\n\n[https://www.youtube.com/watch?v=2\\_zsmgBUsuE](https://www.youtube.com/watch?v=2_zsmgBUsuE)\n\nMade an MCP-based platform that extends what Clau", "score": 2, "created": 1769976539.0, "url": "https://reddit.com/r/ClaudeAI/comments/1qt98g6/built_an_orchestration_layer_for_claude_code/"}, {"source": "Reddit", "text": "Multi-model orchestration - Claude API + local models (Devstral/Gemma) running simultaneously https://preview.redd.it/kfi976ktczgg1.png?width=1919&amp;format=png&amp;auto=webp&amp;s=096e76694b4c6162428aa9087318b7781d3e6722\n\nhttps://preview.redd.it/f60rv9i69zgg1.png?width=1535&amp;format=png&amp;auto=webp&amp;s=910c55642dd31f1f385f95d2ba4e71f65cdc40df\n\n[https://www.youtube.com/watch?v=2\\_zsmgBUsuE](https://www.youtube.com/watch?v=2_zsmgBUsuE)\n\nBuilt an orchestration platform that runs Claude API ", "score": 1, "created": 1769976415.0, "url": "https://reddit.com/r/LocalLLaMA/comments/1qt96ej/multimodel_orchestration_claude_api_local_models/"}, {"source": "Reddit", "text": "Model recommendation question for an old laptop - coding, JAN 2026 I am probably scraping the bottom of the barrel of what's possible with local LLM, but I'll be in a cold hard grave before I become dependent on someone else's API access and I don't have money to invest in a new rig right now.\n\nI am looking into a way to try out new \"agentic\" solutions for coding and I have not yet been able to find something that satisfies my needs with what I have.\n\nI'm running a 1650ti (4GB) with 16gb of RAM.", "score": 0, "created": 1769787022.0, "url": "https://reddit.com/r/LocalLLaMA/comments/1qr8l7j/model_recommendation_question_for_an_old_laptop/"}, {"source": "Reddit", "text": "How do you use vibe effectively? I am using the Vibe CLI to work on a project. I feel like the coding capabilities of the underlying devstral model are pretty good, but I have the impression that I am not using it right. Has anybody here tips and tricks on how to really take the best out of this tool? What do you put in your instructions? Do you write instructions for each project? How do you smoothly integrate with your IDE or code editor?\n\nI suppose we are all still learning how to use coding ", "score": 12, "created": 1769770303.0, "url": "https://reddit.com/r/MistralAI/comments/1qr2987/how_do_you_use_vibe_effectively/"}, {"source": "Reddit", "text": "L'Europa pu\u00f2 ancora essere competitiva in AI Condivido qui i miei pensieri e un\u2019iniziativa a riguardo, spero che non sia considerato spam, ci terrei a creare una discussione.\n\nSento spesso dire che l\u2019Europa \u00e8 indietro e senza speranza in AI. Se da un lato \u00e8 innegabile, dall\u2019altro il modello Devstral 2 di Mistral ha 72.2 su SWE-Bench verified, ossia la stessa performance di Claude Opus 4, rilasciato 7 mesi prima. \n\nUn ritardo di 7 mesi di Mistral nonostante:\n\n- budget **decine** di volte inferior", "score": 9, "created": 1769768495.0, "url": "https://reddit.com/r/IA_Italia/comments/1qr1qwk/leuropa_pu\u00f2_ancora_essere_competitiva_in_ai/"}, {"source": "HackerNews", "text": "Show HN: MindMapp \u2013 Mind mapping app built by AI in 12 hours This is a simple Mind Mapping web based app built with open weight LLMs with focus on using locally deployed models like Devstral Small and Seed OSS.<p>Created this to get more familliar with AI assisted coding. 99% of code was written by LLM, however thinking, testing and debugging was still mostly done by me.<p>App is open source, code is on GitHub: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;cepa&#x2F;mindmapp\" rel=\"nofollow\">https:&", "score": 1, "created": "2026-01-15T11:15:53Z", "url": "https://news.ycombinator.com/item?id=46630918"}, {"source": "HackerNews", "text": "Mistral releases Devstral2 and Mistral Vibe CLI ", "score": 745, "created": "2025-12-09T14:45:01Z", "url": "https://news.ycombinator.com/item?id=46205437"}, {"source": "HackerNews", "text": "Mistral AI releases Devstral-Small-2507 ", "score": 4, "created": "2025-07-10T21:38:15Z", "url": "https://news.ycombinator.com/item?id=44525894"}, {"source": "HackerNews", "text": "Devstral Small and Medium 2507 ", "score": 2, "created": "2025-07-10T16:37:43Z", "url": "https://news.ycombinator.com/item?id=44522848"}, {"source": "HackerNews", "text": "Upgrading agentic coding capabilities with the new Devstral models ", "score": 7, "created": "2025-07-10T14:29:49Z", "url": "https://news.ycombinator.com/item?id=44521522"}, {"source": "HackerNews", "text": "Devstral Finetuned for SIMD Porting ", "score": 2, "created": "2025-07-08T18:34:22Z", "url": "https://news.ycombinator.com/item?id=44502727"}, {"source": "HackerNews", "text": "Ask HN: Openrouter Free Models Hey friends. It&#x27;s interesting to me that openrouter offers a bunch of completely free models, for example: https:&#x2F;&#x2F;openrouter.ai&#x2F;mistralai&#x2F;devstral-small:free ... What&#x27;s the point of these models ? Who is hosting them and offering these resources for free? What are the disadvantages of these? I understand that they are not the best but they seem good enough to me. Why would I pay to use the OpenAI API when I can use these for free ?", "score": 2, "created": "2025-05-26T06:53:59Z", "url": "https://news.ycombinator.com/item?id=44094707"}, {"source": "HackerNews", "text": "Ask HN: What's your favorite architect/editor pair with Aider? I&#x27;m currently using Gemini 2.5 Pro as the architect with Devstral Small (free version via OpenRouter) as the editor. I&#x27;ve only spent a few hours with it so far, but have been impressed with the quality of results and reduction of cost.<p>What pairs have you tried? Do you have any that you&#x27;ve been really impressed with?<p>This LLM leaderboard page is what got me interested:<p>https:&#x2F;&#x2F;aider.chat&#x2F;docs&#x2F;", "score": 14, "created": "2025-05-24T14:37:19Z", "url": "https://news.ycombinator.com/item?id=44081418"}, {"source": "HackerNews", "text": "Mistral's new Devstral AI model was designed for coding ", "score": 8, "created": "2025-05-21T15:35:39Z", "url": "https://news.ycombinator.com/item?id=44052611"}, {"source": "GitHub", "text": "Eval bug: agent software (mini-swe-agent) using unsloth/devstral-small-2 quant model running llama.cpp-server chokes ### Name and Version\n\n```bash\n$ llama-server --version\nggml_cuda_init: found 1 ROCm devices:\n  Device 0: AMD Radeon 890M Graphics, gfx1150 (0x1150), VMM: no, Wave Size: 32\nversion: 7950 (449ec2ab0)\nbuilt with Clang 22.0.0 for Linux\n```\n\n```bash\n#!/bin/bash\n\nexport LLAMA_ARG_MODEL=~/src/models/Devstral-Small-2-24B-Instruct-2512-UD-Q4_K_XL.gguf \n\nexport LLAMA_ARG_N_GPU_LAYERS=99\nexp", "score": 0, "created": "2026-02-24T12:51:21Z", "url": "https://github.com/ggml-org/llama.cpp/issues/19851"}, {"source": "GitHub", "text": "Add Bedrock Devstral 2 Adds Bedrock Devstral 2 DB entries\r\n\r\n## Pre-Submission checklist\r\n\r\n**Please complete all items before asking a LiteLLM maintainer to review your PR**\r\n\r\n- [ ] I have Added testing in the [`tests/litellm/`](https://github.com/BerriAI/litellm/tree/main/tests/litellm) directory, **Adding at least 1 test is a hard requirement** - [see details](https://docs.litellm.ai/docs/extras/contributing_code)\r\n- [ ] My PR passes all unit tests on [`make test-unit`](https://docs.litellm.", "score": 2, "created": "2026-02-22T02:25:16Z", "url": "https://github.com/BerriAI/litellm/pull/21865"}, {"source": "GitHub", "text": "Added Devstral-2 models, Updated Ministral-3 models ", "score": 0, "created": "2026-02-21T16:52:15Z", "url": "https://github.com/Haidra-Org/AI-Horde-text-model-reference/pull/242"}, {"source": "GitHub", "text": "Error: devstral-small-2:24b-cloud - 400 **Error Details**\n\nModel: devstral-small-2:24b-cloud\nProvider: ollama\nStatus Code: 400\n\n**Error Output**\n```\n\"Unexpected tool call id 4889b73de in tool results\"\n```\n\n**Additional Context**\nPlease add any additional context about the error here\n", "score": 0, "created": "2026-02-21T10:10:47Z", "url": "https://github.com/continuedev/continue/issues/10702"}, {"source": "GitHub", "text": "fix: add mergeToolResultText for Mistral/Devstral models on OpenRouter ### Related GitHub Issue\n\nCloses: #10618\n\n### Description\n\nThis PR attempts to address Issue #10618 where `mistralai/devstral-2512` on OpenRouter produces errors like \"Unexpected role user after role tool\" or \"The language model did not provide any assistant messages.\"\n\n**Root cause:** Mistral/Devstral models enforce strict message ordering and reject a `user` role message directly after a `tool` role message. The current cod", "score": 0, "created": "2026-02-20T19:05:28Z", "url": "https://github.com/RooCodeInc/Roo-Code/pull/11643"}, {"source": "GitHub", "text": "Error: LEO Software Engineer - mistralai/devstral-medium-2507 (128k) - 500 **Error Details**\n\nModel: LEO Software Engineer - mistralai/devstral-medium-2507 (128k)\nProvider: mistral\nStatus Code: 500\n\n**Error Output**\n```\n500 Internal Server Error\n```\n\n**Additional Context**\nPlease add any additional context about the error here\n", "score": 0, "created": "2026-02-18T03:05:55Z", "url": "https://github.com/continuedev/continue/issues/10607"}, {"source": "GitHub", "text": "Add missing maximum image documentation for devstral ", "score": 0, "created": "2026-02-17T12:23:31Z", "url": "https://github.com/mittwald/developer-portal/pull/964"}, {"source": "GitHub", "text": "Add support for devstral 2512 model aliases - labs-devstral-small-2512 supports devstral-small-latest\r\n- devstral-2512 supports devstral-latest and devstral-medium-latest\r\n\r\nThe information is available in the models page (source field). Have checked the prices and they match.\r\n\r\nOther mistral models (codestral, magistral, ...) already have the aliases in the database.\r\n\r\nFinal note: I've created #21328 to propose the creation of some support for these aliasing cases. To have to dupe the entries", "score": 2, "created": "2026-02-17T07:23:28Z", "url": "https://github.com/BerriAI/litellm/pull/21372"}, {"source": "GitHub", "text": "Misc. bug: Llama Cpp - Model - Chat Template interactions = Mess (Devstral 2) So I have been successfully running `llama-cpp` together with a quantized version of `Devstral-Small-2` and things are going quite alright. Though I did have to make some adjustments to get here:\n\n1. The default chat template from mistral seems to enforce a user -> assistant -> user -> assistant pattern, this is sub optimal because my main interest in this setup is using `opencode`. But that fails fast, so I have adjus", "score": 8, "created": "2026-02-15T17:26:55Z", "url": "https://github.com/ggml-org/llama.cpp/issues/19647"}, {"source": "GitHub", "text": "Add devstral small 2 (mistral3) support Hello, i want do quantization 3 bit with devstral, but in aqlm it unavaliable", "score": 0, "created": "2026-02-15T11:47:45Z", "url": "https://github.com/Vahe1994/AQLM/issues/185"}, {"source": "GitHub", "text": "Switch to chat completions with role-separated messages for Devstral \u2026 \u2026format compliance\r\n\r\nDevstral was generating well-formed XML in an entirely invented schema (e.g. <action name=\"reflect\">, <params>, CDATA blocks) rather than the Agora protocol. Root cause: the legacy /api/v1/generate endpoint with manual [INST]...[/INST] wrapping doesn't properly apply Mistral's chat template, so the model doesn't treat protocol examples as binding constraints.\r\n\r\nChanges:\r\n- Add generate_chat(system_promp", "score": 0, "created": "2026-02-12T04:23:15Z", "url": "https://github.com/PierreHoule/Agora/pull/7"}, {"source": "GitHub", "text": "feat(syncmodels): smart categorization, replacement recommendations, and badges\n\n- Add `categorizeModel()` function that detects model category from ID/name:\n  coding (coder/devstral/code), reasoning (r1/think/math), fast (flash/mini),\n  or general (fallback)\n- Extract `description`, `supported_parameters` (tools, reasoning) from\n  OpenRouter API during sync\n- Group new models by category in sync picker (\ud83d\udcbb Coding > \ud83e\udde0 Reasoning >\n  \u26a1 Fast > \ud83c\udf10 General) with \ud83d\udd27/\ud83d\udc41\ufe0f/\ud83d\udcad badges\n- Detect replacement recom", "score": 1, "created": "2026-02-11T17:41:33Z", "url": "https://github.com/PetrAnto/moltworker/pull/84"}, {"source": "GitHub", "text": "Add Devstral Small 2 to remaining docs ## Summary\n- Add Devstral Small 2 to the README supported models table\n- Add Devstral Small 2 to the getting-started OpenCode example config\n\nMissed in #7 \u2014 these two spots still referenced the old model list.\n\n\ud83e\udd16 Generated with [Claude Code](https://claude.com/claude-code)", "score": 0, "created": "2026-02-09T02:17:55Z", "url": "https://github.com/filthyrake/filthy-tool-fixer/pull/8"}, {"source": "GitHub", "text": "Add Devstral Small 2 model support ## Summary\n- Add profile and documentation for Mistral's Devstral Small 2 24B, a dense coding agent model purpose-built for agentic tool calling\n- 100% first-attempt tool call accuracy in testing across 40+ messages with zero retries, zero validation failures, zero param repairs\n- Requires `num_ctx = 65536` to prevent wandering on open-ended search tasks (384K native context is too much rope)\n- Bump version to 0.2.0 to match existing git tag\n\n## Changes\n- `prof", "score": 0, "created": "2026-02-09T01:58:44Z", "url": "https://github.com/filthyrake/filthy-tool-fixer/pull/7"}, {"source": "GitHub", "text": "Apply vLLM best practices for Devstral tool calling Add --tokenizer_mode/--config_format/--load_format mistral flags to llm-vllm as prescribed by vLLM docs for native Mistral tokenizer (Tekken). Remove FORCE_NO_STREAM_WITH_TOOLS=false override from rag-vllm so broken streaming is avoided by default. Restore fake_stream_response SSE wrapper for tool calls, and simplify _stream_completion to a plain pass-through now that tool calls no longer flow through it.", "score": 0, "created": "2026-02-07T17:35:23Z", "url": "https://github.com/n0cl0ud/mindfu/pull/1"}, {"source": "GitHub", "text": "fix: use OpenAI-compatible image format for Devstral vision models The Vercel AI Gateway with Mistral expects the OpenAI-compatible format for images: { type: 'image_url', image_url: { url: '...' } }\r\n\r\nThis fixes the 400 Bad Request error \"Invalid input\" on messages.content when sending images to Devstral models through the gateway.\r\n\r\nhttps://claude.ai/code/session_01WLE22WukzHerKY3KSQxcpx\n\n<!-- This is an auto-generated description by cubic. -->\n---\n## Summary by cubic\nFixes image payloads fo", "score": 1, "created": "2026-02-07T15:03:17Z", "url": "https://github.com/Hansade2005/pipilot/pull/189"}, {"source": "GitHub", "text": "fix: use Anthropic provider for Devstral models with images The vercelGateway (OpenAI-compatible) doesn't properly handle image conversion for Mistral models. Now using the @ai-sdk/mistral provider with correct Mistral API model names when images are present:\r\n\r\n- mistral/devstral-2 -> devstral-2512\r\n- mistral/devstral-small-2 -> labs-devstral-small-2512\r\n\r\nThe Mistral provider properly converts Vercel AI SDK image format to Mistral's expected format internally.\r\n\r\nhttps://claude.ai/code/session", "score": 1, "created": "2026-02-07T14:51:32Z", "url": "https://github.com/Hansade2005/pipilot/pull/188"}, {"source": "GitHub", "text": "fix: use Anthropic provider for Devstral models with images - Add getModelWithVision() and requiresAnthropicImageFormat() helpers\r\n- For Devstral with images, use Anthropic provider via gateway\r\n- Convert images to Anthropic format (source object) for Devstral\r\n- This matches how agent-cloud handles vision through the gateway\r\n- Move model initialization after image detection\r\n\r\nThe key fix: Devstral doesn't support images through OpenAI-compatible format, but does work with Anthropic format thr", "score": 1, "created": "2026-02-07T13:48:38Z", "url": "https://github.com/Hansade2005/pipilot/pull/185"}, {"source": "GitHub", "text": "Bug: #1222 broke devstral 123b, outputs nonsense text. ### What happened?\n\nI recently pulled/rebuilt and was surprised to see nonsense commingle out of devstral. I went through commits and before https://github.com/ikawrakow/ik_llama.cpp/commit/b41b8cf813eae5f9d803e49cb220889119a875b8 worked. I went back to head and backed out https://github.com/ikawrakow/ik_llama.cpp/commit/b41b8cf813eae5f9d803e49cb220889119a875b8, now it works again.\n\n### Name and Version\n\nhttps://github.com/ikawrakow/ik_llama", "score": 8, "created": "2026-02-07T13:31:56Z", "url": "https://github.com/ikawrakow/ik_llama.cpp/issues/1248"}, {"source": "GitHub", "text": "Add devstral latest model ", "score": 1, "created": "2026-02-07T08:21:00Z", "url": "https://github.com/Rezkaudi/task-creator/pull/82"}, {"source": "GitHub", "text": "fix: add devstral-small-2 and glm-4.7-flash to model selector The new models were added to lib/ai-models.ts but were not showing in the UI because they were missing from the displayNameMap and allowedModels arrays in the model selector component.\r\n\r\nhttps://claude.ai/code/session_01WLE22WukzHerKY3KSQxcpx\n\n<!-- This is an auto-generated description by cubic. -->\n---\n## Summary by cubic\nFixed missing models in the model selector so new backend additions show in the UI. Added mistral/devstral-small", "score": 1, "created": "2026-02-07T07:41:44Z", "url": "https://github.com/Hansade2005/pipilot/pull/179"}, {"source": "GitHub", "text": "feat: update default AI model and add mistral/devstral-small-2 - Change DEFAULT_CHAT_MODEL from xai/grok-code-fast-1 to mistral/devstral-2\r\n- Add mistral/devstral-small-2 model to available models list\r\n- anthropic/claude-haiku-4.5 was already registered in both files\r\n\r\nhttps://claude.ai/code/session_01WLE22WukzHerKY3KSQxcpx\n\n<!-- This is an auto-generated description by cubic. -->\n---\n## Summary by cubic\nAdds ZAI GLM 4.7 Flash and fixes the initial prompt override to use the correct anthropic/", "score": 1, "created": "2026-02-07T07:27:11Z", "url": "https://github.com/Hansade2005/pipilot/pull/178"}, {"source": "GitHub", "text": "[Bug]: vLLM crashes when trying to load Devstral-2-123B-Instruct-2512 directly from S3 ### Your current environment\n\n<details>\n<summary>The output of <code>python collect_env.py</code></summary>\n\n```text\nCollecting environment information...\n==============================\n        System Info\n==============================\nOS                           : Ubuntu 22.04.5 LTS (x86_64)\nGCC version                  : (Ubuntu 11.4.0-1ubuntu1~22.04.2) 11.4.0\nClang version                : Could not colle", "score": 0, "created": "2026-02-06T20:21:44Z", "url": "https://github.com/vllm-project/vllm/issues/34016"}, {"source": "GitHub", "text": "Error: devstral - Unknown error **Error Details**\n\nModel: devstral\nProvider: ollama\nStatus Code: N/A\n\n**Error Output**\n```\nUnexpected token '<', \"<!DOCTYPE \"... is not valid JSON\n```\n\n**Additional Context**\nManual configuration, the model is working correctly from terminal. The config file is set and read correctly. It was working then suddenly stopped. The chat returns that error. VS Code version 1.109\n\n**config.yaml**\n```\nname: Local Config\nversion: 1.0.0\nschema: v1\nmodels:\n  - name: devstral\n", "score": 1, "created": "2026-02-06T16:43:32Z", "url": "https://github.com/continuedev/continue/issues/10286"}, {"source": "GitHub", "text": "fix: update invalid OpenRouter model IDs\n\n- Replace invalid deepchimera (deepseek-r1t2-chimera) with deepfree (deepseek-r1:free)\n- Replace invalid mimo (xiaomi/mimo-v2) with nemofree (mistral-nemo:free)\n- Fix devstral to use mistralai/devstral-small:free (valid free model)\n- Fix grok to use x-ai/ prefix instead of xai/\n- Fix grokcode to x-ai/grok-code-fast-1\n- Fix flash to google/gemini-3-flash-preview\n- Fix geminipro to google/gemini-3-pro-preview\n- Fix mistrallarge to mistralai/mistral-large-2", "score": 0, "created": "2026-02-06T01:42:33Z", "url": "https://github.com/PetrAnto/moltworker/pull/39"}, {"source": "GitHub", "text": "feat: add new AI models (Claude Opus 4.6, Grok 4.1, Devstral Small, K\u2026 \u2026imi K2.5)\r\n\r\n- Anthropic: Add Claude Opus 4.6 as new flagship, deprecate Opus 4.5\r\n- xAI: Add Grok 4.1 Fast (reasoning + non-reasoning) with 2M context\r\n- Mistral: Add Devstral Small 2 (24B coding model)\r\n- Moonshot: Add Kimi K2.5 (1T MoE), update as default\r\n- Update mention mappings and alias resolvers\n\n<!-- This is an auto-generated comment: release notes by coderabbit.ai -->\n\n## Summary by CodeRabbit\n\n## Release Notes\n\n*", "score": 1, "created": "2026-02-06T01:04:07Z", "url": "https://github.com/theted/vibe-chat/pull/197"}, {"source": "GitHub", "text": "DEVSTRAL: add UNET traffic monitoring with Selenium add UNET traffic monitoring with Selenium\n\n<!-- This is an auto-generated comment: release notes by coderabbit.ai -->\n\n## Summary by CodeRabbit\n\n* **New Features**\n  * Added UNET traffic monitoring and automated data collection capabilities\n  * Traffic information is now integrated and available in the dashboard context\n\n* **Dependencies**\n  * Added Selenium library for web-based traffic data retrieval\n\n<!-- end of auto-generated comment: relea", "score": 1, "created": "2026-02-05T18:31:57Z", "url": "https://github.com/manti-by/odin/pull/4"}, {"source": "GitHub", "text": "Remove Devstral 2 2512 from free models list ## Summary\nRemove Devstral 2 2512 (mistralai/devstral-2512:free) from the list of free models.\n\n## Changes\n- Remove `devstral_2512_free_model` from `kiloFreeModels` array in `src/lib/models.ts`\n- Remove `devstral_2512_free_model` export from `src/lib/providers/mistral.ts`\n- Update `devstral_small_2512_free_model` to be a standalone definition (no longer spreads from the removed model)\n- Remove `mistralai/devstral-2512:free` mapping from `vercelModelId", "score": 2, "created": "2026-02-04T23:00:58Z", "url": "https://github.com/Kilo-Org/cloud/pull/21"}, {"source": "GitHub", "text": "feat: integrate correct system prompt into devstral ", "score": 3, "created": "2026-02-04T22:22:24Z", "url": "https://github.com/trymirai/lalamo/pull/132"}, {"source": "GitHub", "text": "fix: update model IDs and fix image generation endpoint\n\nModel ID corrections based on OpenRouter verification:\n- fluxpro: black-forest-labs/flux-pro\n- fluxmax: black-forest-labs/flux-max\n- deepchimera: deepseek/deepseek-r1t2-chimera:free\n- mimo: xiaomi/mimo-v2:free\n- devstral: mistralai/devstral\n- deep: deepseek/deepseek-chat-v3\n- deepreason: deepseek/deepseek-r1\n- mistrallarge: mistralai/mistral-large-3\n- flash: google/gemini-3-flash\n- geminipro: google/gemini-3-pro\n- grokcode: xai/grok-code-f", "score": 0, "created": "2026-02-04T01:59:33Z", "url": "https://github.com/PetrAnto/moltworker/pull/14"}, {"source": "GitHub", "text": "bug: Devstral 2 vs Small - Subagent Stall ### Component\n\nCLI\n\n### Summary\n\n### Summary\nDevstral 2 stalls on subagent delegation while Devstral Small completes the same task.\n\n### Reproduction\n- Task: Full codebase exploration\n- **Devstral 2**: Delegates to subagent, outputs \"let's wait 5-10 minutes\", then stalls\n- **Devstral Small**: Completes successfully, no stall\n\n### Question\nDuring the \"wait 5-10 minutes\" stall period:\n- Are there active API calls (token consumption)?\n- Or is the agent trul", "score": 0, "created": "2026-02-03T14:45:31Z", "url": "https://github.com/mistralai/mistral-vibe/issues/299"}, {"source": "GitHub", "text": "Problem with mistral vibe and devstral small model Thank you for adding support for tools to vllm-mlx with:\n`--tool-call-parser foo` and `--enable-auto-tool-choice`\n\nI am trying to run Mistral vibe tool and use 'Devstral-Small-2-24B-Instruct-2512-4bit' mlx model served by vllm-mlx like this:\n```\nPython-3.12.12-arm64/bin/python3 Python-3.12.12-arm64/bin/vllm-mlx serve --host 127.0.0.1 --port 6600 --tool-call-parser mistral --enable-auto-tool-choice /path/to/Devstral-Small-2-24B-Instruct-2512-4bit", "score": 1, "created": "2026-02-02T18:47:11Z", "url": "https://github.com/waybarrios/vllm-mlx/issues/29"}, {"source": "GitHub", "text": "Change default models from gemma to ministral and devstral ", "score": 0, "created": "2026-02-02T17:51:55Z", "url": "https://github.com/rashomon-gh/prog-search/issues/5"}, {"source": "GitHub", "text": "Benchmark mistralai/devstral-2512 Adds benchmark for `mistralai/devstral-2512`", "score": 1, "created": "2026-01-31T20:15:04Z", "url": "https://github.com/allenporter/home-assistant-datasets/pull/254"}, {"source": "GitHub", "text": "Benchmark: mistralai/devstral-2512 (OpenRouter) ### Model slug (OpenRouter)\n\nmistralai/devstral-2512\n\n### Notes (optional)\n\n_No response_", "score": 1, "created": "2026-01-31T19:47:41Z", "url": "https://github.com/victorigualada/openrouter-benchmarks/issues/9"}, {"source": "GitHub", "text": "fix(mistral): add devstral models with correct 262K context length ## Summary\nFixes #6185\n\n## Problem\nDevstral models (`devstral-2512` and `devstral-small-2505`) were missing from the Mistral provider's declarative model list, causing Goose to fall back to the default 128K context limit instead of the actual 262144 tokens supported by these models.\n\n## Solution\nAdded both devstral models to `mistral.json` with their correct `context_limit` values (262144) as confirmed by the Mistral API:\n\n```bas", "score": 1, "created": "2026-01-31T10:57:03Z", "url": "https://github.com/block/goose/pull/6867"}, {"source": "GitHub", "text": "fix: add devstral model context limit (262K) ## Summary\nAdds `devstral` to `MODEL_SPECIFIC_LIMITS` with the correct 262,144 token context limit.\n\n## Problem\nDevstral models (e.g., `devstral-2512`) show 128K context limit in Goose instead of the documented 262K.\n\nBoth Mistral and OpenRouter APIs confirm the correct limit:\n```json\n{\n  \"id\": \"devstral-2512\",\n  \"max_context_length\": 262144\n}\n```\n\n## Solution\nSimple one-line addition following the existing pattern in `MODEL_SPECIFIC_LIMITS`:\n```rust\n", "score": 4, "created": "2026-01-31T09:46:41Z", "url": "https://github.com/block/goose/pull/6866"}, {"source": "GitHub", "text": "fix: ensure the mistral ordering fixes also apply to devstral ", "score": 0, "created": "2026-01-31T05:09:37Z", "url": "https://github.com/anomalyco/opencode/pull/11412"}]}