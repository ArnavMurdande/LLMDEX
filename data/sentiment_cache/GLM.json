{"timestamp": 1771962984.392248, "mentions": [{"source": "Reddit", "text": "GLM 4.7 Flash Setup Running Local AI Faster Than Paid Cloud Models GLM 4.7 Flash setup gives you a powerful local AI model that runs entirely on your device.\n\nRunning the whole system for free feels like unlocking paid-level performance without subscriptions.\n\nNothing relies on the cloud once the GLM 4.7 Flash setup is complete.\n\nWatch the video below:\n\n[https://www.youtube.com/watch?v=UoTn-O9WPoc&amp;t=1s](https://www.youtube.com/watch?v=UoTn-O9WPoc&amp;t=1s)\n\nWant to make money and save time w", "score": 1, "created": 1771962297.0, "url": "https://reddit.com/r/AISEOInsider/comments/1rdqgml/glm_47_flash_setup_running_local_ai_faster_than/"}, {"source": "Reddit", "text": "Best reasoning model Rx 9070xt 16 GB vram Title basically says it. Im looking for a model to run Plan mode in Cline, I used to use GLM 5.0, but the costs are running up and as a student the cost is simply a bit too much for me right now. I have a Ryzen 7 7700, 32 gb DDR5 ram. I need something with strong reasoning, perhaps coding knowledge is required although I wont let it code. Purely Planning. Any recommendations? I have an old 1660 ti lying around maybe i can add that for extra vram, if amd ", "score": 2, "created": 1771960446.0, "url": "https://reddit.com/r/LocalLLaMA/comments/1rdplki/best_reasoning_model_rx_9070xt_16_gb_vram/"}, {"source": "Reddit", "text": "Open vs Closed Source SOTA - Benchmark overview Sonnet 4.5 was released about 6 months ago. What's the advantage of the closed source labs? About that amount of time? Even less?\n\n|Benchmark|GPT-5.2|Opus 4.6|Opus 4.5|Sonnet 4.6|Sonnet 4.5|Q3.5 397B-A17B|Q3.5 122B-A10B|Q3.5 35B-A3B|Q3.5 27B|GLM-5|\n|:-|:-|:-|:-|:-|:-|:-|:-|:-|:-|:-|\n|Release date|Dec 2025|Feb 2026|Nov 2025|Feb 2026|Nov 2025|Feb 2026|Feb 2026|Feb 2026|Feb 2026|Feb 2026|\n|**Reasoning &amp; STEM**|||||||||||\n|GPQA Diamond|93.2|91.3|87", "score": 5, "created": 1771960118.0, "url": "https://reddit.com/r/LocalLLaMA/comments/1rdpfy6/open_vs_closed_source_sota_benchmark_overview/"}, {"source": "Reddit", "text": "Frontier LLM Leaderboard Check it out at [https://www.onyx.app/llm-leaderboard](https://www.onyx.app/llm-leaderboard)", "score": 0, "created": 1771959166.0, "url": "https://reddit.com/r/OpenAI/comments/1rdozjs/frontier_llm_leaderboard/"}, {"source": "Reddit", "text": "Local model users! Which model arch do you use? To clarify, the arch is the base the model you use is trained off of. So Cydonia would be mistral.\n\n1. Mistral\n\n2. Nemo\n\n3. GLM\n\n4. Qwen\n\n5. GPT oss\ud83d\udc80 \n\n6. Gemma\n\n7. LFM?\n\n8. Other\n\nThis is not a \u201cbest model\u201d post, I just want to know what y\u2019all use.", "score": 3, "created": 1771956881.0, "url": "https://reddit.com/r/SillyTavernAI/comments/1rdnwtu/local_model_users_which_model_arch_do_you_use/"}, {"source": "Reddit", "text": "GLM 4.7 Flash OpenClaw Is the Free AI Stack Nobody Is Using GLM 4.7 Flash OpenClaw is the simplest way to run a powerful AI agent without paying for tokens.\n\nIt lets you build, automate, and test without worrying about usage caps.\n\nIt runs locally on your own machine and gives you full control.\n\nWatch the video below:\n\n[https://www.youtube.com/watch?v=Z\\_X3TUplCis&amp;t=7s](https://www.youtube.com/watch?v=Z_X3TUplCis&amp;t=7s)\n\nWant to make money and save time with AI? Get AI Coaching, Support &", "score": 1, "created": 1771955537.0, "url": "https://reddit.com/r/AISEOInsider/comments/1rdn9n8/glm_47_flash_openclaw_is_the_free_ai_stack_nobody/"}, {"source": "Reddit", "text": "my go to tools for content and coding thought I'd share my current toolkit. I've decided to move away from depending on just one AI provider. now, I mainly rely on Blackbox AI for its unlimited free access for some of the models.\n\nI\u2019ve been using Minimax M2.5 for coming up with ideas and creative content, while Kimi K2.5 handles my research needs. It\u2019s super handy for gathering competitor insights. GLM-5 is my helper for Python scripts and automation tasks.\n\nMost of my initial drafts come from t", "score": 2, "created": 1771954685.0, "url": "https://reddit.com/r/AIToolMadeEasy/comments/1rdmv1v/my_go_to_tools_for_content_and_coding/"}, {"source": "Reddit", "text": "Referral link? I've been using the coding plan and more (e.g. openclaw) for a while and am looking at extending it with the Chinese Spring Festival deal.\n\nIf anyone needs:\n\n\ud83d\ude80 You\u2019ve been invited to join the GLM Coding Plan! Enjoy full support for Claude Code, Cline, and 20+ top coding tools \u2014 starting at just $10/month. Subscribe now and grab the limited-time deal! Link\uff1a [https://z.ai/subscribe?ic=Q2PCSAQJB9](https://z.ai/subscribe?ic=Q2PCSAQJB9)", "score": 0, "created": 1771954340.0, "url": "https://reddit.com/r/ZaiGLM/comments/1rdmp7q/referral_link/"}, {"source": "Reddit", "text": "\"Agentic Gaming\" \u2014 a deep dive into how I'm using LLMs as a semantic reasoning layer inside an RPG engine (80+ orchestrated AI tasks, multi-LLM, genre-agnostic skills, and a lot of dice rolls) Hi everyone!\n\nI've been working on something for a while now that I think sits squarely in the intersection of this sub's interests, and I wanted to share it \u2014 not just as a project showcase, but because I genuinely want to discuss the underlying design concepts with people who think about AI + game design", "score": 3, "created": 1771954177.0, "url": "https://reddit.com/r/aigamedev/comments/1rdmmfa/agentic_gaming_a_deep_dive_into_how_im_using_llms/"}, {"source": "Reddit", "text": "Kilo - Code with GLM Models in Kilo Code free ", "score": 1, "created": 1771953336.0, "url": "https://reddit.com/r/free_llm_api_chat/comments/1rdm8j0/kilo_code_with_glm_models_in_kilo_code_free/"}, {"source": "Reddit", "text": "[Q] Best binary model for small sample size (n = 45)? I'm studying which environmental variables affect the presence of a rare species across rivers. The problem is that the species is very rare, so my sample size is small (n = 45 rivers). The dependent variable is binary (presence/absence), and the independent variables are continuous environmental variables (e.g., temperature metrics, altitude, etc.).\n\nGiven the small sample size, would a GLM with binomial family be the best option? maybe is t", "score": 1, "created": 1771953075.0, "url": "https://reddit.com/r/statistics/comments/1rdm3yq/q_best_binary_model_for_small_sample_size_n_45/"}, {"source": "Reddit", "text": "I gave 16 LLMs a food truck in Austin for 30 days. Gemini 3 Pro matched Sonnet 4.6 \u2014 5\u00d7 cheaper. I built an agentic benchmark called FoodTruck Bench \u2014 AI models manage a food truck in Austin, TX for 30 days. Location strategy, menu pricing, inventory management, staff hiring \u2014 34 tools, deterministic simulation, 5 runs per model. Same seed, same conditions. 16 models tested so far.\n\n**Gemini 3 Pro is the most efficient model on the entire benchmark.** It reaches +760% ROI \u2014 nearly identical to S", "score": 17, "created": 1771952987.0, "url": "https://reddit.com/r/GeminiAI/comments/1rdm2g1/i_gave_16_llms_a_food_truck_in_austin_for_30_days/"}, {"source": "Reddit", "text": "I am considering cancelling my $20/mo AI subs because one service is asking for just $2 for the first month. I was paying $20/mo for different SOTA models, and it was adding up fast. Im not exactly having money falling out of my pockets.\u00a0\n\nThen I saw Blackbox AI\u2019s promo, $1 for the first month of PRO gets you $20 in credits usable on:\n\n\u2022 Claude Opus 4.6, GPT-5.2, Gemini 3, Grok 4, and a bunch more\n\n\u2022 plus unlimited free requests on Minimax M2.5, GLM-5, and Kimi K2.5.\n\nI\u2019ve been using the unlimit", "score": 1, "created": 1771952054.0, "url": "https://reddit.com/r/Students/comments/1rdlm9a/i_am_considering_cancelling_my_20mo_ai_subs/"}, {"source": "Reddit", "text": "M3 Ultra 512GB - real-world performance of MiniMax-M2.5, GLM-5, and Qwen3-Coder-Next A lot of people have been asking about real-world performance of recent models on apple silicon, especially on the ultra chips. I've been running MiniMax-M2.5, GLM-5, and Qwen3-Coder-80B on my M3 Ultra 512GB and wanted to share the results.\n\n**Quick summary**\n\n**Qwen3-Coder-Next-80B** \\- the standout for local coding. i've been using it as a backend for Claude Code, and it honestly performs at a level comparable", "score": 21, "created": 1771950689.0, "url": "https://reddit.com/r/LocalLLaMA/comments/1rdkze3/m3_ultra_512gb_realworld_performance_of/"}, {"source": "Reddit", "text": "GLM-5 tech report dropped, they're explicitly framing it as \"agentic engineering\" replacing vibe coding Went through the GLM-5 technical report today (https://arxiv.org/pdf/2602.15763). zhipu is positioning this as the model that pushes coding from vibe coding (you prompt, ai writes) to agentic engineering (ai plans, implements, iterates on its own). bold framing but the numbers back it up somewhat.\n\nSwe-bench verified they're competitive with claude opus 4.5, beating gemini 3 pro. lmarena code ", "score": 0, "created": 1771950034.0, "url": "https://reddit.com/r/Verdent/comments/1rdkoi9/glm5_tech_report_dropped_theyre_explicitly/"}, {"source": "Reddit", "text": "New SWE-bench Multilingual Leaderboard: Performance across 9 languages &amp; cost analysis Happy to announce that we just launched our Multilingual leaderboard comparing performance across 9 languages. The benchmark is harder than SWE-bench verified and still shows a wider range of performances.\n\nWe're still adding more models, but this is the current leaderboard:\n\nhttps://preview.redd.it/l0cotc22wglg1.png?width=4752&amp;format=png&amp;auto=webp&amp;s=b7b862332cdb8843100d9919db30accb1bc0c260\n\nIn", "score": 12, "created": 1771950003.0, "url": "https://reddit.com/r/LocalLLaMA/comments/1rdknyh/new_swebench_multilingual_leaderboard_performance/"}, {"source": "Reddit", "text": "GPT 5.2 Pro + Claude Opus 4.6 + Gemini 3.1 Pro For Just $5/Month (With API Access &amp; Agents) **Hey Everybody,**\n\nFor the Vibecoding crowd \u2014 InfiniaxAI just doubled Starter plan rate limits and unlocked high-limit access to Claude 4.6 Opus, GPT 5.2 Pro, and Gemini 3.1 Pro for just $5/month.\n\nHere\u2019s what the Starter plan includes:\n\n* $5 in platform credits\n* Access to 120+ AI models including Opus 4.6, GPT 5.2 Pro, Gemini 3 Pro &amp; Flash, GLM-5, and more\n* Agentic Projects system to build app", "score": 0, "created": 1771947985.0, "url": "https://reddit.com/r/MistralAI/comments/1rdjqkz/gpt_52_pro_claude_opus_46_gemini_31_pro_for_just/"}, {"source": "Reddit", "text": "Pick my poison So originally I was using Cursor but recently picked up z.ai coding plan and using GLM with Claude Code, my only issue is I do a lot of reverse engineering, and Claude Code always hits me with the \u201cI cannot participate with this bla bla, do you have the rights to do this bla bla\u201d whereas on Cursor he just takes the wheel and away he goes, anyone have any tips or which IDE/Provider to use that is less \u201crestrictive\u201d ? \ud83e\udd20", "score": 1, "created": 1771946797.0, "url": "https://reddit.com/r/vibecoding/comments/1rdj75i/pick_my_poison/"}, {"source": "Reddit", "text": "Claude Opus 4.6 + Gemini 3.1 Pro + GPT 5.2 Pro For Just $5/Month (With API Access &amp; Agents) **Hey Everybody,**\n\nFor the Vibecoding crowd \u2014 InfiniaxAI just doubled Starter plan rate limits and unlocked high-limit access to Claude 4.6 Opus, GPT 5.2 Pro, and Gemini 3.1 Pro for just $5/month.\n\nHere\u2019s what the Starter plan includes:\n\n* $5 in platform credits\n* Access to 120+ AI models including Opus 4.6, GPT 5.2 Pro, Gemini 3 Pro &amp; Flash, GLM-5, and more\n* Agentic Projects system to build app", "score": 0, "created": 1771941452.0, "url": "https://reddit.com/r/microsaas/comments/1rdgx00/claude_opus_46_gemini_31_pro_gpt_52_pro_for_just/"}, {"source": "Reddit", "text": "GPT 5.2 Pro + Claude Opus 4.6 + Gemini 3.1 Pro For Just $5/Month (With API Access) **Hey Everybody,**\n\nFor the Vibecoding crowd \u2014 InfiniaxAI just doubled Starter plan rate limits and unlocked high-limit access to Claude 4.6 Opus, GPT 5.2 Pro, and Gemini 3.1 Pro for just $5/month.\n\nHere\u2019s what the Starter plan includes:\n\n* $5 in platform credits\n* Access to 120+ AI models including Opus 4.6, GPT 5.2 Pro, Gemini 3 Pro &amp; Flash, GLM-5, and more\n* Agentic Projects system to build apps, games, sit", "score": 0, "created": 1771941420.0, "url": "https://reddit.com/r/micro_saas/comments/1rdgwj9/gpt_52_pro_claude_opus_46_gemini_31_pro_for_just/"}, {"source": "Reddit", "text": "Claude Opus &amp; Sonnet 4.6 + GPT 5.2 Pro + Gemini 3.1 Pro For Just $5/Month (With API Access &amp; Agents) **Hey Everybody,**\n\nFor the Vibecoding crowd \u2014 InfiniaxAI just doubled Starter plan rate limits and unlocked high-limit access to Claude 4.6 Opus, GPT 5.2 Pro, and Gemini 3.1 Pro for just $5/month.\n\nHere\u2019s what the Starter plan includes:\n\n* $5 in platform credits\n* Access to 120+ AI models including Opus 4.6, GPT 5.2 Pro, Gemini 3 Pro &amp; Flash, GLM-5, and more\n* Agentic Projects system", "score": 0, "created": 1771941387.0, "url": "https://reddit.com/r/ClaudeCode/comments/1rdgw30/claude_opus_sonnet_46_gpt_52_pro_gemini_31_pro/"}, {"source": "Reddit", "text": "Gemini 3.1 Pro + Veo 3.1 + Claude Opus 4.6 + GPT 5.2 Pro For Just $5/Month (With API Access) **Hey Everybody,**\n\nFor all the Gemini users out there, we are doubling InfiniaxAI Starter plans rate limits + Making Claude 4.6 Opus &amp; GPT 5.2 Pro &amp; Gemini 3.1 Pro available with high rate limits for just $5/Month!\n\nHere are some of the features you get with the Starter Plan:\n\n\\- $5 In Credits To Use The Platform\n\n\\- Access To Over 120 AI Models Including Opus 4.6, GPT 5.2 Pro, Gemini 3 Pro &amp", "score": 0, "created": 1771941214.0, "url": "https://reddit.com/r/Bard/comments/1rdgtn5/gemini_31_pro_veo_31_claude_opus_46_gpt_52_pro/"}, {"source": "Reddit", "text": "Built this dashboard with opentelemetry to monitor openclaw token usage Hola folks, i work at signoz(it's an open source observability tool)...have been playing with openclaw in my free time. My token usage was hitting limits very quickly. So I started exploring how we can monitor it easily. You can check the steps to create the above dashboard here: [https://signoz.io/blog/monitoring-openclaw-with-opentelemetry/](https://signoz.io/blog/monitoring-openclaw-with-opentelemetry/) ", "score": 0, "created": 1771941175.0, "url": "https://reddit.com/r/codex/comments/1rdgt37/built_this_dashboard_with_opentelemetry_to/"}, {"source": "Reddit", "text": "ive had it with this quota thingy what i need to do ??? ne,\n\nI\u2019m currently building some projects using tools like AntiGravity, OpenCode, and OpenClaw, but I\u2019m hitting a wall with my setup and could really use some advice from the community.\n\nMy goal is to have a strong, reliable model (like Claude Opus 4.6 or Sonnet 4.5) as my daily driver for the long term. The problem is, I\u2019m constantly running into rate limits and context window issues.\n\nHere is my dilemma:\n\n1. **Free Tiers/Basic Plans:** I ", "score": 3, "created": 1771938729.0, "url": "https://reddit.com/r/google_antigravity/comments/1rdfv8x/ive_had_it_with_this_quota_thingy_what_i_need_to/"}, {"source": "Reddit", "text": "Is GLM / Z.AI down right now? I keep getting socket hang up error while using Coding Plan. The API platform also says \"No response\". I just want to know if others are experiencing this right now too or if it's just me.", "score": 12, "created": 1771926295.0, "url": "https://reddit.com/r/SillyTavernAI/comments/1rdby71/is_glm_zai_down_right_now_i_keep_getting_socket/"}, {"source": "Reddit", "text": "GLM 5 vs Kimi K2.5: Which Open Model Actually Wins? GLM 5 vs Kimi K2.5 is the comparison that should be dominating discussions right now.\n\nMost people are still arguing about proprietary models, while GLM 5 vs Kimi K2.5 has quietly pushed open-weight AI into serious frontier territory.\n\nIf you are building real systems instead of chasing hype cycles, this is the decision that actually affects your stack.\n\nWatch the video below:\n\n[https://www.youtube.com/watch?v=C37CTjCTYn4](https://www.youtube.c", "score": 2, "created": 1771919061.0, "url": "https://reddit.com/r/AISEOInsider/comments/1rda1qo/glm_5_vs_kimi_k25_which_open_model_actually_wins/"}, {"source": "Reddit", "text": "Will I get a good package job in 2026 my placements start from sept- dec ", "score": 3, "created": 1771916747.0, "url": "https://reddit.com/r/Free_Vedic_Astro/comments/1rd9exo/will_i_get_a_good_package_job_in_2026_my/"}, {"source": "Reddit", "text": "my stack for content and code generation (mostly free) wanted to share my current setup. i've moved away from relying solely on one provider. currently using blackbox ai as my main hub primarily for the unlimited free access to,\n\nminimax m2.5, exclusively for creative copy and brainstorming ideas. kimi k2.5, my research analyst. i feed it competitor docs and reports and glm-5, my assistant for python scripts and automation.\n\ni still use claude for the final polish, but 90 per cent of the draft w", "score": 6, "created": 1771915413.0, "url": "https://reddit.com/r/VibeCodeDevs/comments/1rd91ef/my_stack_for_content_and_code_generation_mostly/"}, {"source": "Reddit", "text": "We're heading into the dark ages in terms of free proxies aren't we? (Openrouter) First tngtech gets rid of all their free deepseek chimera LLMs because assholes were abusing it, and now Deepseek is getting rid of their free version of R1 on the 24th. Pony Alpha blessed us with their presence for a measly few days just to ruin us for everyone else. I miss the glory days when we had Deepseek V3.1 Nex, Mimo V2 flash, and there was another one that I can't remember the name of. They were all so goo", "score": 0, "created": 1771911334.0, "url": "https://reddit.com/r/SillyTavernAI/comments/1rd7rmf/were_heading_into_the_dark_ages_in_terms_of_free/"}, {"source": "Reddit", "text": "When a company wants to give you a free trial but not really free I saw this and had to double check the pricing.\n\nBlackbox AI is offering its PRO plan for $2 for the first month. Not free. But cheap enough that you don\u2019t really hesitate.\n\nHere\u2019s what you get:\n\n$20 in credits for Claude Opus 4.6, GPT-5.2, Gemini 3, Grok 4, and 400+ models\n\nUnlimited requests on Minimax M2.5, GLM-5, Kimi K2.5\n\nAccess to chat, image, and video models\n\nIt\u2019s basically a paid free trial.\n\nYou put down $2, so you feel", "score": 2, "created": 1771910330.0, "url": "https://reddit.com/r/OnlyAICoding/comments/1rd7fv0/when_a_company_wants_to_give_you_a_free_trial_but/"}, {"source": "Reddit", "text": "I tried glm-5-free on Zen and exceeded my quota in 11 prompts These were tiny prompts, meant to just test that the integration was working. I successfully got through 11 such prompts before I got \"API RATE LIMIT REACHED\". Definitely not enough time to get an idea of how well the model performed. Maybe I'll try the paid model down the road, but back to another free or known-performance model for now.", "score": 1, "created": 1771903620.0, "url": "https://reddit.com/r/opencodeCLI/comments/1rd3n2x/i_tried_glm5free_on_zen_and_exceeded_my_quota_in/"}, {"source": "Reddit", "text": "Claude Opus 4.6 + GPT 5.2 Pro + Gemini 3.1 Pro For Just $5/Month (With API Access) **Hey Everybody,**\n\nFor all the users out there, we are doubling InfiniaxAI Starter plans rate limits + Making Claude 4.6 Opus &amp; GPT 5.2 Pro &amp; Gemini 3.1 Pro available with high rate limits for just $5/Month!\n\nHere are some of the features you get with the Starter Plan:\n\n\\- $5 In Credits To Use The Platform\n\n\\- Access To Over 120 AI Models Including Opus 4.6, GPT 5.2 Pro, Gemini 3 Pro &amp; Flash, GLM 5, E", "score": 1, "created": 1771903174.0, "url": "https://reddit.com/r/GenAI4all/comments/1rd3h70/claude_opus_46_gpt_52_pro_gemini_31_pro_for_just/"}, {"source": "Reddit", "text": "Is it always this slow?  Using Kimi k2.5, GLM-5, or Minimax 2.5 is practically impossible; Qwen models run at decent speeds randomly, which is frustrating. Is there any way to achieve a good speed?\n\ntnks ", "score": 12, "created": 1771898622.0, "url": "https://reddit.com/r/chutesAI/comments/1rd1sbu/is_it_always_this_slow/"}, {"source": "Reddit", "text": "Claude Opus 4.6 + Gemini 3.1 Pro + GPT 5.2 Pro For Just $5/Month **Hey Everybody,**\n\nFor all the Claude users out there, we are doubling InfiniaxAI Starter plans rate limits + Making Claude 4.6 Opus &amp; GPT 5.2 Pro &amp; Gemini 3.1 Pro available with high rate limits for just $5/Month!\n\nHere are some of the features you get with the Starter Plan:\n\n\\- $5 In Credits To Use The Platform\n\n\\- Access To Over 120 AI Models Including Opus 4.6, GPT 5.2 Pro, Gemini 3 Pro &amp; Flash, GLM 5, Etc\n\n\\- Acce", "score": 0, "created": 1771894560.0, "url": "https://reddit.com/r/Anthropic/comments/1rczy79/claude_opus_46_gemini_31_pro_gpt_52_pro_for_just/"}, {"source": "Reddit", "text": "A Guide: Companion That Can Handle Text Based Tools and Archiving Ok I posted this as a question a day or two ago [https://www.reddit.com/r/huggingface/comments/1rblwxl/companion\\_that\\_can\\_handle\\_text\\_based\\_tools\\_and/](https://www.reddit.com/r/huggingface/comments/1rblwxl/companion_that_can_handle_text_based_tools_and/)\n\n  \nAnd I didn't get a lot of feedback so I'm going to share what I've found that works pretty well for my use case.  Not perfectly mind you, but good enough that I can liv", "score": 2, "created": 1771890789.0, "url": "https://reddit.com/r/huggingface/comments/1rcyazu/a_guide_companion_that_can_handle_text_based/"}, {"source": "Reddit", "text": "How to break the trauma-resolution loop in role play sessions? After many years of running **SillyTavern** locally with small models mostly for romantic RP (wink, wink), I decided for the first time to use an API with a paid subscription. After seeing everyone talking about **GLM-5** I subscribed to **NanoGPT** and I've been using exclusively that model for a couple of weeks.\n\n  \nI was blown away, the creativity, the details, how it really adheres to the card given, the context size. I felt like", "score": 33, "created": 1771890776.0, "url": "https://reddit.com/r/SillyTavernAI/comments/1rcyasp/how_to_break_the_traumaresolution_loop_in_role/"}, {"source": "Reddit", "text": "How Do Backends Like Ollama, LMStudio, etc. Adapt to All The Different Chat Templates of The Various Models They Support? Same as Title, I go through the chat templates of different small local models (GLM-4.7-Flash, Nanbeige-4.1-3b, GPT-OSS-20B, etc.) and see that all of them have different chat templates and formats. I am trying to use mlx-lm to run these models and parse the response into reasoning and content blocks but the change in format always stumps me and the mlx-lm's inbuilt reasoning", "score": 6, "created": 1771889488.0, "url": "https://reddit.com/r/LocalLLaMA/comments/1rcxrs4/how_do_backends_like_ollama_lmstudio_etc_adapt_to/"}, {"source": "Reddit", "text": "A Deep Analysis of the \u201cBest R&amp;B Vocal Performance by a Duo or Group\u201d Grammy Award Recipients and Nominees Here are all of the winners in the Grammy category of \"Best R&amp;B Vocal Performance by a Duo or Group\" along with the nominees that they beat out in each respective year. This will be a follow-up to my previous analyses of the soloist categories for [male artists](https://www.reddit.com/r/rnb/s/AmXvXRtJgO) and [female artists](https://www.reddit.com/r/rnb/s/0UNqCfh9ow). In this catego", "score": 2, "created": 1771889356.0, "url": "https://reddit.com/r/rnb/comments/1rcxps8/a_deep_analysis_of_the_best_rb_vocal_performance/"}, {"source": "Reddit", "text": "GPT 5.2 Pro + Claude Opus 4.6 + Gemini 3.1 Pro For Just $5/Month (with API Access to run locally) **Hey Everybody,**\n\nFor all the AI users out there, we are doubling InfiniaxAI Starter plans rate limits + Making Claude 4.6 Opus &amp; GPT 5.2 Pro &amp; Gemini 3.1 Pro available with high rate limits for just $5/Month!\n\nHere are some of the features you get with the Starter Plan:\n\n\\- $5 In Credits To Use The Platform\n\n\\- Access To Over 120 AI Models Including Opus 4.6, GPT 5.2 Pro, Gemini 3 Pro &am", "score": 0, "created": 1771888979.0, "url": "https://reddit.com/r/LocalLLM/comments/1rcxk8m/gpt_52_pro_claude_opus_46_gemini_31_pro_for_just/"}, {"source": "Reddit", "text": "GPT 5.2 Pro + Gemini 3.1 Pro + Claude Opus 4.6 For Just $5/Month (With API Access) **Hey Everybody,**\n\nFor all the AI users out there, we are doubling InfiniaxAI Starter plans rate limits + Making Claude 4.6 Opus &amp; GPT 5.2 Pro &amp; Gemini 3.1 Pro available with high rate limits for just $5/Month!\n\nHere are some of the features you get with the Starter Plan:\n\n\\- $5 In Credits To Use The Platform\n\n\\- Access To Over 120 AI Models Including Opus 4.6, GPT 5.2 Pro, Gemini 3 Pro &amp; Flash, GLM 5", "score": 0, "created": 1771888878.0, "url": "https://reddit.com/r/LLM/comments/1rcxirk/gpt_52_pro_gemini_31_pro_claude_opus_46_for_just/"}, {"source": "Reddit", "text": "glm-5 spazzing out again feels like its been getting worse lately", "score": 5, "created": 1771884279.0, "url": "https://reddit.com/r/ZaiGLM/comments/1rcvilx/glm5_spazzing_out_again/"}, {"source": "Reddit", "text": "The AI Model War Is Accelerating \ud83c\udf0f\ud83e\udd2f This video shows how GLM, Kimi and MiniMax are pushing AI forward fast.\n\nBetter reasoning.  \nLower cost.  \nStronger agent capabilities.\n\nThat means builders now have more options to create workflows and products without relying on one ecosystem.\n\n\ud83c\udfa5 Watch the full breakdown  \n\ud83d\udc49 Learn how people use multi-model stacks: [https://crdigitalmarketer.com/b/AIPBRD](https://crdigitalmarketer.com/b/AIPBRD)\n\nhttps://reddit.com/link/1rcuy3f/video/6twy2k2odblg1/player\n\n", "score": 1, "created": 1771883022.0, "url": "https://reddit.com/r/u_Worldly-Stage1445/comments/1rcuy3f/the_ai_model_war_is_accelerating/"}, {"source": "Reddit", "text": "\ud83c\udd95 New Tavern Models Added! Hey everyone! \ud83d\udc4b\n\nWe\u2019ve added several new Tavern models \u2014 check out the details and pricing below:\n\n# GLM 5\n\n**Usage Cost (Per Message)**\n\n* **Default &amp; Plus**: 15 unpaid/paid YoBeans\n* **Pro**: 14 unpaid/paid YoBeans\n* **Premium**: 12 unpaid/paid YoBeans\n* **Platinum**: 12 unpaid/paid YoBeans\n\n# Claude Sonnet 4.6\n\n**Usage Cost (Per Message)**\n\n* **Default &amp; Plus**: 18 paid YoBeans\n* **Pro**: 18 paid YoBeans\n* **Premium**: 17 paid YoBeans\n* **Platinum**: 17 paid", "score": 2, "created": 1771882573.0, "url": "https://reddit.com/r/YodayoTavern/comments/1rcuquz/new_tavern_models_added/"}, {"source": "Reddit", "text": "GLM-4.7-flash isn\u2019t editing files with openclaw Recently tried to install openclaw and have used a few different models and the all seem to have the same issue for me which is that the don\u2019t change or edit any of the workspace files? Has anyone else had this issue and how have they resolved it? ", "score": 0, "created": 1771881818.0, "url": "https://reddit.com/r/OpenCL/comments/1rcueln/glm47flash_isnt_editing_files_with_openclaw/"}, {"source": "Reddit", "text": "Serious question: do you think Dario (or any other major AI players or political players) have enough power and influence that they will get Chinese local AI and/or local AI in general banned in the U.S.? What do you think the odds are? I guess I'll put Dario in the title, since he's the most relevant hater of the day, and I guess fairly powerful in regards to this as far as any one specific guy goes, but, obviously if something like this happened, it would involve a lot more people combining th", "score": 29, "created": 1771881573.0, "url": "https://reddit.com/r/LocalLLaMA/comments/1rcuaip/serious_question_do_you_think_dario_or_any_other/"}, {"source": "Reddit", "text": "Me (Mincoffical)and Elc29 created an AI Agent inside of Apple shortcuts that can execute multiple tasks and is entirely free with no apps or API keys needed This shortcut loops around and decides what actions to do next. powered by the pollinations API, you can choose to either not use an API key and get free access to GPT-5-Mini, or get your own API key from pollinations for free and get 1.5 pollen (dollars of credits) per week and use other models like Deepseek, GPT-5.2, GLM-5, Kimi K2.5, and ", "score": 9, "created": 1771880527.0, "url": "https://reddit.com/r/shortcuts/comments/1rctv1b/me_mincofficaland_elc29_created_an_ai_agent/"}, {"source": "Reddit", "text": "M4 Mac mini 24Gig - Best GLM-4.7-Flash  model optimization in progress Hey Everyone,\n\nI've spent a few days trying to fit the biggest/best model I possibly could on an M4 Mac mini with 24gig RAM running OpenClaw and Ollama.\n\nI could fit GPT-OSS:20B with high thinking and plenty of context window. It performed well and fit comfortably in memory. That has been the best performing model speed-wise, but I found it lacking in multi step agentic tasks (not coding - just experimenting with butler/assis", "score": 0, "created": 1771880431.0, "url": "https://reddit.com/r/openclaw/comments/1rcttgw/m4_mac_mini_24gig_best_glm47flash_model/"}, {"source": "Reddit", "text": "Tired of $20 \u201cVibe Coding\u201d Plans That Explode in Token Cost? I Built a Flexi Alternative. Vibe for nearly zero cost!! I've been deep in the \"vibe coding\" scene, watching platforms like Lovable and Replit make waves. But as an Indian founder/dev, I constantly hit a wall that I know many of you feel too: the cost and vendor lock-in. \ud83d\ude29\n\nIt's always the same story: a tempting $25 to get started, then BAM! \"Credit limits\" or usage spikes hit the moment you try to build anything serious. And don't eve", "score": 0, "created": 1771875109.0, "url": "https://reddit.com/r/VibeCodeDevs/comments/1rcrblf/tired_of_20_vibe_coding_plans_that_explode_in/"}, {"source": "Reddit", "text": "Are the free models broken rn? Whenever I use my bots with Stellar Odyssey or the new GLM thing they only ever spit out reponses that are completely unrelated to the greeting and the previous message I sent and instead make up a scenario that seems based on the character description. Can anybody tell me why that is?? It's been like this for a few days now, I think since the update.\n\nI have all the default settings and all. The bots I use are my own private ones. Is it just because bots pre-updat", "score": 8, "created": 1771873963.0, "url": "https://reddit.com/r/WyvernChat/comments/1rcqs5b/are_the_free_models_broken_rn/"}, {"source": "Reddit", "text": "Arij - OSS project - Another agent / project manager. Kanban powered by any agent CLI Beware, non ai slop text onward.\n\nI present Arij to you (you can pronounce it how you want), a project / agent manager UI, that let you easily manage multiple agent across multiple CLI / models, and enforce an easy-to-read workflow.\n\nThe core idea is born during my own work habit. I usually work on many project at the same time, and as part of my job it to try and work with many different LLMs and coding agent ", "score": 1, "created": 1771872180.0, "url": "https://reddit.com/r/OpenSourceeAI/comments/1rcpxpj/arij_oss_project_another_agent_project_manager/"}, {"source": "Reddit", "text": "Thoughts on this benchmark? vCopied from X post:\n\n\"\"\"\n\nIntroducing the latest results of our Long-Context Agentic Orchestration Benchmark.\n\n\u2022 31 high-complexity, non-coding scenarios (100k+ tokens) where the model must select the correct next-step action using proprietary orchestration logic with no public precedent \u2014 a pure test of instruction following and long-context decision-making.\n\n\u2022 All models run at minimum thinking/reasoning settings and temperature 0 \u2014 simulating production orchestrat", "score": 6, "created": 1771869094.0, "url": "https://reddit.com/r/Bard/comments/1rcog4p/thoughts_on_this_benchmark/"}, {"source": "Reddit", "text": "Thoughts on this benchmark? Copied from X post:\n\n\"\"\"\n\nIntroducing the latest results of our Long-Context Agentic Orchestration Benchmark.\n\n\u2022 31 high-complexity, non-coding scenarios (100k+ tokens) where the model must select the correct next-step action using proprietary orchestration logic with no public precedent \u2014 a pure test of instruction following and long-context decision-making.\n\n\u2022 All models run at minimum thinking/reasoning settings and temperature 0 \u2014 simulating production orchestrati", "score": 2, "created": 1771869021.0, "url": "https://reddit.com/r/GeminiAI/comments/1rcoevm/thoughts_on_this_benchmark/"}, {"source": "Reddit", "text": "FoodTruck Bench update: tested Sonnet 4.6, Gemini 3.1 Pro, Qwen 3.5. Case studies with comparisons for each. Three new models tested and added to the leaderboard since last week's post: Claude Sonnet 4.6, Gemini 3.1 Pro, and Qwen 3.5 397B. Wrote detailed case studies for each. Here's the summary.\n\nClaude Sonnet 4.6 \u2014 massive leap from Sonnet 4.5. Genuine business reasoning, zero bankruptcies, $17.4K net worth. But here's the thing: a single simulation run on Sonnet costs only 10% less than Opus ", "score": 0, "created": 1771868165.0, "url": "https://reddit.com/r/LocalLLaMA/comments/1rco0d9/foodtruck_bench_update_tested_sonnet_46_gemini_31/"}, {"source": "Reddit", "text": "i tested every ai subscription heres the actual cost per model i got tired of paying for multiple ai apps so i actually sat down and compared what you're paying vs what you get\n\nrough breakdown from my end\n\nchatgpt plus $20/month\u00a0 \n\ngpt 5.2 with rate limits\n\nclaude pro $20/month\u00a0 \n\nopus 4.6 sonnet\n\ngemini advanced $20/month\u00a0 \n\ngemini 3\n\nso if you're using all three thats basically $60/month just to access 3 model families\n\nwhat caught me off guard was blackbox pro\n\nfirst month is $1 normally $10", "score": 6, "created": 1771868091.0, "url": "https://reddit.com/r/aiHub/comments/1rcnz59/i_tested_every_ai_subscription_heres_the_actual/"}, {"source": "Reddit", "text": "GLM-5 is the new top open-weights model on the Extended NYT Connections benchmark, with a score of 81.8, edging out Kimi K2.5 Thinking (78.3) More info: [https://github.com/lechmazur/nyt-connections/](https://github.com/lechmazur/nyt-connections/)", "score": 127, "created": 1771867862.0, "url": "https://reddit.com/r/LocalLLaMA/comments/1rcnv9h/glm5_is_the_new_top_openweights_model_on_the/"}, {"source": "Reddit", "text": "Gemini 3.1 Pro + Claude Opus 4.6 + GPT 5.2 Pro For Just $5/Month (With API Access) **Hey Everybody,**\n\nFor all the AI users out there, we are doubling InfiniaxAI Starter plans rate limits + Making Claude 4.6 Opus &amp; GPT 5.2 Pro &amp; Gemini 3.1 Pro available with high rate limits for just $5/Month!\n\nHere are some of the features you get with the Starter Plan:\n\n\\- $5 In Credits To Use The Platform\n\n\\- Access To Over 120 AI Models Including Opus 4.6, GPT 5.2 Pro, Gemini 3 Pro &amp; Flash, GLM 5", "score": 0, "created": 1771865032.0, "url": "https://reddit.com/r/VibeCodeDevs/comments/1rcmipd/gemini_31_pro_claude_opus_46_gpt_52_pro_for_just/"}, {"source": "Reddit", "text": "Built this dashboard with opentelemetry to monitor openclaw I work at signoz, and have been playing with openclaw in my free time. My token usage was hitting limits very quickly. So I started exploring how we can monitor it easily. You can check the steps to create the above dashboard here: more details: [https://signoz.io/blog/monitoring-openclaw-with-opentelemetry/](https://signoz.io/blog/monitoring-openclaw-with-opentelemetry/)", "score": 3, "created": 1771864915.0, "url": "https://reddit.com/r/ClaudeCode/comments/1rcmgmg/built_this_dashboard_with_opentelemetry_to/"}, {"source": "Reddit", "text": "Could GLM\u2019s token speed be a hardware limitation? The GLM code plan\u2019s tokens per second isn\u2019t slow because of usage alone\u2014it could be limited by the hardware they\u2019re running it on. It\u2019s possible the model is already operating at the maximum tokens per second that their current infrastructure can support ?", "score": 4, "created": 1771864404.0, "url": "https://reddit.com/r/ZaiGLM/comments/1rcm7zn/could_glms_token_speed_be_a_hardware_limitation/"}, {"source": "Reddit", "text": "Multi-Model Invoice OCR Pipeline Built an open-source **invoice OCR pipeline** that combines multiple OCR / layout / extraction models into a single reproducible pipeline.\n\nRepo: [https://github.com/dakshjain-1616/Multi-Model-Invoice-OCR-Pipeline](https://github.com/dakshjain-1616/Multi-Model-Invoice-OCR-Pipeline)\n\n# What it does\n\n* Runs **multiple OCR + layout models** on invoices\n* Aggregates outputs into structured fields (invoice number, totals, line items, etc.)\n* Designed for **real invoic", "score": 4, "created": 1771863082.0, "url": "https://reddit.com/r/LocalLLaMA/comments/1rclm3z/multimodel_invoice_ocr_pipeline/"}, {"source": "Reddit", "text": "GLM-4.7-Flash vs Qwen3-Coder-Next vs GPT-OSS-120b Which is the best to sue with Openclaw (i have been using Qwen3-Coder-Next, and so far it is great but slow so i am looking to switch any hints ?)\n\nIn my previous experience with GLM-4.7-Flash it was too but tool call with absolutely bad, however I learned that it could be fixed (in Cline for an example) and by adjusting the temp and other parameters for agentic usage\n\nFor GPT-OSS, i am not sure whether to sue it or not ?\n\nAny help ?\n\nEDIT3: the ", "score": 0, "created": 1771862859.0, "url": "https://reddit.com/r/LocalLLaMA/comments/1rclied/glm47flash_vs_qwen3codernext_vs_gptoss120b/"}, {"source": "Reddit", "text": "How to drastically reduce token usage in OpenClaw? (context, memory, gateway optimization) I\u2019m currently running OpenClaw on my own VPS with external LLM providers (Kimi, Qwen, GLM, etc.), and I\u2019m trying to significantly reduce token consumption while maintaining good reasoning quality.\n\nRight now, token usage increases very quickly, especially due to:\n\n\t\u2022\tlarge conversation context being resent every turn\n\n\t\u2022\tmemory injection from the memory plugin\n\n\t\u2022\tsystem prompts and tool schemas being repe", "score": 3, "created": 1771862341.0, "url": "https://reddit.com/r/openclaw/comments/1rcl9oz/how_to_drastically_reduce_token_usage_in_openclaw/"}, {"source": "Reddit", "text": "LLM request Timed out Hey guys,\n\nI just started with openclaw last week but had some trouble. I have a lot of errors and almost constantly debuging openclaw rather then using it. Especially the LLM request timed out error i get Everyday but idk what it really means. I use GLM 5 via modal.com through llm lite and Gemini 3.1 through regular api keys. Can someone help me ? ", "score": 1, "created": 1771862165.0, "url": "https://reddit.com/r/clawdbot/comments/1rcl6wa/llm_request_timed_out/"}, {"source": "Reddit", "text": "How I built an OpenClaw skill for tax preparation. TaxClaw Hey OpenClaw folks \u2014 Doug here.\n\nTax season is the worst part of the \"AI can do everything\" timeline\u2026 because the first thing you run into is: \"cool, but I'm not uploading PDFs with SSNs/EINs to some random cloud OCR.\"\n\nSo I built TaxClaw: a local-first tax document extractor packaged as an OpenClaw skill.\n\nWhy this exists\n\n\u2022 I wanted to point OpenClaw at a folder of tax PDFs and get clean structured output.\n\n\u2022 I wanted a UI where I can ", "score": 2, "created": 1771861380.0, "url": "https://reddit.com/r/openclaw/comments/1rckufr/how_i_built_an_openclaw_skill_for_tax_preparation/"}, {"source": "HackerNews", "text": "Off Grid: On-device AI-web browsing, tools, vision, image gen, voice \u2013 3x faster Nine days ago I posted Off Grid here and you showed up - 124 points, 66 comments, bug reports I fixed same-day, and the kind of feedback that makes open source worth it.<p>You told me what you wanted. \nHere&#x27;s what I shipped:\nYour AI can now use tools - entirely offline.<p>Web search, calculator, date&#x2F;time, device info - with automatic tool loops.<p>Your 3B parameter model doesn&#x27;t just generate text an", "score": 1, "created": "2026-02-24T19:43:48Z", "url": "https://news.ycombinator.com/item?id=47141803"}, {"source": "HackerNews", "text": "Show HN: Interactive 3D Moon with real NASA data and WebGPU A photorealistic Moon viewer running entirely in the browser. WebGPU primary renderer with WebGL 2 fallback.<p>- NASA CGI Moon Kit textures served via a quadtree LOD tile system\n- Oren-Nayar BRDF (lunar regolith is non-Lambertian with strong backscatter)\n- Sun position calculated from astronomy-engine (\u00b11 arcminute)\n- Scrub through the full lunation cycle or watch in real time\n- Earth and Tycho-2 starfield in the background<p>Tech: Thre", "score": 2, "created": "2026-02-24T19:43:01Z", "url": "https://news.ycombinator.com/item?id=47141789"}, {"source": "HackerNews", "text": "Show HN: I applied Markowitz port. theory to agent teams / proved it in a zkVM I run multi-agent teams in high-consequence scenarios. Read: fuckups at 3 AM = I&#x27;m awake.<p>I kept hitting the same issue. I couldn&#x27;t get a rules-based system to enforce behavior <i>and</i> I had no real way to prove that agents really did what they said they did. I can log and monitor them - set up (a million) Slack alerts but none of these things are PROOF. Logs are mutable. And that matters more every day", "score": 2, "created": "2026-02-24T19:41:04Z", "url": "https://news.ycombinator.com/item?id=47141753"}, {"source": "HackerNews", "text": "Show HN: Srclight \u2013 Deep code indexing MCP server (FTS5 and Tree-sitter) Hi HN, I built srclight because AI coding agents (Claude Code, Cursor, etc.) waste 40-60% of their tokens just searching for code and understanding structure. grep and glob aren&#x27;t enough when you need to know who calls a function, what changed recently, or find code by concept.<p>srclight indexes your codebase with tree-sitter and builds three FTS5 indexes (symbol names, source code via trigram, docstrings via Porter s", "score": 1, "created": "2026-02-24T19:33:48Z", "url": "https://news.ycombinator.com/item?id=47141625"}, {"source": "HackerNews", "text": "Show HN: Vis Pro \u2013 A Formula-Based Workout Program Editor Hey HN,<p>About 5 years ago, I built a weightlifting app for 5&#x2F;3&#x2F;1 that got me on the front page of HN [0]. After that, life happened. I had kids and so decided to get a job and put that project on ice. Eventually I grew too disappointed with my job, and decided to try building something again.<p>The biggest feedback I kept getting from users was simple:\n\u201cLet me create my own programs.\u201d<p>That\u2019s how Vis started.<p>The initial id", "score": 3, "created": "2026-02-24T19:22:38Z", "url": "https://news.ycombinator.com/item?id=47141440"}, {"source": "HackerNews", "text": "Agents of Chaos: a red team study of autonomous LLM agents with full access ", "score": 2, "created": "2026-02-24T19:17:35Z", "url": "https://news.ycombinator.com/item?id=47141373"}, {"source": "HackerNews", "text": "Show HN: Open-source EU AI Act compliance layer for AI agents (8/2026 deadline) We built AIR Blackbox \u2014 open-source compliance infrastructure for AI agents targeting the EU AI Act enforcement deadline on August 2, 2026.\nIf you&#x27;re deploying LLM-based agents (LangChain, CrewAI, AutoGen, OpenAI Agents SDK) into production, the EU AI Act requires tamper-evident audit trails, human oversight mechanisms, data governance controls, and injection defense \u2014 for any system classified as high-risk.\nMos", "score": 1, "created": "2026-02-24T19:15:45Z", "url": "https://news.ycombinator.com/item?id=47141347"}, {"source": "HackerNews", "text": "Show HN: Permanent Underclass \u2013 Terminal game about AI acceleration (Rust) ", "score": 3, "created": "2026-02-24T19:06:44Z", "url": "https://news.ycombinator.com/item?id=47141214"}, {"source": "HackerNews", "text": "Show HN: QueryVeil \u2013 An AI data analyst that investigates your data Hi HN,<p>I built QueryVeil because I was tired of two things: (1) uploading data to third-party tools, and (2) AI tools that just translate English to one SQL query and call it done.<p>QueryVeil is an AI data analyst that actually investigates. When you ask &quot;why did revenue drop last month?&quot;, it doesn&#x27;t just run one query \u2014 it plans an approach, runs multiple queries, self-corrects when it hits errors, and builds ", "score": 2, "created": "2026-02-24T19:06:13Z", "url": "https://news.ycombinator.com/item?id=47141207"}, {"source": "HackerNews", "text": "Show HN: Run any LLM inside Claude Code. A local auditable proxy for 7 providers Make your Claude Code run Codex, Gemini, OLLama, Groq or anything else. Read full capture logs of what&#x27;s going. Small, local, transparent.", "score": 1, "created": "2026-02-24T18:58:00Z", "url": "https://news.ycombinator.com/item?id=47141088"}, {"source": "HackerNews", "text": "Show HN: Connector-OSS \u2013 Memory integrity kernel for AI agents I built connector-oss because I kept running into the same problem with\nmulti-agent AI systems: when something goes wrong, you have no cryptographic\nground truth of what any agent stored, read, or accessed. Log files are\nmutable. There&#x27;s no tamper-proof record.<p>connector-oss is an in-process Rust kernel that adds memory integrity\nprimitives to existing agent frameworks (LangChain, CrewAI, AutoGen).<p>How it works:<p>1. Every m", "score": 1, "created": "2026-02-24T18:53:41Z", "url": "https://news.ycombinator.com/item?id=47141020"}, {"source": "HackerNews", "text": "Inception Launches Mercury 2, the Fastest Reasoning LLM ", "score": 2, "created": "2026-02-24T18:23:12Z", "url": "https://news.ycombinator.com/item?id=47140637"}, {"source": "HackerNews", "text": "Ask HN: Demand for a compliance-first deterministic context compiler? I built a UK-filed, patent-pending system that sits between enterprise data and an LLM to compile the smallest structurally complete context packet for a\n given query.<p>Most context tooling is built around semantic similarity. This is built first around governance: in regulated environments, teams need outputs they can\n explain, audit, and defend.<p>Core properties:\n - deterministic, budget-bounded context assembly\n - graph-a", "score": 1, "created": "2026-02-24T18:20:44Z", "url": "https://news.ycombinator.com/item?id=47140602"}, {"source": "HackerNews", "text": "Ask HN: How to exhaustively search the scientific literature? I have a need for a comprehensive database of a certain type of event described in the scientific literature. For what it&#x27;s worth, the event is a &#x27;paleoearthquake&#x27;, which is a historic or prehistoric earthquake that is found in the geologic record, usually by digging a trench across a fault line and identifying the disturbances in the geologic strata across or adjacent to the fault and, if possible, dating them via radi", "score": 2, "created": "2026-02-24T18:19:27Z", "url": "https://news.ycombinator.com/item?id=47140588"}, {"source": "HackerNews", "text": "Show HN: Open-next-router (ONR) \u2013 An Nginx-inspired, config-driven LLM router Despite the name, OpenRouter&#x27;s core isn&#x27;t truly open. I\u2019m building open-next-router (ONR) to change that.<p>Inspired by Nginx\u2019s atomic configuration philosophy, ONR allows developers to integrate and switch between various LLM channels through simple config files rather than writing boilerplate code. The goal is to move away from &quot;hard-coded integrations&quot; and toward a modular, high-performance routi", "score": 1, "created": "2026-02-24T18:08:10Z", "url": "https://news.ycombinator.com/item?id=47140415"}, {"source": "HackerNews", "text": "Show HN: Tag Promptless on any GitHub PR/Issue to get updated user-facing docs Hi HN! I&#x27;m Prithvi\u2014my co-founder Frances and I launched Promptless almost a year ago here (<a href=\"https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=43092522\">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=43092522</a>). It&#x27;s an AI teammate that watches your workflows\u2014code changes, support tickets, Slack threads, etc.\u2014and automatically drafts doc updates when it spots something that should be documen", "score": 13, "created": "2026-02-24T18:01:16Z", "url": "https://news.ycombinator.com/item?id=47140332"}, {"source": "HackerNews", "text": "Frontier LLM Leaderboard ", "score": 1, "created": "2026-02-24T17:41:20Z", "url": "https://news.ycombinator.com/item?id=47140049"}, {"source": "HackerNews", "text": "I built a governance layer for multi-agent AI coding \u2013 lessons after 6 months Six months ago I started coordinating multiple AI coding agents (Claude Code, Codex CLI, Gemini CLI) across parallel terminals for a production project. The agents were productive, but I had no idea what they were actually deciding or why.<p>The problem wasn&#x27;t capability \u2014 it was accountability. An agent would make a choice buried in a 50-file commit, and I&#x27;d only find out weeks later when something broke. No", "score": 2, "created": "2026-02-24T17:35:57Z", "url": "https://news.ycombinator.com/item?id=47139978"}, {"source": "HackerNews", "text": "Why Your LLM NPCs Sound the Same (and How to Fix It) ", "score": 1, "created": "2026-02-24T17:22:07Z", "url": "https://news.ycombinator.com/item?id=47139780"}, {"source": "HackerNews", "text": "I built an AI browser with prompt-injection defense at 16 on an i5 with 8GB RAM URL: https:&#x2F;&#x2F;github.com&#x2F;Preet3627&#x2F;Comet-AI<p>TEXT:\nHey HN, I&#x27;m Preet, 16 years old, and I&#x27;ve been building Comet AI Browser for the past 2 months while preparing for JEE.\nI want to be upfront about what this is and what it isn&#x27;t.\nWhat it is:\nA cross-platform AI browser (Windows&#x2F;macOS&#x2F;Linux&#x2F;Android&#x2F;iOS) with a security architecture I couldn&#x27;t find anywhere el", "score": 3, "created": "2026-02-24T17:21:29Z", "url": "https://news.ycombinator.com/item?id=47139768"}, {"source": "HackerNews", "text": "Large-Scale Online Deanonymization with LLM Agents ", "score": 1, "created": "2026-02-24T17:18:17Z", "url": "https://news.ycombinator.com/item?id=47139716"}, {"source": "HackerNews", "text": "Be Careful with LLM \"Agents\" ", "score": 2, "created": "2026-02-24T17:06:09Z", "url": "https://news.ycombinator.com/item?id=47139556"}, {"source": "HackerNews", "text": "Show HN: Open-source LLM and dataset for sports forecasting (Pro Golf) Hey HN, I fine-tuned a small open-source model on golf forecasting and it beats GPT-5 at predicting golf outcomes. The same approach can be used to build a specialized model in any domain, you just need to update a few search queries.<p>We fine-tuned gpt-oss-120b with LoRA on 3,178 golf forecasting questions, using GRPO with Brier score as the reward.<p>Our model outperformed GPT-5 on Brier Skill (17% vs 12.8%) and ECE (6% vs", "score": 7, "created": "2026-02-24T16:58:45Z", "url": "https://news.ycombinator.com/item?id=47139434"}, {"source": "HackerNews", "text": "Subtask \u2013 Multi-LLM routing for Claude Code quota limits ", "score": 5, "created": "2026-02-24T16:36:48Z", "url": "https://news.ycombinator.com/item?id=47139170"}, {"source": "HackerNews", "text": "Show HN: oMLX \u2013 coding agents on local LLMs without the painful reprefill I was frustrated that coding agents like Claude Code were basically unusable with local models. every few turns the prefix shifts, KV cache gets invalidated, and your mac has to re-prefill the entire context from scratch.<p>So i built oMLX. it persists KV cache blocks to SSD, and when a previous context comes back, it restores from disk instead of recomputing. this alone made Qwen3-Coder-80B on my M3 Ultra actually usable ", "score": 3, "created": "2026-02-24T16:35:45Z", "url": "https://news.ycombinator.com/item?id=47139158"}, {"source": "HackerNews", "text": "Show HN: Pilo \u2013 open-source agentic web automation engine by Mozilla Hello HN,<p>We are the team behind Tabstack (<a href=\"https:&#x2F;&#x2F;tabstack.ai\" rel=\"nofollow\">https:&#x2F;&#x2F;tabstack.ai</a>) - part of Mozilla. We just open sourced Pilo (pronounce PIE-low), the core engine that powers our automation platform. You can check it out on Github at <a href=\"https:&#x2F;&#x2F;github.com&#x2F;mozilla&#x2F;pilo\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;mozilla&#x2F;pilo</a>.<p>Pilo is", "score": 14, "created": "2026-02-24T16:31:39Z", "url": "https://news.ycombinator.com/item?id=47139110"}, {"source": "HackerNews", "text": "Show HN: Turn human decisions into blocking tool-calls for AI agents (iOS+CLI) WHY was I SSH\u2019ing into my laptop from my phone at parties?!<p>Either I had a feature idea I wanted an agent to build right then, or I was worried my agents were blocked waiting on my decision.<p>It dawned on me: humans are just another dependency in an agent workflow, so I turned myself into a tool-call.<p>I built an iOS app (Extendo) where agents can reach me to request approvals, choices, or plan reviews.  They just", "score": 2, "created": "2026-02-24T16:23:03Z", "url": "https://news.ycombinator.com/item?id=47138996"}, {"source": "HackerNews", "text": "Show HN: Code Execution for LLMs via a Sandboxed Lua REPL ", "score": 1, "created": "2026-02-24T16:03:53Z", "url": "https://news.ycombinator.com/item?id=47138784"}, {"source": "HackerNews", "text": "Paying publishers when LLMs use their pages: recognyze.ai ", "score": 1, "created": "2026-02-24T16:02:29Z", "url": "https://news.ycombinator.com/item?id=47138769"}, {"source": "HackerNews", "text": "Artificial Intelligence: What Should the Educational System Do to Survive? Artificial Intelligence: What Should the Educational System Do to Survive?<p>The educational system is currently being offered something fundamentally unclear in relation to AI. A system that makes mistakes. Imagine an honest dialogue between a minister of education and the chief developer of a company working in artificial intelligence.<p>Yes, the system can sometimes say things that are not true. It can sometimes inadeq", "score": 1, "created": "2026-02-24T16:01:38Z", "url": "https://news.ycombinator.com/item?id=47138762"}, {"source": "HackerNews", "text": "Show HN: Detect LLM hallucinations via geometric drift (0.9 AUC, 1% overhead) I built SIB-ENGINE, a real-time hallucination detection system\nthat monitors LLM internal structure rather than output content.<p>KEY RESULTS (Gemma-2B, N=1000):\n\u2022 54% hallucination detection with 7% false positive rate\n\u2022 &lt;1% computational overhead (runs on RTX 3050 with 4GB VRAM)\n\u2022 ROC-AUC: 0.8995<p>WHY IT&#x27;S DIFFERENT:\nTraditional methods analyze the output text semantically.\nSIB-ENGINE monitors &quot;geometri", "score": 1, "created": "2026-02-24T15:59:16Z", "url": "https://news.ycombinator.com/item?id=47138734"}, {"source": "HackerNews", "text": "Show HN: Noodles \u2013 Turn any codebase into a diagram with Claude and Tree-sitter I built a CLI tool that turns codebases and PRs into diagrams so you can quickly understand how things fit together. Originally made it because I couldn&#x27;t follow my own AI-generated repos.<p>Just shipped a big update:<p>- Switched from D2 to Mermaid for rendering\n- Tree-sitter AST parsing + agentic flow instead of raw LLM calls. ~50x faster.\n- Works on any GitHub repo or PR, not just local\n- Dropped the web fron", "score": 6, "created": "2026-02-24T15:53:20Z", "url": "https://news.ycombinator.com/item?id=47138656"}, {"source": "HackerNews", "text": "Show HN: Neuron \u2013 Independent Rust crates for building AI agents The core of every agent framework is the same ReAct loop. It&#x27;s commodity code. What actually matters is everything around that loop \u2014 how you manage context windows, how you pipeline tool execution, how you handle durability and replay. These are hard problems with real design trade-offs, and yet every framework bundles them into one monolith where you buy all of it or none of it.<p>neuron is the layer below frameworks. It def", "score": 1, "created": "2026-02-24T15:49:56Z", "url": "https://news.ycombinator.com/item?id=47138618"}, {"source": "HackerNews", "text": "Show HN: TTSLab \u2013 Text-to-speech that runs in the browser via WebGPU I built TTSLab \u2014 a free, open-source tool for running text-to-speech and speech-to-text models directly in the browser using WebGPU and WASM.\nNo API keys, no backend, no data leaves your machine.<p>When you open the site, you&#x27;ll hear it immediately \u2014 the landing page auto-generates speech from three different sentences right in your browser, no setup required.<p>You can then try any model yourself: type text, hit generate,", "score": 2, "created": "2026-02-24T15:44:26Z", "url": "https://news.ycombinator.com/item?id=47138571"}, {"source": "HackerNews", "text": "Ask HN: Who has seen productivity increases from AI I would love examples of positions and industries where AI has been revolutionary. I have a friend at one of the largest consulting firms who has said it&#x27;d been a game-changer in terms of processing huge amounts of documentation over a short period of time. Whether or not that gives better results is another question, but I would love to hear more stories of AI actually making things better.", "score": 6, "created": "2026-02-24T15:24:44Z", "url": "https://news.ycombinator.com/item?id=47138336"}, {"source": "HackerNews", "text": "Show HN: Glass Box: writing editor that exports a verifiable PDF of your process We built a local-first writing editor that records the composition process (keystrokes, paste events, AI interactions) and exports a PDF with annotation highlights + an AI usage appendix.<p>The motivation: AI detectors are genuinely broken. Stanford research showed &gt;60% false positive rates for non-native English writers on TOEFL essays. The &quot;solution&quot; from institutions has been to buy more detectors or", "score": 2, "created": "2026-02-24T15:20:16Z", "url": "https://news.ycombinator.com/item?id=47138273"}, {"source": "HackerNews", "text": "LLM-as-a-Judge: Evaluating Output Without a Ground Truth ", "score": 1, "created": "2026-02-24T15:14:27Z", "url": "https://news.ycombinator.com/item?id=47138187"}, {"source": "HackerNews", "text": "Show HN: Jsonchunk \u2013 Parse incomplete JSON from streaming LLM responses GitHub: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;jbingen&#x2F;jsonchunk\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;jbingen&#x2F;jsonchunk</a><p>npm: <a href=\"https:&#x2F;&#x2F;www.npmjs.com&#x2F;package&#x2F;jsonchunk\" rel=\"nofollow\">https:&#x2F;&#x2F;www.npmjs.com&#x2F;package&#x2F;jsonchunk</a><p>If you&#x27;re building on top of LLMs with structured output, you&#x27;ve hit this: the model streams JSON token by to", "score": 1, "created": "2026-02-24T15:04:56Z", "url": "https://news.ycombinator.com/item?id=47138060"}, {"source": "HackerNews", "text": "Show HN: Omni \u2013 Open-source workplace search and chat, built on Postgres Hey HN!<p>Over the past few months, I&#x27;ve been working on building Omni - a workplace search and chat platform that connects to apps like Google Drive&#x2F;Gmail, Slack, Confluence, etc. Essentially an open-source alternative to Glean, fully self-hosted.<p>I noticed that some orgs find Glean to be expensive and not very extensible. I wanted to build something that small to mid-size teams could run themselves, so I decid", "score": 1, "created": "2026-02-24T15:04:08Z", "url": "https://news.ycombinator.com/item?id=47138046"}, {"source": "HackerNews", "text": "Global AI data center boom hits delays ", "score": 1, "created": "2026-02-24T14:34:41Z", "url": "https://news.ycombinator.com/item?id=47137666"}, {"source": "HackerNews", "text": "I ported one program to 10 languages to see how an LLM thinks ", "score": 1, "created": "2026-02-24T14:28:46Z", "url": "https://news.ycombinator.com/item?id=47137584"}, {"source": "HackerNews", "text": "Show HN: MCP server that lets AI build, play, and debug Godot games (84 tools) ", "score": 1, "created": "2026-02-24T14:28:44Z", "url": "https://news.ycombinator.com/item?id=47137583"}, {"source": "HackerNews", "text": "Show HN: Rebuilt My Social Network with Codex in One Day Hi HN,\nI&#x27;m Micah Blachman, the creator of Crosspassion. After posting about my previous version a few days ago, I rebuilt the entire app from the ground using Codex.<p>Crosspassion aims to be Reddit, but for finding where your interests overlap. If I was interested in, say, mechanical keyboards and type design, I could check the mechanical keyboard&#x2F;type design community, and look at the latest discoveries of projects that combine", "score": 1, "created": "2026-02-24T14:23:56Z", "url": "https://news.ycombinator.com/item?id=47137522"}, {"source": "HackerNews", "text": "Show HN: Train a 230KB text classifier from 50 examples \u2013 no API keys, no GPU I kept running into the same pattern: calling an LLM API thousands of times with the same prompt template, just swapping in different text. Classify this contract clause. Route this support ticket. Categorize this log line.\nFor teams handling contracts, patient records, or internal logs, sending that data to a third-party API isn&#x27;t always an option. And at scale, you&#x27;re paying per-token for what&#x27;s essent", "score": 1, "created": "2026-02-24T14:22:53Z", "url": "https://news.ycombinator.com/item?id=47137506"}, {"source": "HackerNews", "text": "Show HN: Pythia\u2013 Moody's-style AAA\u2013C rating for any site (perf and other vitals) Hey HN,<p>A few years ago the worst boss I ever had told me \u201cI don\u2019t encourage side projects.\u201d<p>So obviously I spent a few evenings recently to build Pythia (live beta at <a href=\"https:&#x2F;&#x2F;pythia-rating.com\" rel=\"nofollow\">https:&#x2F;&#x2F;pythia-rating.com</a>).<p>It\u2019s a single letter grade (AAA down to C) that combines five indices into one executive-friendly score:\n\u2022Performance (40 %) \u2013 real CrUX field", "score": 1, "created": "2026-02-24T14:22:30Z", "url": "https://news.ycombinator.com/item?id=47137502"}, {"source": "HackerNews", "text": "Show HN: Axon \u2013 A Kubernetes-native framework for AI coding agents Hi HN,\nI originally started this project simply to safely run autonomous coding agents (like Claude in auto-mode) in isolated environments. But as I built it, I realized the potential was much bigger than just sandboxing: it&#x27;s about making a coding agent callable like a standard API, and ultimately, defining your entire development workflow as infrastructure.<p>Axon is a Kubernetes-native framework that abstracts the coding ", "score": 1, "created": "2026-02-24T14:21:39Z", "url": "https://news.ycombinator.com/item?id=47137491"}, {"source": "GitHub", "text": "[Bug]: opencode/glm-4.7-free no longer exists ### Prerequisites\n\n- [x] I will write this issue in English (see our [Language Policy](https://github.com/code-yeongyu/oh-my-opencode/blob/dev/CONTRIBUTING.md#language-policy))\n- [x] I have searched existing issues to avoid duplicates\n- [x] I am using the latest version of oh-my-opencode\n- [x] I have read the [documentation](https://github.com/code-yeongyu/oh-my-opencode#readme) or asked an AI coding agent with this project's GitHub URL loaded and co", "score": 0, "created": "2026-02-24T19:12:23Z", "url": "https://github.com/code-yeongyu/oh-my-opencode/issues/2101"}, {"source": "GitHub", "text": "[Issue]: GPU memory access faults on GLM-4.6 FP8 ### Problem Description\n\nHey team, \n\nI'm noticing occasional GPU memory access faults when running GLM-4.6-FP8 with AITER enabled when running on vLLM. Any idea what's happening? \n\nDocker image: `rocm/vllm-dev:nightly@sha256:24d2353d410b8544b70edc93cde353f072d40ada0a8dba3fdbcd56138ae19316`\n\nCommand:\n```\nVLLM_ROCM_USE_AITER=1 VLLM_ROCM_USE_AITER_MOE=1 vllm serve zai-org/GLM-4.6-FP8 --tensor-parallel-size 4 --quantization compressed-tensors  --gpu-m", "score": 0, "created": "2026-02-24T17:53:46Z", "url": "https://github.com/ROCm/aiter/issues/2098"}, {"source": "GitHub", "text": "debug: log GLM request/response body on API errors Logs the full request body and response body when the GLM API returns a non-2xx status code, making it easier to diagnose API errors like code 1210.", "score": 0, "created": "2026-02-24T17:33:46Z", "url": "https://github.com/tomasmach/vespra/pull/36"}, {"source": "GitHub", "text": "feat: Add GLM 5 implementation ## Summary\n- Add `nemo_automodel/components/models/glm_moe_dsa/` for the GLM-MoE-DSA architecture ([zai-org/GLM-5](https://huggingface.co/zai-org/GLM-5))\n- Reuses `DeepseekV32MLA` (MLA + DSA Indexer) from `deepseek_v32/layers.py` and follows the `glm4_moe_lite` pattern for Block/Model/ForCausalLM\n- Extends `Glm4MoeStateDictAdapter` with indexer-specific non-quantized key handling\n- Bumps `transformers` dependency to `>=5.2.0` for `GlmMoeDsaConfig` support\n\n## Test ", "score": 1, "created": "2026-02-24T17:31:50Z", "url": "https://github.com/NVIDIA-NeMo/Automodel/pull/1372"}, {"source": "GitHub", "text": "Add support for GLM-5 model (GlmMoeDsaForCausalLM in HF) ", "score": 0, "created": "2026-02-24T17:31:11Z", "url": "https://github.com/d9d-project/d9d/issues/11"}, {"source": "GitHub", "text": "Support for GLM series models from Zhipu AI ## Question\n\nDoes this project support GLM series models from Zhipu AI (\u667a\u8c31)?\n\n## Context\n\nI noticed the project supports OpenAI-compatible APIs, and Zhipu's GLM models provide an OpenAI-compatible interface. I'd like to confirm whether GLM models (such as GLM-5, GLM-4.7, etc.) are officially supported or tested with epub-translator.\n\n## Technical Details\n\nZhipu's API endpoint format:\n- Base URL: `https://open.bigmodel.cn/api/paas/v4/`\n- Compatible with", "score": 0, "created": "2026-02-24T17:29:23Z", "url": "https://github.com/oomol-lab/epub-translator/issues/129"}, {"source": "GitHub", "text": "feat: add Z.AI provider support (GLM models, free tier, reasoning, we\u2026 \u2026b search)\r\n\r\nAdd Z.AI (Zhipu AI) as the 17th built-in provider. Z.AI offers GLM models with free tiers (GLM-4.7-Flash, GLM-4.5-Flash), toggleable reasoning for GLM-4.5+ models, and web search support.\r\n\r\n7 models across 5 tiers. Reasoning uses thinking parameter (enabled/disabled) with reasoning_content in responses (like DeepSeek). Web search uses tools API.\r\n\r\nSee: #48", "score": 0, "created": "2026-02-24T16:49:00Z", "url": "https://github.com/zeeyado/koassistant.koplugin/pull/50"}, {"source": "GitHub", "text": "fix: deactivate retired models and fix glm-5 ## Summary\n- Add `deactivatedAt` for `anthropic/claude-3-5-haiku` (retired Feb 19, 2026 per [Anthropic docs](https://platform.claude.com/docs/en/about-claude/model-deprecations))\n- Add `deactivatedAt` for `cerebras/qwen-3-32b` (deprecated Feb 16, 2026 per [Cerebras docs](https://inference-docs.cerebras.ai/models/qwen-3-32b))\n- Update `together.ai/glm-5` model ID from `THUDM/glm-5` to `zai-org/GLM-5` (old ID returns `model_not_available`)\n- Disable str", "score": 1, "created": "2026-02-24T15:47:15Z", "url": "https://github.com/theopenco/llmgateway/pull/1745"}, {"source": "GitHub", "text": "fix(failover): recognize GLM Chinese model-not-found error for fallback ## Problem\n\nWhen GLM (Zhipu AI) returns a Chinese error message \"\u6a21\u578b\u4e0d\u5b58\u5728\uff0c\u8bf7\u68c0\u67e5\u6a21\u578b\u4ee3\u7801\" (model does not exist, please check model code), OpenClaw does not recognize it as a `model_not_found` error and therefore does not trigger fallback to the configured fallback model.\n\n## Root Cause\n\n`isModelNotFoundErrorMessage()` in `src/agents/pi-embedded-helpers/errors.ts` only checks for English error patterns like:\n- \"unknown model\"\n- \"model", "score": 0, "created": "2026-02-24T15:45:28Z", "url": "https://github.com/openclaw/openclaw/pull/25577"}, {"source": "GitHub", "text": "fix(failover): recognize GLM Chinese model-not-found error for fallback ## Problem\n\nWhen the GLM (Zhipu AI) model is unavailable or misconfigured, it returns an error message in Chinese:\n```\n\u6a21\u578b\u4e0d\u5b58\u5728\uff0c\u8bf7\u68c0\u67e5\u6a21\u578b\u4ee3\u7801\n```\n\nThis error was not recognized by `isModelNotFoundErrorMessage()`, preventing automatic fallback to the next configured model.\n\n## Root Cause\n\nThe `classifyFailoverReason()` function in `src/agents/pi-embedded-helpers/errors.ts` only checks for English error patterns like \"model not found\" ", "score": 1, "created": "2026-02-24T15:35:50Z", "url": "https://github.com/openclaw/openclaw/pull/25571"}, {"source": "GitHub", "text": "Kiro,MiniMax and GLM Not Working <img width=\"1490\" height=\"906\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/ff670ee3-c18c-4607-99f3-978c931a2122\" />", "score": 3, "created": "2026-02-24T14:46:39Z", "url": "https://github.com/diegosouzapw/OmniRoute/issues/125"}, {"source": "GitHub", "text": "Add Z.AI provider support (GLM models, free tier available) Adds Z.AI (Zhipu AI) as a built-in provider. Z.AI offers GLM models with a free tier (GLM-4.7-flash, GLM-4.5-flash) and an OpenAI-compatible chat completions API.\r\n\r\n**API endpoint** (from [Z.AI docs](https://docs.z.ai/api-reference/llm/chat-completion)):\r\n```\r\nPOST https://api.z.ai/api/paas/v4/chat/completions\r\nAuthorization: Bearer <token>\r\n```\r\n\r\n### Changes\r\n\r\n- **`koassistant_api/zai.lua`** \u2014 New handler extending `OpenAICompatible", "score": 3, "created": "2026-02-24T14:36:14Z", "url": "https://github.com/zeeyado/koassistant.koplugin/pull/49"}, {"source": "GitHub", "text": "feat: GLM-OCR SDK sidecar with PP-DocLayoutV3 layout detection ## Summary\n\nAdds a Python sidecar container running the [GLM-OCR SDK](https://github.com/zai-org/GLM-OCR) Flask server that provides a two-stage OCR pipeline:\n\n1. **Layout detection** (PP-DocLayoutV3 on CPU) identifies page regions: text, tables, formulas, code\n2. **Per-region OCR** via Ollama (GPU) with region-specific prompts\n3. **Reassembly** into structured Markdown\n\nThis is the pipeline behind the 94.62 OmniDocBench score. The e", "score": 0, "created": "2026-02-24T14:02:19Z", "url": "https://github.com/Loa212/local-ocr-ollama/pull/5"}, {"source": "GitHub", "text": "Add support for new AI models: Gemini 3.1 Pro and GLM-5 ### Pre-submit Checks\n\n- [x] I have [searched Warp feature requests](https://github.com/warpdotdev/warp/issues?q=is%3Aissue+label%3AFEATURE) and there are no duplicates\n- [x] I have [searched Warp docs](https://docs.warp.dev) and my feature is not there\n\n### Describe the solution you'd like?\n\nI would like Warp to add support for the following new AI models in its AI assistant feature:\n\n1. **Gemini 3.1 Pro** \u2013 Google's latest and most capabl", "score": 2, "created": "2026-02-24T12:42:11Z", "url": "https://github.com/warpdotdev/Warp/issues/8758"}, {"source": "GitHub", "text": "Merge Interspecies Dependence GLM Function to main branch ", "score": 0, "created": "2026-02-24T12:41:04Z", "url": "https://github.com/Steph0705/Geo-data-and-Methods-in-R/pull/7"}, {"source": "GitHub", "text": "Fix GLM-OCR prompt and temperature ## Summary\n- Replace verbose freeform OCR instruction with the terse `\"Text Recognition:\"` trigger phrase that GLM-OCR was trained on (per [HuggingFace model card](https://huggingface.co/zai-org/GLM-OCR))\n- Add `temperature: 0.01` to match the SDK default config \u2014 previously unset, defaulting to Ollama's 0.7\n- Should eliminate repetition loops and significantly improve output quality\n\nCloses #2\n\n## Test plan\n- [ ] Run OCR on 5-10 sample documents and compare ou", "score": 0, "created": "2026-02-24T11:33:24Z", "url": "https://github.com/Loa212/local-ocr-ollama/pull/4"}, {"source": "GitHub", "text": "v2: GLM-OCR SDK sidecar with PP-DocLayoutV3 layout detection Extracted from #2. Step 1 (prompt fix) stays in #2, this tracks the SDK pipeline investigation.\n\n## Context\n\nThe raw model through Ollama sends one whole page with one prompt. The benchmark score (94.62 on OmniDocBench) comes from a two-stage pipeline:\n```\nImage \u2192 PP-DocLayoutV3 (layout detection) \u2192 crop regions \u2192 per-region OCR \u2192 reassemble\n```\n\nThis means tables get `\"Table Recognition:\"`, formulas get `\"Formula Recognition:\"`, etc. ", "score": 0, "created": "2026-02-24T11:30:25Z", "url": "https://github.com/Loa212/local-ocr-ollama/issues/3"}, {"source": "GitHub", "text": "Fix GLM-OCR prompt and temperature, investigate SDK pipeline for v2 ## Problem\n\nGLM-OCR scores 94.62 on OmniDocBench V1.5 (#1 overall), but in real-world blind testing on [OCR Arena](https://www.ocrarena.ai/leaderboard) it ranks #23 (ELO ~1329). Our app is likely performing even worse because of two issues:\n\n1. **Wrong prompt** \u2014 we send a verbose freeform instruction, but the model was trained on terse trigger phrases\n2. **Missing pipeline** \u2014 the benchmark score comes from a two-stage SDK pipe", "score": 0, "created": "2026-02-24T11:27:05Z", "url": "https://github.com/Loa212/local-ocr-ollama/issues/2"}, {"source": "GitHub", "text": "KV Cache Memory Estimation Error for GLM-4.7-Flash-AWQ on V100 ### Description\nWhen running GLM-4.7-Flash-AWQ on a single V100-32G-SXM2, the KV Cache memory estimation appears to be incorrect, causing premature context length truncation.\n\n### Environment\n- GPU: NVIDIA V100-32G-SXM2\n- CUDA: 12.9\n- Model: GLM-4.7-Flash-AWQ (18.4GB weights)\n- Framework: LMDeploy (TurboMind backend) with PR #4362 \n\n### Observed Behavior\n- **Context length initialized**: Only 4928 tokens (severely truncated)\n- **Expe", "score": 2, "created": "2026-02-24T11:06:59Z", "url": "https://github.com/InternLM/lmdeploy/issues/4366"}, {"source": "GitHub", "text": "Can you Add GLM and Minimax Please Add GLM and Minimax", "score": 3, "created": "2026-02-24T10:22:24Z", "url": "https://github.com/heyhuynhgiabuu/proxypal/issues/182"}, {"source": "GitHub", "text": "Switch use of openrouter/z-ai/glm-5 in CI to openrouter/minimax/minimax-m2.5 Would test the same OpenRouter path with a cheaper model (~33% of the cost).\n\nTriggered by me seeing GLM-5 fail frequently in CI, we should probably drop the subprovider suffix like `@z-ai` too so that OpenRouter can select provider and use suitable fallbacks (should decrease errors when a provider is struggling).\n\nAlso helps https://github.com/gptme/gptme/issues/1434\n\nOpen a PR", "score": 2, "created": "2026-02-24T10:20:34Z", "url": "https://github.com/gptme/gptme/issues/1461"}, {"source": "GitHub", "text": "fix(daemon): stop model selection from sticking to glm-5 in system:init ## Summary\n- fix provider/model drift during pre-query model switches by syncing `session.config.provider` with the selected model\n- clear leaked GLM routing env vars before Anthropic/non-GLM runs so SDK init is not forced to `glm-5`\n- make env restoration key-scoped so only captured vars are restored\n- add regression tests for provider env cleanup and cross-provider pre-query switches\n\n## Problem\nWhen both Anthropic and GLM", "score": 0, "created": "2026-02-24T08:46:11Z", "url": "https://github.com/lsm/neokai/pull/146"}, {"source": "GitHub", "text": "[Upstream #14874] Bug: GLM 5 Free no longer available ## Upstream Issue Fix\n\n**Source:** https://github.com/anomalyco/opencode/issues/14874\n**Upstream labels:** bug, zen\n\n### Problem Description\n\n### Description\n\nIn the Zen Model provider, GLM 5 Free is no longer available, although its still listed in the documentation. I have seen that there is a similar issue with Kimi K2.5, where the model is not being shown because of a deprecation flag (according to the issue logs).\n\n### Plugins\n\n_No respo", "score": 3, "created": "2026-02-24T08:42:03Z", "url": "https://github.com/templarsco/lightweight/issues/482"}, {"source": "GitHub", "text": "[BUG] config.json seem not work on docker compose (Error creating provider: no API key configured for model: glm-4.7) ## Quick Summary\nconfig.json seem not work on docker compose, i got Error creating provider: no API key configured for model: glm-4.7\n\n##  Environment & Tools\n- **PicoClaw Version:** docker image `sipeed/picoclaw`\n- **Go Version:** (e.g., go 1.22)\n- **AI Model & Provider:** (e.g., GPT-4o via OpenAI / DeepSeek via SiliconFlow)\n- **Operating System:** (e.g., Ubuntu 22.04 / macOS / ", "score": 2, "created": "2026-02-24T08:07:00Z", "url": "https://github.com/sipeed/picoclaw/issues/713"}, {"source": "GitHub", "text": "feat(ci): GLM-5 \uae30\ubc18 AI \ucf54\ub4dc \ub9ac\ubdf0 \uc6cc\ud06c\ud50c\ub85c\uc6b0 \ucd94\uac00 ## Summary\n- PR \uc0dd\uc131/\uc5c5\ub370\uc774\ud2b8 \uc2dc Z.ai GLM-5 API\ub85c \uc790\ub3d9 \ucf54\ub4dc \ub9ac\ubdf0 \uc2e4\ud589\n- self-hosted runner (192.168.0.5)\uc5d0\uc11c \ub3d9\uc791\n- \uc7ac\uc2dc\ub3c4 2\ud68c (\uc9c0\uc218 \ubc31\uc624\ud504), \uc778\uc99d \uc2e4\ud328 \uc2dc \uc989\uc2dc \uc911\ub2e8\n- lock/\uc790\ub3d9\uc0dd\uc131 \ud30c\uc77c \uc81c\uc678, diff 100KB \uc81c\ud55c\n- PR \ucf54\uba58\ud2b8 \ub9c8\ucee4(`<!-- ai-review -->`)\ub85c \uc911\ubcf5 \ubc29\uc9c0\n\n## \uc0dd\uc131\ub41c \ud30c\uc77c\n| \ud30c\uc77c | \uc5ed\ud560 |\n|------|------|\n| `.github/workflows/code-review.yml` | \uc6cc\ud06c\ud50c\ub85c\uc6b0 \uc815\uc758 |\n| `.github/prompts/review-system.md` | \ub9ac\ubdf0 \uc2dc\uc2a4\ud15c \ud504\ub86c\ud504\ud2b8 (FSD, \ubcf4\uc548 \uaddc\uce59 \ud3ec\ud568) |\n\n## \uc0ac\uc6a9 \uc804 \ud544\uc694 \uc124\uc815\n1. Repository Settings \u2192 Secrets \u2192 `ZHIPU_API_KEY` \ub4f1\ub85d\n2. Z.ai (open.bigmodel.cn \ub610\ub294 z.ai)", "score": 1, "created": "2026-02-24T08:05:07Z", "url": "https://github.com/krdn/ai-afterschool-fsd/pull/19"}, {"source": "GitHub", "text": "Bug: GLM 5 Free no longer available ### Description\n\nIn the Zen Model provider, GLM 5 Free is no longer available, although its still listed in the documentation. I have seen that there is a similar issue with Kimi K2.5, where the model is not being shown because of a deprecation flag (according to the issue logs).\n\n### Plugins\n\n_No response_\n\n### OpenCode version\n\n1.2.10\n\n### Steps to reproduce\n\nConnect with Zen\nLook at available models\n\n### Screenshot and/or share link\n\n_No response_\n\n### Oper", "score": 3, "created": "2026-02-24T08:03:23Z", "url": "https://github.com/anomalyco/opencode/issues/14874"}, {"source": "GitHub", "text": "fix: filter debug from context, stronger GLM system prompt **Debug filtering:**\n- Filter `\ud83d\udd27` debug lines from bot messages in channel context (both `gather_channel_context` and `_format_messages`)\n- Skip bot messages that are empty after filtering (debug-only messages)\n- Applies to all commands (clai, sclai, glm)\n\n**GLM system prompt:**\n- Separate, stronger system prompt for GLM (weaker model needs firmer guardrails)\n- Explicitly forbids meta-commentary (`[thinking...]`, `[I previously said...]`", "score": 0, "created": "2026-02-24T07:46:27Z", "url": "https://github.com/iamsix/palbot/pull/195"}, {"source": "GitHub", "text": "fix: silently ignore empty GLM responses Empty GLM responses (model refusal/safety filter) previously showed a misleading error about reasoning tokens. Now silently returns, same as if the model chose not to respond.", "score": 0, "created": "2026-02-24T07:41:44Z", "url": "https://github.com/iamsix/palbot/pull/194"}, {"source": "GitHub", "text": "fix: add glm_max_compaction_input setting (40K default) GLM was using the shared `max_compaction_input=120000`, which exceeds its 64K context window. Compaction would try to send 120K tokens to a 64K model \u2014 causing failures.\n\nAdds `glm_max_compaction_input` (default 40000), configurable via `!glmconfig`. Leaves room for system prompt + output within the 64K window.", "score": 0, "created": "2026-02-24T07:29:37Z", "url": "https://github.com/iamsix/palbot/pull/193"}, {"source": "GitHub", "text": "Max395(ubuntu24.04)AMD\u663e\u5361GLM-4.7-UD-IQ1-M\u91cf\u5316\u6a21\u578b\u90e8\u7f72\u624b\u518c\u6cc9\u5dde\u77f3\u72eeUw \u76ee\u6807\uff1a\n\u4e3a anylink \u589e\u52a0\u53ef\u89c2\u6d4b\u6027\u652f\u6301\uff0c\u63d0\u4f9b\u5b8c\u6574\u7684\u53ef\u89c2\u6d4b\u6027\uff08\u65e5\u5fd7\uff0c\u6307\u6807\uff0c\u94fe\u8def\u8ffd\u8e2a\uff09\u3002\n\n\u5b9e\u73b0\u65b9\u5f0f\uff1a\n\u57fa\u4e8e OpenTelemetry \u89c4\u8303\uff0c\u5b9e\u73b0 OTLP \u683c\u5f0f\u7684\u6807\u51c6\u53ef\u89c2\u6d4b\u6027\u652f\u6301\uff0c\u80fd\u591f\u7075\u6d3b\u63a5\u5165\u4e30\u5bcc\u7684\u53ef\u89c2\u6d4b\u6027\u6280\u672f\u6808\u3002\u800c\u4e14\u4f9d\u7136\u53ef\u4ee5\u901a\u8fc7\u914d\u5408 otel prometheus-exporter \u65e0\u7f1d\u517c\u5bb9\u5f53\u524d\u4e3b\u6d41\u57fa\u4e8e prometheus \u6307\u6807\u6280\u672f\u6808\u3002\n\n\u5feb\u901f\u5b9e\u73b0\uff1a\n\u901a\u8fc7\u5f15\u5165 prometheus cleint_golang \u4f9d\u8d56\uff0c\u5728\u903b\u8f91\u4e2d\u57cb\u70b9\u6307\u6807\u4fe1\u606f\uff0c\u5373\u53ef\u81ea\u52a8\u66b4\u9732 /metrics \u6307\u6807\u63a5\u53e3\uff0c\u901a\u8fc7 prometheus \u6536\u96c6\u6307\u6807\uff0c\u5728 grafana \u4e2d\u5c31\u53ef\u4ee5\u5b9a\u4e49\u770b\u677f\uff0c\u5c55\u793a\u7cfb\u7edf\u5b9e\u65f6\u72b6\u6001\u3002\nshare.suazr.cn/ArTicle/712078.Pdf\nshare.fmxdc.cn/ArTicle/623583.Pdf\nshare.abbrl.cn/ArTicle/657354.Pdf\nshare.wclvy.cn/ArTicle/846250.Pdf\nshare.hvikp.cn", "score": 0, "created": "2026-02-24T07:22:56Z", "url": "https://github.com/damikhade/j7w8/issues/463"}, {"source": "GitHub", "text": "feat: add GLM-specific compaction settings GLM was sharing clai's compaction defaults (raw_max_tokens=5000, compact_max_tokens=2000), resulting in only ~7-8K of channel context in a 64K window.\n\nAdds GLM-specific settings configurable via `!glmconfig`:\n- `glm_raw_max_tokens`: 10000 (was 5000)\n- `glm_compact_max_tokens`: 4000 (was 2000)  \n- `glm_raw_hours`: 6\n- `glm_compact_days`: 7\n- `glm_recompact_raw_tokens`: 6000 (was 3000)\n\nWhen `build_full_context` is called with `compact_model` (GLM path),", "score": 0, "created": "2026-02-24T07:22:38Z", "url": "https://github.com/iamsix/palbot/pull/192"}, {"source": "GitHub", "text": "feat: add GLM/Gemma local LLM electricity cost pricing Adds electricity-based cost calculation for local GLM/Gemma models on RTX 4090.\n\n- $0.50/M input tokens, $3.00/M output tokens, $0 cached\n- Based on ~750W system draw, $0.40/kWh, ~100 t/s generation\n- Shows in debug output alongside cloud model costs", "score": 0, "created": "2026-02-24T06:59:36Z", "url": "https://github.com/iamsix/palbot/pull/190"}, {"source": "GitHub", "text": "Max395(ubuntu24.04)AMD\u663e\u5361GLM-4.7-UD-IQ1-M\u91cf\u5316\u6a21\u578b\u90e8\u7f72\u624b\u518c\u81ea\u8d21\u8d21\u4e95Ad \u76ee\u6807\uff1a\n\u4e3a anylink \u589e\u52a0\u53ef\u89c2\u6d4b\u6027\u652f\u6301\uff0c\u63d0\u4f9b\u5b8c\u6574\u7684\u53ef\u89c2\u6d4b\u6027\uff08\u65e5\u5fd7\uff0c\u6307\u6807\uff0c\u94fe\u8def\u8ffd\u8e2a\uff09\u3002\n\n\u5b9e\u73b0\u65b9\u5f0f\uff1a\n\u57fa\u4e8e OpenTelemetry \u89c4\u8303\uff0c\u5b9e\u73b0 OTLP \u683c\u5f0f\u7684\u6807\u51c6\u53ef\u89c2\u6d4b\u6027\u652f\u6301\uff0c\u80fd\u591f\u7075\u6d3b\u63a5\u5165\u4e30\u5bcc\u7684\u53ef\u89c2\u6d4b\u6027\u6280\u672f\u6808\u3002\u800c\u4e14\u4f9d\u7136\u53ef\u4ee5\u901a\u8fc7\u914d\u5408 otel prometheus-exporter \u65e0\u7f1d\u517c\u5bb9\u5f53\u524d\u4e3b\u6d41\u57fa\u4e8e prometheus \u6307\u6807\u6280\u672f\u6808\u3002\n\n\u5feb\u901f\u5b9e\u73b0\uff1a\n\u901a\u8fc7\u5f15\u5165 prometheus cleint_golang \u4f9d\u8d56\uff0c\u5728\u903b\u8f91\u4e2d\u57cb\u70b9\u6307\u6807\u4fe1\u606f\uff0c\u5373\u53ef\u81ea\u52a8\u66b4\u9732 /metrics \u6307\u6807\u63a5\u53e3\uff0c\u901a\u8fc7 prometheus \u6536\u96c6\u6307\u6807\uff0c\u5728 grafana \u4e2d\u5c31\u53ef\u4ee5\u5b9a\u4e49\u770b\u677f\uff0c\u5c55\u793a\u7cfb\u7edf\u5b9e\u65f6\u72b6\u6001\u3002\nshare.vjjxa.cn/ArTicle/655131.Pdf\nshare.enaex.cn/ArTicle/387487.Pdf\nshare.pvibz.cn/ArTicle/925691.Pdf\nshare.lnmca.cn/ArTicle/046812.Pdf\nshare.yknvl.cn", "score": 0, "created": "2026-02-24T06:57:41Z", "url": "https://github.com/sicadmon/mklb/issues/369"}, {"source": "GitHub", "text": "fix: genericize debug output, independent glm toggle ## Summary\n\nFollow-up to PR #188 (context-gatherer refactor). Fixes that didn't make it into the merge:\n\n### Changes\n\n**Genericized debug output:**\n- Added `_should_debug()`, `_add_usage_debug()`, `_send_with_debug()` helper methods\n- All three commands (`!clai`, `!sclai`, `!glm`) now share the same debug formatting\n- `!glm` now shows debug line (tokens, cost, elapsed, model) \u2014 previously had none\n\n**Independent toggle:**\n- `!glm` only checks ", "score": 0, "created": "2026-02-24T06:51:00Z", "url": "https://github.com/iamsix/palbot/pull/189"}, {"source": "GitHub", "text": "[megatron] support GLM-5 megatron ", "score": 1, "created": "2026-02-24T06:49:44Z", "url": "https://github.com/modelscope/ms-swift/pull/8085"}, {"source": "GitHub", "text": "Max395(ubuntu24.04)AMD\u663e\u5361GLM-4.7-UD-IQ1-M\u91cf\u5316\u6a21\u578b\u90e8\u7f72\u624b\u518c\u94c1\u5cad\u660c\u56feDO \u76ee\u6807\uff1a\n\u4e3a anylink \u589e\u52a0\u53ef\u89c2\u6d4b\u6027\u652f\u6301\uff0c\u63d0\u4f9b\u5b8c\u6574\u7684\u53ef\u89c2\u6d4b\u6027\uff08\u65e5\u5fd7\uff0c\u6307\u6807\uff0c\u94fe\u8def\u8ffd\u8e2a\uff09\u3002\n\n\u5b9e\u73b0\u65b9\u5f0f\uff1a\n\u57fa\u4e8e OpenTelemetry \u89c4\u8303\uff0c\u5b9e\u73b0 OTLP \u683c\u5f0f\u7684\u6807\u51c6\u53ef\u89c2\u6d4b\u6027\u652f\u6301\uff0c\u80fd\u591f\u7075\u6d3b\u63a5\u5165\u4e30\u5bcc\u7684\u53ef\u89c2\u6d4b\u6027\u6280\u672f\u6808\u3002\u800c\u4e14\u4f9d\u7136\u53ef\u4ee5\u901a\u8fc7\u914d\u5408 otel prometheus-exporter \u65e0\u7f1d\u517c\u5bb9\u5f53\u524d\u4e3b\u6d41\u57fa\u4e8e prometheus \u6307\u6807\u6280\u672f\u6808\u3002\n\n\u5feb\u901f\u5b9e\u73b0\uff1a\n\u901a\u8fc7\u5f15\u5165 prometheus cleint_golang \u4f9d\u8d56\uff0c\u5728\u903b\u8f91\u4e2d\u57cb\u70b9\u6307\u6807\u4fe1\u606f\uff0c\u5373\u53ef\u81ea\u52a8\u66b4\u9732 /metrics \u6307\u6807\u63a5\u53e3\uff0c\u901a\u8fc7 prometheus \u6536\u96c6\u6307\u6807\uff0c\u5728 grafana \u4e2d\u5c31\u53ef\u4ee5\u5b9a\u4e49\u770b\u677f\uff0c\u5c55\u793a\u7cfb\u7edf\u5b9e\u65f6\u72b6\u6001\u3002\nread.eqrkg.cn/ArTicle/876263.Pdf\nread.emwbg.cn/ArTicle/871770.Pdf\nread.cexyj.cn/ArTicle/654031.Pdf\nread.xruam.cn/ArTicle/882850.Pdf\nread.zdheu.cn/ArTi", "score": 0, "created": "2026-02-24T06:48:12Z", "url": "https://github.com/thesusweld/immq/issues/157"}, {"source": "GitHub", "text": "Max395(ubuntu24.04)AMD\u663e\u5361GLM-4.7-UD-IQ1-M\u91cf\u5316\u6a21\u578b\u90e8\u7f72\u624b\u518c\u8fbd\u9633\u5f13\u957f\u5cadfk \u76ee\u6807\uff1a\n\u4e3a anylink \u589e\u52a0\u53ef\u89c2\u6d4b\u6027\u652f\u6301\uff0c\u63d0\u4f9b\u5b8c\u6574\u7684\u53ef\u89c2\u6d4b\u6027\uff08\u65e5\u5fd7\uff0c\u6307\u6807\uff0c\u94fe\u8def\u8ffd\u8e2a\uff09\u3002\n\n\u5b9e\u73b0\u65b9\u5f0f\uff1a\n\u57fa\u4e8e OpenTelemetry \u89c4\u8303\uff0c\u5b9e\u73b0 OTLP \u683c\u5f0f\u7684\u6807\u51c6\u53ef\u89c2\u6d4b\u6027\u652f\u6301\uff0c\u80fd\u591f\u7075\u6d3b\u63a5\u5165\u4e30\u5bcc\u7684\u53ef\u89c2\u6d4b\u6027\u6280\u672f\u6808\u3002\u800c\u4e14\u4f9d\u7136\u53ef\u4ee5\u901a\u8fc7\u914d\u5408 otel prometheus-exporter \u65e0\u7f1d\u517c\u5bb9\u5f53\u524d\u4e3b\u6d41\u57fa\u4e8e prometheus \u6307\u6807\u6280\u672f\u6808\u3002\n\n\u5feb\u901f\u5b9e\u73b0\uff1a\n\u901a\u8fc7\u5f15\u5165 prometheus cleint_golang \u4f9d\u8d56\uff0c\u5728\u903b\u8f91\u4e2d\u57cb\u70b9\u6307\u6807\u4fe1\u606f\uff0c\u5373\u53ef\u81ea\u52a8\u66b4\u9732 /metrics \u6307\u6807\u63a5\u53e3\uff0c\u901a\u8fc7 prometheus \u6536\u96c6\u6307\u6807\uff0c\u5728 grafana \u4e2d\u5c31\u53ef\u4ee5\u5b9a\u4e49\u770b\u677f\uff0c\u5c55\u793a\u7cfb\u7edf\u5b9e\u65f6\u72b6\u6001\u3002\nh3.efdsc.cn/ArTicle/137401.Pdf\nh3.jzrsg.cn/ArTicle/318984.Pdf\nh3.qjrcy.cn/ArTicle/964520.Pdf\nh3.rayks.cn/ArTicle/975784.Pdf\nh3.fohss.cn/ArTicle/97489", "score": 0, "created": "2026-02-24T06:47:47Z", "url": "https://github.com/thesusweld/immq/issues/145"}, {"source": "GitHub", "text": "Fix: Add missing glm-cn provider definition for Web UI discovery ## Problem\r\n\r\nThe \"GLM (China)\" option is not visible in Web UI, preventing users from selecting this provider.\r\n\r\n## Root Cause\r\n\r\nThe `APIKEY_PROVIDERS` object in `src/shared/constants/providers.js` was missing the `glm-cn` definition, even though `open-sse/config/providerModels.js` contains glm-cn model definitions.\r\n\r\n## Fix\r\n\r\nAdded `glm-cn` provider definition to `APIKEY_PROVIDERS`:\r\n\r\n```javascript\r\n\"glm-cn\": { id: \"glm-cn\",", "score": 0, "created": "2026-02-24T04:45:54Z", "url": "https://github.com/decolua/9router/pull/186"}]}