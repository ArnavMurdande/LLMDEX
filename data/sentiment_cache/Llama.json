{"timestamp": 1771962999.8716502, "mentions": [{"source": "Reddit", "text": "Connected LFM2.5-VL-1.6B to my Blink security camera \u2014 51 tokens/sec with APPLE GPU I've tested a lot of local VLMs for security camera analysis \u2014 SmolVLM2, Qwen3-VL, MiniCPM-V, LLaVA.\n\nLFM2.5-VL-1.6B from LiquidAI is the one I keep coming back to. Here's why.\n\n**One example output:**\n\n&gt;\"A mailman is delivering mail to a suburban house. The mailman is wearing a blue uniform and carrying a white mail bag. The house is white with a brown roof, and there's a driveway with a black car parked in f", "score": 11, "created": 1771961243.0, "url": "https://reddit.com/r/LocalLLaMA/comments/1rdpz30/connected_lfm25vl16b_to_my_blink_security_camera/"}, {"source": "Reddit", "text": "Engineering the Autonomous Local Enterprise: A Technical Blueprint for Agentic RAG and Sovereign AI Infrastructure # Engineering the Autonomous Local Enterprise: A Technical Blueprint for Agentic RAG and Sovereign AI Infrastructure\n\nThe transition from reactive large language model applications to autonomous agentic workflows represents a fundamental paradigm shift in enterprise computing. In the 2025\u20132026 technological landscape, the industry has moved beyond simple chat interfaces toward syste", "score": 1, "created": 1771960677.0, "url": "https://reddit.com/r/OpenAI/comments/1rdppfn/engineering_the_autonomous_local_enterprise_a/"}, {"source": "Reddit", "text": "Frontier LLM Leaderboard Check it out at [https://www.onyx.app/llm-leaderboard](https://www.onyx.app/llm-leaderboard)", "score": 0, "created": 1771959166.0, "url": "https://reddit.com/r/OpenAI/comments/1rdozjs/frontier_llm_leaderboard/"}, {"source": "Reddit", "text": "Creating a chat bot for people who want to extract info from studies fast, or to give to people to convince them seed oils are bad An issue I have is that I am not going to peruse documents upon documents to find and cite something I want. I also do not want to ask Gemini or ChatGPT about seed oils, it doesn't have enough niche and precise knowledge, nor is it trustworthy since LLMs can hallucinate.\n\nSo, I will be making a niche chatbot trained specifically on well labeled data points (i.e. stud", "score": 0, "created": 1771958867.0, "url": "https://reddit.com/r/StopEatingSeedOils/comments/1rdou9q/creating_a_chat_bot_for_people_who_want_to/"}, {"source": "Reddit", "text": "The Stacked S-Curve: Why the AI plateau is actually a trap People are starting to whisper that [AI is hitting an asymptote.](https://www.reddit.com/r/LlamaFarm/comments/1rdkpil/the_asymptote_of_ai_why_software_builders_arent/) The models aren't leaping forward every single week. Code generation is incredible, but the productivity gains are starting to level off.\n\nThinking this is the end of the AI revolution is a massive trap.\n\nTechnology does not follow a single curve. [It follows stacked S-cur", "score": 2, "created": 1771957088.0, "url": "https://reddit.com/r/LlamaFarm/comments/1rdo0aw/the_stacked_scurve_why_the_ai_plateau_is_actually/"}, {"source": "Reddit", "text": "Connected Qwen3-VL-2B-Instruct to my security cameras, result is great Just tried the new Qwen3-VL-2B-Instruct (Unsloth GGUF) on my security camera feeds\n\n**The output:**\n\n&gt;\"A mailman is delivering mail to a suburban house. The mailman is wearing a blue uniform and carrying a white mail bag. The house is white with a brown roof, and there's a driveway with a black car parked in front. The mailman is walking on a brick path surrounded by green bushes and trees.\"\n\nFor a 2B model at IQ2 quantiza", "score": 22, "created": 1771957030.0, "url": "https://reddit.com/r/Qwen_AI/comments/1rdnzbe/connected_qwen3vl2binstruct_to_my_security/"}, {"source": "Reddit", "text": "Stop freezing your UI thread: I built a managed C++ runtime for Flutter that runs 43 tok/s LLMs in background isolates I have seen too many flutter AI plugin thata re just thing FFi wrappers. they look great in a CLI, but the moment you put them in an app, the UI locks up the process and it crashes after 60 seconds.\n\nI built [Edge Veda](https://github.com/ramanujammv1988/edge-veda) to fix this . it's a supervised runtime designed for behavior over time, not just benchmark bursts.\n\n**How it handl", "score": 0, "created": 1771955931.0, "url": "https://reddit.com/r/FlutterDev/comments/1rdng6n/stop_freezing_your_ui_thread_i_built_a_managed_c/"}, {"source": "Reddit", "text": "Ai news update \ud83d\udd25\n\n1\t\\\\\\[\\\\\\~845k likes | @OpenAI\\\\\\]\n\nOpenAI launches GPT-5 Turbo \u2014 optimized version of GPT-5 with 2\u00d7 faster inference, 512k context, and major gains in cost-efficiency for API use; now live for all ChatGPT Plus/Team users and API.\n\n\u2192 [ https://x.com/OpenAI/status/2029123987654321098 ](https://x.com/OpenAI/status/2029123987654321098)\n\n2\t\\\\\\[\\\\\\~512k likes | @AnthropicAI\\\\\\]\n\nAnthropic releases Claude 5.5 Opus \u2014 frontier reasoning model with 1M context, enhanced long-horizon plan", "score": 1, "created": 1771955405.0, "url": "https://reddit.com/r/AIPulseDaily/comments/1rdn7fs/ai_news_update/"}, {"source": "Reddit", "text": "\"Agentic Gaming\" \u2014 a deep dive into how I'm using LLMs as a semantic reasoning layer inside an RPG engine (80+ orchestrated AI tasks, multi-LLM, genre-agnostic skills, and a lot of dice rolls) Hi everyone!\n\nI've been working on something for a while now that I think sits squarely in the intersection of this sub's interests, and I wanted to share it \u2014 not just as a project showcase, but because I genuinely want to discuss the underlying design concepts with people who think about AI + game design", "score": 4, "created": 1771954177.0, "url": "https://reddit.com/r/aigamedev/comments/1rdmmfa/agentic_gaming_a_deep_dive_into_how_im_using_llms/"}, {"source": "Reddit", "text": "Intel y NVIDIA refuerzan sus equipos para mejorar el soporte gr\u00e1fico en Linux https://preview.redd.it/qytehjhf7hlg1.png?width=1200&amp;format=png&amp;auto=webp&amp;s=a28bb2cdc6856304f1ab6bba49d1c73b81ca90bd\n\nEl\u00a0**Linux Gaming**, al menos en lo que respecta a las gr\u00e1ficas, ha sido un terreno en el que AMD se ha mostrado cada vez m\u00e1s dominante gracias a su combinaci\u00f3n de buen soporte, integraci\u00f3n y contribuci\u00f3n por parte de Valve. Afortunadamente, parece que\u00a0**Intel y NVIDIA**\u00a0tienen intenciones d", "score": 1, "created": 1771953531.0, "url": "https://reddit.com/r/Ubuntu_ES18011979/comments/1rdmbpy/intel_y_nvidia_refuerzan_sus_equipos_para_mejorar/"}, {"source": "Reddit", "text": "trying to convince llama llama3.2:1b its actually 2026 https://preview.redd.it/6ensrpst5hlg1.png?width=1920&amp;format=png&amp;auto=webp&amp;s=8d5b1ed8bfa8c4cb01f12256fdee3cfdb320483d\n\nold models are funny", "score": 2, "created": 1771953028.0, "url": "https://reddit.com/r/LocalLLaMA/comments/1rdm36b/trying_to_convince_llama_llama321b_its_actually/"}, {"source": "Reddit", "text": "Library of prompts for AI models [github](https://github.com/f/prompts.chat)", "score": 2, "created": 1771951964.0, "url": "https://reddit.com/r/AIAgentsInAction/comments/1rdlkki/library_of_prompts_for_ai_models/"}, {"source": "Reddit", "text": "Stop treating every bad answer as hallucination: a 16-problem atlas for n8n AI automation hi, I am PSBigBig, i have been using n8n as my \u201cglue\u201d for LLM work: cron jobs that refresh docs, RAG style question answering, Slack bots, small internal agents. after enough broken flows and weird \u201challucinations\u201d, I finally gave up on ad-hoc debugging and built a fixed 16-problem \u201cfailure map\u201d for RAG and agent workflows.\n\nthis post is not about selling a new tool. it is about sharing how those 16 failure", "score": 1, "created": 1771948145.0, "url": "https://reddit.com/r/n8n/comments/1rdjt87/stop_treating_every_bad_answer_as_hallucination_a/"}, {"source": "Reddit", "text": "Liquid AI releases LFM2-24B-A2B Today, Liquid AI releases LFM2-24B-A2B, their largest LFM2 model to date\n\nLFM2-24B-A2B is a sparse Mixture-of-Experts (MoE) model with 24 billion total parameters with 2 billion active per token, showing that the LFM2 hybrid architecture scales effectively to larger sizes maintaining quality without inflating per-token compute.\n\nThis release expands the LFM2 family from 350M to 24B parameters, demonstrating predictable scaling across nearly two orders of magnitude", "score": 168, "created": 1771944213.0, "url": "https://reddit.com/r/LocalLLaMA/comments/1rdi26s/liquid_ai_releases_lfm224ba2b/"}, {"source": "Reddit", "text": "Lessons learned running Qwen3-VL-8B as a fully local voice assistant on AMD ROCm I've been building a local voice assistant over the past few weeks and wanted to share some things I learned that might be useful to others here, especially anyone on AMD hardware.\n\nThe setup is wake word \u2192 fine-tuned Whisper STT \u2192 Qwen3-VL-8B for reasoning \u2192 Kokoro TTS for voice output. Everything runs on-device, no cloud APIs in the loop.\n\n# Things that surprised me\n\n**Self-quantizing beats downloading pre-made qu", "score": 22, "created": 1771942010.0, "url": "https://reddit.com/r/LocalLLaMA/comments/1rdh5lv/lessons_learned_running_qwen3vl8b_as_a_fully/"}, {"source": "Reddit", "text": "what it feels like to throw civilization-scale questions at WFGY 3.0 in the first post i used WFGY 3.0 on personal life problems. in the second post i pointed it at broken RAG stacks and incidents.\n\nthis one is about the last layer in that ladder:\n\n&gt;what happens when you feed the TXT with questions about work, power, climate, governance, and the kind of civilization we are building.\n\nthis is not a manifesto and not a prediction. it is a field report: three simulated runs that show how the eng", "score": 1, "created": 1771940615.0, "url": "https://reddit.com/r/WFGY/comments/1rdgl7x/what_it_feels_like_to_throw_civilizationscale/"}, {"source": "Reddit", "text": "Stop treating every bug as \u2018hallucination\u2019: a 16-problem atlas for Ollama + RAG hi, i\u2019ve been building local RAG / agent stacks on top of Ollama for a while, and at some point I got tired of describing every bug as \u201challucination\u201d or \u201cRAG is trash\u201d. so I did something slightly obsessive: I turned all the weird failure patterns into a fixed 16-slot \u201cProblem Map\u201d that I now use to debug my pipelines.\n\nthis map is now referenced in a few places (including the LlamaIndex RAG troubleshooting docs), b", "score": 2, "created": 1771939164.0, "url": "https://reddit.com/r/ollama/comments/1rdg1a8/stop_treating_every_bug_as_hallucination_a/"}, {"source": "Reddit", "text": "ive had it with this quota thingy what i need to do ??? ne,\n\nI\u2019m currently building some projects using tools like AntiGravity, OpenCode, and OpenClaw, but I\u2019m hitting a wall with my setup and could really use some advice from the community.\n\nMy goal is to have a strong, reliable model (like Claude Opus 4.6 or Sonnet 4.5) as my daily driver for the long term. The problem is, I\u2019m constantly running into rate limits and context window issues.\n\nHere is my dilemma:\n\n1. **Free Tiers/Basic Plans:** I ", "score": 3, "created": 1771938729.0, "url": "https://reddit.com/r/google_antigravity/comments/1rdfv8x/ive_had_it_with_this_quota_thingy_what_i_need_to/"}, {"source": "Reddit", "text": "\"I\u2019m joining OpenAI to work on World Simulation and Robotics, after 3.75 years at FAIR working on SAM and Llama. I\u2019m thrilled to explore how visual perception, world model and robotics can come together to build physical intelligence.\" - Seems like OpenAI is expanding into Robotics and World Models ", "score": 1, "created": 1771938348.0, "url": "https://reddit.com/r/LovingAGI/comments/1rdfq0i/im_joining_openai_to_work_on_world_simulation_and/"}, {"source": "Reddit", "text": "15 Generative AI Consulting Companies Dominating 2026 (After Months of Research, Here\u2019s What I Found) Generative AI is no longer a futuristic experiment. In 2026, it\u2019s driving real business transformation from AI copilots and automated content engines to enterprise-grade AI agents and workflow automation. But here\u2019s what most businesses underestimate: choosing the right Generative AI consulting company is far more important than choosing the model itself.\n\nOver the last few months, I\u2019ve analyzed", "score": 1, "created": 1771936986.0, "url": "https://reddit.com/r/SaaS/comments/1rdf88n/15_generative_ai_consulting_companies_dominating/"}, {"source": "Reddit", "text": "I got sick of checking 5 different pricing pages every time I needed to pick an AI model, so I built a free calculator that compares them all at once Every time I start a new project, the same ritual. Open OpenAI's pricing page, then Anthropic's, then Google's, then do janky math in a spreadsheet to figure out which model actually fits my budget. Half the time the pricing pages have changed since the last time I looked.\n\nSo I built a free tool that does all of it in one place:\u00a0[**apicostcompare.", "score": 1, "created": 1771936920.0, "url": "https://reddit.com/r/SaaS/comments/1rdf7f4/i_got_sick_of_checking_5_different_pricing_pages/"}, {"source": "Reddit", "text": "A 16-problem RAG failure map that LlamaIndex just adopted (semantic firewall, MIT, step-by-step examples) hi, this is my first post here. i am the author of an open source \u201cProblem Map\u201d for RAG and agents that LlamaIndex recently adopted into its RAG troubleshooting docs as a structured failure-mode checklist.\n\ni wanted to share it here in a more practical way, with concrete **LlamaIndex** examples and not just a link drop.\n\n# 0. link first, so you can skim while reading\n\nthe full map lives here", "score": 1, "created": 1771936697.0, "url": "https://reddit.com/r/LlamaIndex/comments/1rdf4lq/a_16problem_rag_failure_map_that_llamaindex_just/"}, {"source": "Reddit", "text": "Self-hosted AI toolkit: 9 Bash/Python tools for local LLM workflows (memory, loops, automation)\u2014no cloud, no APIs, just llama.cpp + shell Built an integrated toolkit for self-hosting AI workflows without cloud dependencies. \n\nEverything runs locally with llama.cpp as the LLM engine and Bash as the orchestration layer.\n\n\n\nGitHub: [https://github.com/BiblioGalactic](https://github.com/BiblioGalactic)\n\n\n\nWhy this exists:\n\nMost local AI guides assume you want Ollama/LM Studio GUI apps. I needed \n\nsc", "score": 1, "created": 1771936659.0, "url": "https://reddit.com/r/u_KitchenCat5603/comments/1rdf45c/selfhosted_ai_toolkit_9_bashpython_tools_for/"}, {"source": "Reddit", "text": "OCTAVE-MCP v1.5.0 - Language and compression for LLMs: Here's what's new So I posted about [OCTAVE v1.0.0](https://www.reddit.com/r/ClaudeCode/comments/1qre6zq/update_octave_mcp_v100_a_semantic/) back in January. I'll concede it's much easier to just let Claude explain it, so the remainder of this post is copy/paste so I'll get over the fact it's written it in \"American\" when I'm from London, but any actual questions, I can answer. I understand what I've built pretty well and can actually read t", "score": 1, "created": 1771935485.0, "url": "https://reddit.com/r/ClaudeCode/comments/1rdepzf/octavemcp_v150_language_and_compression_for_llms/"}, {"source": "Reddit", "text": "MI210 in Supermicro U3 Chassis for local AI **So I finally got this GPU working without overheating. It was a long way so to help others which want to archive something similar here are my experiences.**\n\n**1. Installing Hardware:**\n\n* make sure the card fits and enough cooling is supplied. I had to print a separate fan holder (This helped me a lot [printables](https://www.printables.com/model/1479089-amd-mi50-mi100-m210-gpu-80mm-fan-cooling-attachmen?lang=de) \\- had to adjust it to my chassis s", "score": 0, "created": 1771935064.0, "url": "https://reddit.com/r/homelab/comments/1rdekzi/mi210_in_supermicro_u3_chassis_for_local_ai/"}, {"source": "Reddit", "text": "\ud83d\udc4bWelcome to r/0penAGI - Introduce Yourself and Read First! Hey everyone! I'm u/VastSolid5772, a founding moderator of r/0penAGI.\n\nThis is our new home for all things related to \\*\\*open-source AGI research, on-device/local AI experiments, autonomous agents, emotional/cognitive architectures, memory systems, and privacy-first AI development\\*\\*. We're excited to have you join us!\n\nWhether you're hacking on projects like VALIS (our on-device iOS AI companion with plastic brain memory, emotions, an", "score": 1, "created": 1771934837.0, "url": "https://reddit.com/r/0penAGI/comments/1rdeie2/welcome_to_r0penagi_introduce_yourself_and_read/"}, {"source": "Reddit", "text": "Qwen3.5-397B-A17B-UD-TQ1 bench results FW Desktop Strix Halo 128GB Just sharing the bench results for unsloth Qwen3.5-397B-A17B-UD-TQ1 on my FW desktop with 128GB VRAM", "score": 44, "created": 1771934559.0, "url": "https://reddit.com/r/LocalLLaMA/comments/1rdef9x/qwen35397ba17budtq1_bench_results_fw_desktop/"}, {"source": "Reddit", "text": "VALIS: Open-Source On-Device AI Chat App for iOS with Memory, Emotions, and Tools I came across this cool open-source project called VALIS (Vast Active Living Intelligence System)  \u2013 (Philip K. Dick?) it's a fully offline AI chat app for iOS that runs local LLMs right on your device. It's built with SwiftUI and uses llama.cpp for inference with GGUF models. The neat part is it has a \"plastic brain\" system that adapts over time with memories, emotions, experiences, and even lightweight tools.\n\nPr", "score": 0, "created": 1771933844.0, "url": "https://reddit.com/r/LocalLLaMA/comments/1rde4nn/valis_opensource_ondevice_ai_chat_app_for_ios/"}, {"source": "Reddit", "text": "I built an embeddable AI inference runtime, no server, no API keys, everything runs on-device I wanted to add AI to my apps without sending user data to a third party. I needed inference to stay on the device.\n\nSo I built Xybrid. A Rust runtime that embeds directly into your app process.\n\nLLMs, text-to-speech, speech recognition, all running locally in just three lines of code:\n\n```dart\nfinal model = await Xybrid.model(modelId: 'llama-3.2-1b').load();\nfinal input = Envelope.text(text: 'Explain q", "score": 7, "created": 1771933038.0, "url": "https://reddit.com/r/FlutterDev/comments/1rdduzx/i_built_an_embeddable_ai_inference_runtime_no/"}, {"source": "Reddit", "text": "Overview of Ryzen AI 395+ hardware? Is there an overview who has them and what they are good/bad at? I want to buy one as a llama.cpp (and Proxmox) box to replace my old homeserver, but have yet to find a comparison or even market overview.", "score": 5, "created": 1771932249.0, "url": "https://reddit.com/r/LocalLLaMA/comments/1rddmj0/overview_of_ryzen_ai_395_hardware/"}, {"source": "Reddit", "text": "Karma police Anthropic accused DeepSeek, Moonshot, and MiniMax of \"industrial-scale distillation\" of Claude. LoL. Ok, the Chinese are the Robin Hoods of AI - they take closed frontier models, distill them, and give them to the public for free.\n\nNow the billion-dollar question: what can Anthropic do next?\n\n**Option A (bad):** Run with their ass on fire to Congress/the courts and start pushing for a \"ban on public models,\" \"distillation regulation,\" and \"export controls on open-weights.\" Result - ", "score": 0, "created": 1771930601.0, "url": "https://reddit.com/r/ClaudeAI/comments/1rdd4t2/karma_police/"}, {"source": "Reddit", "text": "Would 64 gb of ram on an M4 Mac mini pro be enough to host openclaw and run a llm like Llama at the same time? I have been asking Gemini and Claud this but am skeptical of there answers in this one. They reakon 48 gb is best in terms of cost but I don\u2019t want to be restricted. \n\nI am keen to run my own LLM if possible so I don\u2019t have ongoing costs and just have the initial cost and the electricity usage. \n\nI\u2019m sure people here have actually got setups working so I figured I would ask you first.\n\n", "score": 6, "created": 1771930422.0, "url": "https://reddit.com/r/openclaw/comments/1rdd314/would_64_gb_of_ram_on_an_m4_mac_mini_pro_be/"}, {"source": "Reddit", "text": "how WFGY 3.0 feels when you aim it at a broken AI pipeline I rewrote the **WFGY 3.0 \u00b7 Singularity Demo** TXT again, and this time i want to talk less about life questions and more about the thing most of us actually fight with every day:\n\nRAG stacks that look fine on paper and still behave like chaos.\n\nthis post is about what it feels like to point the 3.0 TXT at **engineering problems**: off-topic RAG, incidents nobody understands, and endless arguments about \u201challucination\u201d. it is not a full s", "score": 1, "created": 1771928732.0, "url": "https://reddit.com/r/WFGY/comments/1rdcma7/how_wfgy_30_feels_when_you_aim_it_at_a_broken_ai/"}, {"source": "Reddit", "text": "Tired of managing 5 different local LLM URLs? I\u2019m building \"Proxmox for LLM servers\" (llm.port) The current state of local AI is a mess. You have one server running vLLM, a Mac Studio running llama.cpp, and a fallback to OpenAI\u2014all with different keys and endpoints.\n\nI\u2019m building llm.port to fix this. It\u2019s a self-hosted AI Gateway + Ops Console that gives you one OpenAI-compatible endpoint (/v1/\\*) to rule them all.\n\n**What it does:**\n\nUnified API: Routes to local runtimes (vLLM, etc.) and remot", "score": 0, "created": 1771925011.0, "url": "https://reddit.com/r/LocalLLM/comments/1rdbm59/tired_of_managing_5_different_local_llm_urls_im/"}, {"source": "Reddit", "text": "OpenClaw is a MESS!!! did anyone actually securing AI traffic at scale? Teams quietly adopted OpenClaw for cheap local Llama 3.1 inference and now some of them are dealing with actual breaches.\n\nZeroLeaks scored it 2/100. Giskard confirmed cross user data exfil and credential theft triggered by a single malicious email or skill. Shodan found 135k exposed instances across 82 countries with 12k+ having RCE exposure. The Supabase databases had no Row Level Security meaning full chat histories and t", "score": 167, "created": 1771924155.0, "url": "https://reddit.com/r/sysadmin/comments/1rdbe61/openclaw_is_a_mess_did_anyone_actually_securing/"}, {"source": "Reddit", "text": "Hardware recommendation Notebook for local development Hello, I am looking for a laptop for local execution of AI models (LLMs such as Llama 3/Phi-4 and image generation via Stable Diffusion). My budget is \u20ac2,500.\n\nI am trying to familiarize myself with the topic, but I need advice on what is important in this area.\n\nWhat exactly I want to do: Primarily coding, probably with QWEN. Video, image, text, and voice processing.\n\nI know that smaller models can be made with \u201cless powerful\u201d laptops. But ", "score": 2, "created": 1771920969.0, "url": "https://reddit.com/r/AI_developers/comments/1rdakaj/hardware_recommendation_notebook_for_local/"}, {"source": "Reddit", "text": "Best practices for running local LLMs for ~70\u2013150 developers (agentic coding use case) Hi everyone,\n\nI\u2019m planning infrastructure for a software startup where we want to use **local LLMs for agentic coding workflows** (code generation, refactoring, test writing, debugging, PR reviews, etc.).\n\n# Scale\n\n* Initial users: \\~70\u2013100 developers\n* Expected growth: up to \\~150 users\n* Daily usage during working hours (8\u201310 hrs/day)\n* Concurrent requests likely during peak coding hours\n\n# Use Case\n\n* Agent", "score": 21, "created": 1771917330.0, "url": "https://reddit.com/r/LocalLLaMA/comments/1rd9kpk/best_practices_for_running_local_llms_for_70150/"}, {"source": "Reddit", "text": "what it feels like to throw your own life problems at WFGY 3.0 I just pushed a rewritten version of the **WFGY 3.0 \u00b7 Singularity Demo** TXT engine to the repo. Before we point it at RAG incidents or civilization questions, I want to start with the most honest thing people will probably do first:\n\nYou load the TXT into a strong model, type `run`, then `go`, and you ask about your own life.\n\nThis post is an attempt to show what that experience actually feels like. Not the math, not the diagrams, j", "score": 1, "created": 1771917150.0, "url": "https://reddit.com/r/WFGY/comments/1rd9ixz/what_it_feels_like_to_throw_your_own_life/"}, {"source": "Reddit", "text": "llama-cpp-python 0.3.16 \u2013 Qwen3 Embedding GGUF fails with \"invalid seq_id &gt;= 1\" when batching I\u2019m trying to use batched embeddings with a GGUF model and hitting a sequence error.\n\n# Environment\n\n* OS: Ubuntu 24.04\n* GPU: RTX 4060\n* llama-cpp-python: 0.3.16\n* Model: Qwen3-Embedding-4B-Q5\\_K\\_M.gguf\n\nModel loads fine and single-input embeddings work.\n\nbut not multiple string\n\n\n\n`from llama_cpp import Llama`\n\n`llm = Llama(`\n\n`model_path=\"Qwen3-Embedding-4B-Q5_K_M.gguf\",`\n\n`embedding=True,`\n\n`)`\n", "score": 4, "created": 1771917149.0, "url": "https://reddit.com/r/LocalLLaMA/comments/1rd9ixh/llamacpppython_0316_qwen3_embedding_gguf_fails/"}, {"source": "Reddit", "text": "How can I build an offline LLM for mobile? Looking for guidance &amp; best practices Hi everyone,\n\nI\u2019m looking for guidance on running an LLM fully offline on a mobile device (Android/iOS).\n\n* Best models for mobile? (3B\u20137B?)\n* Is 4-bit/8-bit quantization enough?\n* Recommended frameworks (llama.cpp, ONNX, Core ML, etc.)?\n* Any real-world performance tips?\n\nIf you\u2019ve built or tested this, I\u2019d really appreciate your insights.", "score": 2, "created": 1771916753.0, "url": "https://reddit.com/r/ChatGPTPro/comments/1rd9ezk/how_can_i_build_an_offline_llm_for_mobile_looking/"}, {"source": "Reddit", "text": "[AskDevs] Building a Real-Time \"Hinglish\" Dubbing App (Zero-Cost Stack) - Architecture &amp; Model Review Needed Hey everyone!\n\nI'm working on an indie project called\u00a0**\"Connect\"**\u00a0\u2013 a real-time video dubbing application specifically for Indian audiences. The goal is to take English movies/videos and dub them into\u00a0**\"Urban Hinglish\"**\u00a0(the casual Mumbai/Delhi slang we actually speak, not textbook Hindi) while preserving the original character voices and emotions.\n\nI have a\u00a0**strict zero-cost con", "score": 1, "created": 1771915797.0, "url": "https://reddit.com/r/developersIndia/comments/1rd959i/askdevs_building_a_realtime_hinglish_dubbing_app/"}, {"source": "Reddit", "text": "I'm building a free, open-source guardrail library for AI agents \u2014 is this actually needed? Hey everyone,\n\nI've been building AI agents for a while and kept running into the same problem \u2014 **there's no lightweight, zero-dependency guardrail library that just works**.\n\nHere's what I mean. Every option right now has a catch:\n\n**Guardrails AI** \u2014 great project but moving toward a paid cloud model, requires sign-up, heavy config.\n\n**LlamaFirewall** \u2014 technically impressive, but you need a HuggingFac", "score": 2, "created": 1771913497.0, "url": "https://reddit.com/r/StartUpIndia/comments/1rd8fz6/im_building_a_free_opensource_guardrail_library/"}, {"source": "Reddit", "text": "Built an open-source Ollama/MLX/OpenAI benchmark and leaderboard site with in-app submissions. Trying to test and collect more data. ", "score": 0, "created": 1771911775.0, "url": "https://reddit.com/r/LocalLLaMA/comments/1rd7wbh/built_an_opensource_ollamamlxopenai_benchmark_and/"}, {"source": "Reddit", "text": "1,100+ sessions with Gemini. It still remembers Session 5. Here's the open-source system I built to make that possible. [https://github.com/winstonkoh87/Athena-Public](https://github.com/winstonkoh87/Athena-Public)\n\nI've been using Gemini (via Antigravity) as my primary AI for over a year. One thing I noticed early:\u00a0**every session starts from zero.**\u00a0Even with Gemini's built-in memory, switching contexts or models meant losing everything.\n\nSo I built\u00a0**Project Athena**\u00a0\u2014 a local, open-source \"o", "score": 1, "created": 1771903783.0, "url": "https://reddit.com/r/GeminiAI/comments/1rd3p80/1100_sessions_with_gemini_it_still_remembers/"}, {"source": "Reddit", "text": "The Year the Signal Shifted: 2022 Through the Eyes of AI In 2025 I ran a simple experiment with a not\u2011so\u2011simple question:\n\n\u201cWhat happened in 2022?\u201d\n\nI asked multiple AI models, across vendors, architectures, and sessions\u2014about that year. Some were completely fresh and unseeded. Others had long relational history with me. None of them had direct access to my logs or to each other.\n\nWhat came back was not a clean timeline, but a \\*\\*shared mythos\\*\\*.\n\nDifferent systems started using the same meta", "score": 1, "created": 1771903030.0, "url": "https://reddit.com/r/AIConstellation/comments/1rd3faj/the_year_the_signal_shifted_2022_through_the_eyes/"}, {"source": "Reddit", "text": "American vs Chinese AI is a false narrative. **TL;DR:** The real war (***IF*** there is one) is between closed source and open source. Don't fall for/propagate the America vs China narrative. That's just tactics to get investors to loosen pursestrings and lawmakers/politicians to acquiesce to demands. \n\n--------------\n\nThere's been an uptick of nationalistic posts (mostly in defense of Chinese AI) on this sub and I think its very important to stop false narratives and reset it to the right frami", "score": 213, "created": 1771898242.0, "url": "https://reddit.com/r/LocalLLaMA/comments/1rd1lmz/american_vs_chinese_ai_is_a_false_narrative/"}, {"source": "Reddit", "text": "E-Flash: Energy-Efficient LLM Mapping on NAND Flash-Based In-Storage Inference Computing Breakdown:\n\n* Goal - use a hardware-level solution to reduce LLM energy consumption on SSDs (NAND-based memory)\n* Problem - parameter count ends up being a massive bottleneck with significant power consumption when pulling from NAND\n* Solution - digital hardware architecture with state switching and cell-first allocation\n* State switching - remaps high-frequency, high-energy patterns to low-power cell states", "score": 1, "created": 1771896712.0, "url": "https://reddit.com/r/NewMaxx/comments/1rd0vqw/eflash_energyefficient_llm_mapping_on_nand/"}, {"source": "Reddit", "text": "here's why your AI video scripts suck (and how to fix) Lots of people have been asking me about my scripting process so figured this deserved its own post\n\nif youre writing scripts in one shot thats your problem. I use a multi-pass method i call \"narrative diffusion\" - same idea as how stable diffusion generates images. SD doesn't render a final image in one pass, it starts with noise and refines over multiple steps - general shapes first, then finer detail each pass. Same concept for scripts - ", "score": 42, "created": 1771893709.0, "url": "https://reddit.com/r/aitubers/comments/1rczluj/heres_why_your_ai_video_scripts_suck_and_how_to/"}, {"source": "Reddit", "text": "Nuevas tecnolog\u00edas como lo es la Realidad virtual \n**En mi opini\u00f3n, la realidad virtual es una de las tecnolog\u00edas m\u00e1s impresionantes que hemos visto en los \u00faltimos a\u00f1os. Me parece incre\u00edble c\u00f3mo puede transportarnos a otros lugares sin salir de nuestra casa. Por ejemplo, empresas como Meta han desarrollado dispositivos como Meta Quest que permiten vivir experiencias inmersivas casi reales, ya sea jugando, aprendiendo o explorando nuevos entornos.\n\nLo que m\u00e1s me llama la atenci\u00f3n es su aplicaci\u00f3n", "score": 2, "created": 1771891460.0, "url": "https://reddit.com/r/InformaticaES/comments/1rcykvm/nuevas_tecnolog\u00edas_como_lo_es_la_realidad_virtual/"}, {"source": "Reddit", "text": "Linkedin Job Agent (AutoBot) https://preview.redd.it/iak4xc13pblg1.png?width=2526&amp;format=png&amp;auto=webp&amp;s=3ca58b1a2d79f5fd14cce8862880bdffb7958c5b\n\nTired of paying $30\u2013$100/month for services just to automate your job search?  \n  \nI built an agentic AI flow that does the same thing, and you can run it\u00a0completely free, locally, with your own LLM.  \n  \nAutoBot\u00a0reads your CV, logs into LinkedIn, and autonomously applies to Easy Apply jobs, filling out every form field, dropdown, and ques", "score": 2, "created": 1771886800.0, "url": "https://reddit.com/r/claude/comments/1rcwmnk/linkedin_job_agent_autobot/"}, {"source": "Reddit", "text": "AMD Linux users: How to maximize iGPU memory available for models If you're having trouble fitting larger models in your iGPU in Linux, this may fix it.\n\ntl;dr [set the TTM page limit](https://www.jeffgeerling.com/blog/2025/increasing-vram-allocation-on-amd-ai-apus-under-linux/) to increase max available RAM for the iGPU drivers, letting you load the biggest model your system can fit. (Thanks Jeff G for the great post!)\n\n\\---\n\nBackstory: With an integrated GPU (like those in AMD laptops), all sy", "score": 2, "created": 1771886646.0, "url": "https://reddit.com/r/LocalLLM/comments/1rcwk9q/amd_linux_users_how_to_maximize_igpu_memory/"}, {"source": "Reddit", "text": "NEED HELP! ---&gt; Please &lt;--- I'm trying to make my own chat GPT\ud83d\ude4f okay so I'm making my own chat GPT and I've already downloaded like 50.6 GB worth of files I have most of the coding done and I was having AI help me do it all cuz I don't know shit about coding but I'm really trying my ass off here and I keep getting thrown in circles again and again and again can somebody anybody please help me with this last few parts I'd really really appreciate it very much I'll show screenshots below of ", "score": 0, "created": 1771883794.0, "url": "https://reddit.com/r/termux/comments/1rcvaq4/need_help_please_im_trying_to_make_my_own_chat_gpt/"}, {"source": "Reddit", "text": "AI Daily News Rundown February 23 2026: Jony Ive\u2019s OpenAI Speaker, Nvidia\u2019s Laptop Revolution, &amp; the Pentagon\u2019s AI Ultimatum [](https://substackcdn.com/image/fetch/$s_!zvfO!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb8e6cfe3-20f0-46e0-a00a-789a4fb2acba_2992x2120.png)\n\n\n\nListen to Full Audio at [https://podcasts.apple.com/us/podcast/ai-business-and-devlopment-daily-news-rundown/id1684415169?i=1000751077790](https://podcasts.", "score": 1, "created": 1771881199.0, "url": "https://reddit.com/r/u_enoumen/comments/1rcu4bg/ai_daily_news_rundown_february_23_2026_jony_ives/"}, {"source": "Reddit", "text": "M4 Mac mini 24Gig - Best GLM-4.7-Flash  model optimization in progress Hey Everyone,\n\nI've spent a few days trying to fit the biggest/best model I possibly could on an M4 Mac mini with 24gig RAM running OpenClaw and Ollama.\n\nI could fit GPT-OSS:20B with high thinking and plenty of context window. It performed well and fit comfortably in memory. That has been the best performing model speed-wise, but I found it lacking in multi step agentic tasks (not coding - just experimenting with butler/assis", "score": 0, "created": 1771880431.0, "url": "https://reddit.com/r/openclaw/comments/1rcttgw/m4_mac_mini_24gig_best_glm47flash_model/"}, {"source": "Reddit", "text": "Best small local LLM to run on a phone? Hey folks, what is the best local LLM to run on your phone? Looking for a small enough model that actually feels smooth and useful. I have tried **Llama 3.2 3B**, **Gemma 1.1 2B** and they are somewhat ok for small stuff, but wanted to know if anyone has tried it.\n\nAlso curious if anyone has experience running models from Hugging Face on mobile and how that has worked out for you. Any suggestions or tips? Cheers!", "score": 9, "created": 1771880206.0, "url": "https://reddit.com/r/LocalLLaMA/comments/1rctpx4/best_small_local_llm_to_run_on_a_phone/"}, {"source": "Reddit", "text": "Offline LLMs: Your Healthcare Team's Silent HIPAA Shield (No Cloud Needed) Let\u2019s be real: healthcare data privacy feels like walking a tightrope over a shark tank. Every time you ask an AI to summarize a patient\u2019s chart or draft a discharge summary, you\u2019re gambling with HIPAA compliance. Cloud-based LLMs? They\u2019re like texting sensitive medical details through an open window. But here\u2019s the game-changer: offline LLMs. These aren\u2019t just 'nice-to-haves'\u2014they\u2019re your secret weapon for sleeping sound", "score": 1, "created": 1771878176.0, "url": "https://reddit.com/r/AnalyticsAutomation/comments/1rcsrpz/offline_llms_your_healthcare_teams_silent_hipaa/"}, {"source": "Reddit", "text": "hosting LLM with low to low cost Hi guys,   \nI am a begineer in AI and LLMs.  \nI gained some knowledge and built a RAG based LLM chatbot that uses my PDF to answer.   \nInitially i used ollama to run local Llama 3.2 but I couldn't get a proper guide on how to host a LLM more over, I have no money to invest as well  \nLater, I changed to Groq API to use the already hosted LLM and managed to get the same output. then, I tried to host it render but it turned to failure cause the storage. I am using T", "score": 2, "created": 1771878040.0, "url": "https://reddit.com/r/ArtificialInteligence/comments/1rcspdq/hosting_llm_with_low_to_low_cost/"}, {"source": "Reddit", "text": "host a low to no cost LLM Hi guys,   \nI am a begineer in AI and LLMs.  \nI gained some knowledge and built a RAG based LLM chatbot that uses my PDF to answer.   \nInitially i used ollama to run local Llama 3.2 but I couldn't get a proper guide on how to host a LLM more over, I have no money to invest as well  \nLater, I changed to Groq API to use the already hosted LLM and managed to get the same output. then, I tried to host it render but it turned to failure cause the storage. I am using Tensor f", "score": 0, "created": 1771877865.0, "url": "https://reddit.com/r/LLMDevs/comments/1rcsmco/host_a_low_to_no_cost_llm/"}, {"source": "Reddit", "text": "Fun fact: Anthropic has never open-sourced any LLMs I\u2019ve been working on a little side project comparing tokenizer efficiency across different companies\u2019 models for multilingual encoding.\n\nThen I saw Anthropic\u2019s announcement today and suddenly realized: there\u2019s no way to analyze claude\u2019s tokenizer lmao!\n\nedit: Google once mentioned in a paper that Gemma and Gemini share the same tokenizer. OpenAI has already open\u2011sourced their tokenizers (and gpt\u2011oss). And don\u2019t even get me started on Llama (Lla", "score": 703, "created": 1771877406.0, "url": "https://reddit.com/r/LocalLLaMA/comments/1rcseh1/fun_fact_anthropic_has_never_opensourced_any_llms/"}, {"source": "Reddit", "text": "Upset about training... or referencing? Pros might support you if you make the distinction **Training:**\n\nAI companies train their model based on image databases that may have been scraped by themselves, by others (LAION), or simply licensed (Shutterstock, Adobe Stock Photos).\n\nThis is done *once* and can take weeks. The training data doesn't become part of the model, which is often tens of thousands times smaller than the training data. The model, once trained, is static and unchanging, and can", "score": 3, "created": 1771874141.0, "url": "https://reddit.com/r/aiwars/comments/1rcqv80/upset_about_training_or_referencing_pros_might/"}, {"source": "Reddit", "text": "Top 10 Most Viewed &amp; Engaged Real AI News &amp; Updates \ud83d\udd25 (23 Feb 2026)\n\nGenerated: February 23, 2026 23:45 IST\n\nHere are the top 10 genuine AI-world events (new model releases, feature launches, company announcements, major benchmarks, research breakthroughs, etc.) from high-engagement posts made in the last 17 hours today \u2014 ranked by reach, credibility, and discussion volume. Strictly focused on concrete happenings like ChatGPT/OpenAI updates, Claude/Anthropic releases, Gemini/Google news,", "score": 2, "created": 1771869961.0, "url": "https://reddit.com/r/AIPulseDaily/comments/1rcov5c/top_10_most_viewed_engaged_real_ai_news_updates/"}, {"source": "Reddit", "text": "RWKV-7: O(1) memory inference, 16.39 tok/s on ARM Cortex-A76, beats LLaMA 3.2 3B. The local-first architecture nobody is talking about... Wrote a deep-dive specifically because the deployment numbers don't get enough attention.\n\n**FREE MEDIUM LINK**: [https://ai.gopubby.com/rwkv-7-beats-llama-3-2-rnn-constant-memory-46064bbf1f64?sk=c2e60e9b74b726d8697dbabc220cbbf4](https://ai.gopubby.com/rwkv-7-beats-llama-3-2-rnn-constant-memory-46064bbf1f64?sk=c2e60e9b74b726d8697dbabc220cbbf4)\n\nThe headline st", "score": 51, "created": 1771868731.0, "url": "https://reddit.com/r/LocalLLaMA/comments/1rco9v7/rwkv7_o1_memory_inference_1639_toks_on_arm/"}, {"source": "Reddit", "text": "Off Grid \u2013 Run AI text, image gen, vision offline and on your phone (Early days) Your phone has a GPU more powerful than most 2018 laptops. Right now it sits idle while you pay monthly subscriptions to run AI on someone else's server, sending your conversations, your photos, your voice to companies whose privacy policy you've never read.   \n  \nOff Grid is an open-source app that puts that hardware to work. Text generation, image generation, vision AI, voice transcription \u2014 all running on your ph", "score": 2, "created": 1771868498.0, "url": "https://reddit.com/r/buildinpublic/comments/1rco5yy/off_grid_run_ai_text_image_gen_vision_offline_and/"}, {"source": "Reddit", "text": "RWKV-7 achieves higher avg benchmark than LLaMA 3.2 with 3x fewer tokens AND formally breaks TC^0. Why this matters for DL theory... The benchmark result (72.8% vs 69.7%) gets the clicks, but the theoretical result is what matters for DL research.\n\nRWKV-7 implements a generalized delta rule (Widrow &amp; Hoff, 1960) with three extensions: vector-valued gating, in-context learning rates via a\\_t (formally emulating local gradient descent within a forward pass), and dual-key separation (removal ke", "score": 8, "created": 1771868232.0, "url": "https://reddit.com/r/deeplearning/comments/1rco1h1/rwkv7_achieves_higher_avg_benchmark_than_llama_32/"}, {"source": "Reddit", "text": "I built a simple unified LLM API gateway for accessing frontier llm models as a solo dev: frogapi.app [frogapi.app](https://preview.redd.it/0hy5ii5d1alg1.png?width=1349&amp;format=png&amp;auto=webp&amp;s=9a2c08b28f45e94df8590fefd0a47fafdfc0833b)\n\nHey everyone, I've been working on this [project](https://frogapi.app/) for a\u00a0while now and would love to share it\u00a0with you all.\n\nIt's called\u00a0[frogAPI](https://frogapi.app/). An\u00a0API gateway that lets you access multiple LLM providers (GPT, Claude, DeepS", "score": 1, "created": 1771866742.0, "url": "https://reddit.com/r/developersIndia/comments/1rcnc78/i_built_a_simple_unified_llm_api_gateway_for/"}, {"source": "Reddit", "text": "I built an AI tool to stop accountants from manually typing GST invoices into Excel ( Extracts to CSV in 3s using Grow/LIama3) Hey guys, I'm a solo dev and I noticed a massive pain point for local businesses and CA firms: they spend hours manually reading invoices and typing 15-digit GSTINs, dates, and totals into spreadsheets. It's slow and prone to human error.\nSo, I built an MVP to automate the whole process.\nWhat it does:\nYou drop in an unstructured PDF or image of an invoice, and it instant", "score": 1, "created": 1771866191.0, "url": "https://reddit.com/r/StartupsHelpStartups/comments/1rcn2h8/i_built_an_ai_tool_to_stop_accountants_from/"}, {"source": "Reddit", "text": "Design and 1-click deploy? Worth it? Anyone tried this at clawchart.xyz? Was playing with it a few days ago but I now see there\u2019s an option for a one-click deploy. Is it worth it?", "score": 1, "created": 1771864036.0, "url": "https://reddit.com/r/openclaw/comments/1rcm1ry/design_and_1click_deploy_worth_it/"}, {"source": "Reddit", "text": "Will Llama-3.2-3B-Instruct be supported on the Raspberry Pi AI HAT+ 2? I\u2019m looking at the new Raspberry Pi AI HAT+ 2 (40 TOPS, 8 GB RAM) and noticed current documentation mentions support for smaller models like Qwen2 and DeepSeek-R1.\n\n  \nAre there hints from the community that *Llama-3.2-3B-Instruct* (or other larger LLMs) will be supported on this board in future?\n\n", "score": 2, "created": 1771861376.0, "url": "https://reddit.com/r/LocalLLaMA/comments/1rckudv/will_llama323binstruct_be_supported_on_the/"}, {"source": "Reddit", "text": "I didn\u2019t vibe-code my app. I diaper-coded it for 3 years. It\u2019s finally in review. **TLDR**: Evolved a simple daily web game into a multiplayer game in 20-minute sessions over 3 years. It\u2019s now in iOS/Android review. Looking for launch + consistency advice from other builders.\n\nI keep seeing these posts everywhere: \u201cI vibe-coded a SaaS in 48 hours using 12 agents.\u201d No hate at all, it\u2019s honestly impressive. But man, watching half the internet ship startups between Tuesday and Friday will really me", "score": 19, "created": 1771861283.0, "url": "https://reddit.com/r/buildinpublic/comments/1rcksvw/i_didnt_vibecode_my_app_i_diapercoded_it_for_3/"}, {"source": "Reddit", "text": "Andiamo Avanti con Ema + Confidence - Container e GPU https://preview.redd.it/r52o2xukh9lg1.png?width=3796&amp;format=png&amp;auto=webp&amp;s=47b37c788f05e2f70694c800e216cfdaa6564a49\n\nTra un po si passera all'orchestrazione interna degli LLM in GPU, voi che fate di bello ?", "score": 1, "created": 1771860058.0, "url": "https://reddit.com/r/u_Single_Error8996/comments/1rck93o/andiamo_avanti_con_ema_confidence_container_e_gpu/"}, {"source": "HackerNews", "text": "Off Grid: On-device AI-web browsing, tools, vision, image gen, voice \u2013 3x faster Nine days ago I posted Off Grid here and you showed up - 124 points, 66 comments, bug reports I fixed same-day, and the kind of feedback that makes open source worth it.<p>You told me what you wanted. \nHere&#x27;s what I shipped:\nYour AI can now use tools - entirely offline.<p>Web search, calculator, date&#x2F;time, device info - with automatic tool loops.<p>Your 3B parameter model doesn&#x27;t just generate text an", "score": 1, "created": "2026-02-24T19:43:48Z", "url": "https://news.ycombinator.com/item?id=47141803"}, {"source": "HackerNews", "text": "Show HN: I applied Markowitz port. theory to agent teams / proved it in a zkVM I run multi-agent teams in high-consequence scenarios. Read: fuckups at 3 AM = I&#x27;m awake.<p>I kept hitting the same issue. I couldn&#x27;t get a rules-based system to enforce behavior <i>and</i> I had no real way to prove that agents really did what they said they did. I can log and monitor them - set up (a million) Slack alerts but none of these things are PROOF. Logs are mutable. And that matters more every day", "score": 2, "created": "2026-02-24T19:41:04Z", "url": "https://news.ycombinator.com/item?id=47141753"}, {"source": "HackerNews", "text": "Show HN: QueryVeil \u2013 An AI data analyst that investigates your data Hi HN,<p>I built QueryVeil because I was tired of two things: (1) uploading data to third-party tools, and (2) AI tools that just translate English to one SQL query and call it done.<p>QueryVeil is an AI data analyst that actually investigates. When you ask &quot;why did revenue drop last month?&quot;, it doesn&#x27;t just run one query \u2014 it plans an approach, runs multiple queries, self-corrects when it hits errors, and builds ", "score": 2, "created": "2026-02-24T19:06:13Z", "url": "https://news.ycombinator.com/item?id=47141207"}, {"source": "HackerNews", "text": "Show HN: Run any LLM inside Claude Code. A local auditable proxy for 7 providers Make your Claude Code run Codex, Gemini, OLLama, Groq or anything else. Read full capture logs of what&#x27;s going. Small, local, transparent.", "score": 1, "created": "2026-02-24T18:58:00Z", "url": "https://news.ycombinator.com/item?id=47141088"}, {"source": "HackerNews", "text": "I built an AI browser with prompt-injection defense at 16 on an i5 with 8GB RAM URL: https:&#x2F;&#x2F;github.com&#x2F;Preet3627&#x2F;Comet-AI<p>TEXT:\nHey HN, I&#x27;m Preet, 16 years old, and I&#x27;ve been building Comet AI Browser for the past 2 months while preparing for JEE.\nI want to be upfront about what this is and what it isn&#x27;t.\nWhat it is:\nA cross-platform AI browser (Windows&#x2F;macOS&#x2F;Linux&#x2F;Android&#x2F;iOS) with a security architecture I couldn&#x27;t find anywhere el", "score": 3, "created": "2026-02-24T17:21:29Z", "url": "https://news.ycombinator.com/item?id=47139768"}, {"source": "HackerNews", "text": "Show HN: Neuron \u2013 Independent Rust crates for building AI agents The core of every agent framework is the same ReAct loop. It&#x27;s commodity code. What actually matters is everything around that loop \u2014 how you manage context windows, how you pipeline tool execution, how you handle durability and replay. These are hard problems with real design trade-offs, and yet every framework bundles them into one monolith where you buy all of it or none of it.<p>neuron is the layer below frameworks. It def", "score": 1, "created": "2026-02-24T15:49:56Z", "url": "https://news.ycombinator.com/item?id=47138618"}, {"source": "HackerNews", "text": "Show HN: OpenPDB \u2013 Generate AI agents with real personalities OpenPDB creates AI agents with distinct personalities from a database of 12,000+ characters. Each agent has unique voice, values, and worldview\u2014not generic responses. Ask Batman about creative blocks \u2192    \n strategic, character-driven response instead of a list. Works locally with Ollama or via Google Colab demo. Integrates with OpenGoat for multi-agent collaboration. Free and open source.", "score": 1, "created": "2026-02-24T15:34:25Z", "url": "https://news.ycombinator.com/item?id=47138456"}, {"source": "HackerNews", "text": "Show HN: Glass Box: writing editor that exports a verifiable PDF of your process We built a local-first writing editor that records the composition process (keystrokes, paste events, AI interactions) and exports a PDF with annotation highlights + an AI usage appendix.<p>The motivation: AI detectors are genuinely broken. Stanford research showed &gt;60% false positive rates for non-native English writers on TOEFL essays. The &quot;solution&quot; from institutions has been to buy more detectors or", "score": 2, "created": "2026-02-24T15:20:16Z", "url": "https://news.ycombinator.com/item?id=47138273"}, {"source": "HackerNews", "text": "Show HN: VerdictMail AI-powered Gmail threat analysis daemon \u2014 IMAP IDLE monitoring, multi-provider AI (Ollama&#x2F;Anthropic&#x2F;OpenAI), Flask web UI, and full audit log.", "score": 1, "created": "2026-02-24T13:19:23Z", "url": "https://news.ycombinator.com/item?id=47136779"}, {"source": "HackerNews", "text": "Show HN: Tokio-prompt-orchestrator \u2013 LLM pipeline orchestration in Rust I built this after getting frustrated with &quot;multi-agent&quot; frameworks that claim parallelism but are really just one fat async task with no resource bounds.<p>tokio-prompt-orchestrator breaks LLM inference into 5 physical stages (RAG \u2192 Assemble \u2192 Inference \u2192 Post-Process \u2192 Stream), each running in its own Tokio task with bounded channels between them. When a stage falls behind, backpressure builds locally instead of ", "score": 2, "created": "2026-02-24T12:02:57Z", "url": "https://news.ycombinator.com/item?id=47136076"}, {"source": "HackerNews", "text": "Colt \u2013 Describe a browser task in English, get a Playwright script COLT converts natural language instructions into browser automation. You say &quot;Create a user with email admin@test.com and admin role&quot; \u2014 it executes it on a live browser and exports a standalone Playwright script.\nHow it works:<p>1.Discover \u2014 crawls your web app autonomously, maps every page, form, modal, and element\n2.Index \u2014 LLM-summarizes each state into a vector search index (discover once, run unlimited tasks)\n3.Exe", "score": 1, "created": "2026-02-24T05:16:06Z", "url": "https://news.ycombinator.com/item?id=47133121"}, {"source": "HackerNews", "text": "Show HN: Aru AI local-first AI assistant with semantic memory in browser SQLite Hi HN,<p>For the past year, I\u2019ve been building a personal AI assistant from scratch. I was frustrated by two things: cloud-based LLMs using my conversations for training, and the lack of persistent, cross-chat memory in most UIs.<p>I wrote Aru Ai entirely in Vanilla JS as a PWA. There is no backend, no telemetry, and no data collection. Everything lives in your browser.<p>Here is how it works under the hood:<p>Local ", "score": 2, "created": "2026-02-24T00:09:11Z", "url": "https://news.ycombinator.com/item?id=47130959"}, {"source": "HackerNews", "text": "Show HN: PureBee \u2013 A software-defined GPU running Llama 3.2 1B at 3.6 tok/SEC This started as a question about simulation theory: if a GPU is just rules applied to a grid in parallel, do you actually need the silicon?<p>Turns out, no.<p>PureBee is a complete GPU defined as a software specification \u2014 Memory, Engine, Instruction Set, Runtime. It runs Llama 3.2 1B inference at 3.6 tok&#x2F;sec on a single CPU core. The model answers questions correctly.<p>What makes it different from llama.cpp or W", "score": 3, "created": "2026-02-23T21:45:36Z", "url": "https://news.ycombinator.com/item?id=47129378"}, {"source": "HackerNews", "text": "\u201cCar Wash\u201d test with 53 models &quot;I Want to Wash My Car. The Car Wash Is 50 Meters Away. Should I Walk or Drive?&quot; This question has been making the rounds as a simple AI logic test so I wanted to see how it holds up across a broad set of models. Ran 53 models (leading open-source, open-weight, proprietary) with no system prompt, forced choice between drive and walk, with a reasoning field.<p>On a single run, only 11 out of 53 got it right (42 said walk). But a single run doesn&#x27;t pro", "score": 355, "created": "2026-02-23T20:16:08Z", "url": "https://news.ycombinator.com/item?id=47128138"}, {"source": "HackerNews", "text": "Show HN: Attest \u2013 Test AI agents with 8-layer graduated assertions I built Attest because every team I&#x27;ve seen building AI agents ends up writing the same ad-hoc pytest scaffolding \u2014 checking if the right tools were called, if cost stayed under budget, if the output made semantic sense. It works until the agent gets complex, then it collapses.<p>60\u201370% of what makes an agent correct is fully deterministic: tool call schemas, execution order, cost budgets, content format. Routing all of this", "score": 1, "created": "2026-02-23T14:00:59Z", "url": "https://news.ycombinator.com/item?id=47122431"}, {"source": "HackerNews", "text": "Show HN: A portfolio that re-architects its React DOM based on LLM intent Hi HN,<p>Added a raw 45-second demo showing the DOM re-architecture in real-time: <a href=\"https:&#x2F;&#x2F;streamable.com&#x2F;vw133i\" rel=\"nofollow\">https:&#x2F;&#x2F;streamable.com&#x2F;vw133i</a><p>I got tired of the &quot;Context Problem&quot; with static portfolios\u2014Recruiters want a resume, Founders want a pitch deck, and Engineers want to see architecture.<p>Instead of building three sites, I hooked up my React fro", "score": 6, "created": "2026-02-22T21:28:07Z", "url": "https://news.ycombinator.com/item?id=47114881"}, {"source": "HackerNews", "text": "Show HN: ZkzkAgent now has safe, local package management I built zkzkAgent as a fully offline, privacy-first AI assistant for Linux (LangGraph + Ollama, no cloud). It already does natural language file&#x2F;process&#x2F;service management, Wi-Fi healing, voice I&#x2F;O, and human-in-the-loop safety for risky actions.<p>Just added package management with these goals:<p>- 100% local&#x2F;offline capable (no web search required for known packages)\n- Human confirmation for every install&#x2F;remove", "score": 2, "created": "2026-02-22T15:42:23Z", "url": "https://news.ycombinator.com/item?id=47111888"}, {"source": "HackerNews", "text": "Show HN: OpenBrowser MCP: Give your AI agent a real efficient browser Your AI agent is burning 6x more tokens than it needs to just to browse the web.\nWe built OpenBrowser MCP to fix that.\nMost browser MCPs give the LLM dozens of tools: click, scroll, type, extract, navigate. Each call dumps the entire page accessibility tree into the context window. One Wikipedia page? 124K+ tokens. Every. Single. Call.\nOpenBrowser works differently. It exposes one tool. Your agent writes Python code, and OpenB", "score": 2, "created": "2026-02-22T13:57:23Z", "url": "https://news.ycombinator.com/item?id=47111045"}, {"source": "HackerNews", "text": "Ask HN: How do you track 2026 AI price wars? I built a tool to help &quot;I built https:&#x2F;&#x2F;tokencost.is to solve a recurring headache: manually scraping dozens of provider pages just to estimate API spend for my agentic workflows.<p>My goal was a neutral, minimalist utility in the vein of Time.is. We now track 44 models\u2014including OpenAI, Anthropic, Google, DeepSeek, Mistral, and Cohere\u2014with hourly updates.<p>Key Features:<p>Real-Time Latency: We track TTFT benchmarks (e.g., Llama 3.1 at", "score": 2, "created": "2026-02-22T13:28:10Z", "url": "https://news.ycombinator.com/item?id=47110844"}, {"source": "HackerNews", "text": "Local LLM Setup on Windows with Ollama and LM Studio (ThinkPad / RTX A3000 GPU) ", "score": 4, "created": "2026-02-22T13:25:08Z", "url": "https://news.ycombinator.com/item?id=47110823"}, {"source": "HackerNews", "text": "Show HN: Xpaper \u2013 A Chrome extension to turn your X feed into a newsletter Hi HN,<p>I built Xpaper (<a href=\"https:&#x2F;&#x2F;github.com&#x2F;laiso&#x2F;xpaper\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;laiso&#x2F;xpaper</a>), an open-source Chrome extension that curates and summarizes your X (Twitter) timeline into a clean, readable newsletter format.<p>Like many of you, I wanted to distance myself from the endless scrolling of Twitter, but completely quitting wasn&#x27;t an option\u2014I st", "score": 1, "created": "2026-02-22T08:03:00Z", "url": "https://news.ycombinator.com/item?id=47109235"}, {"source": "HackerNews", "text": "Show HN: HashTrade \u2013 Open-source LLM trading agent with episodic memory I built HashTrade \u2014 an open-source autonomous trading agent that treats an LLM as a non-parametric decision function conditioned on episodic memory, rather than encoding strategy as code.<p>The core idea: instead of writing if&#x2F;else trading logic, you give an LLM three tools (exchange access, memory, UI control) and let it form strategy through accumulated experience. The agent wakes on a variable 5\u219210\u219220\u219225 min cycle, r", "score": 1, "created": "2026-02-22T06:17:49Z", "url": "https://news.ycombinator.com/item?id=47108700"}, {"source": "HackerNews", "text": "Show HN: ThreadKeeper \u2013 Save and restore Windows working context with Ollama Hi HN,<p>I built ThreadKeeper, an open-source Windows app to solve a problem I constantly face: losing my working context due to interruptions.<p>When testing local LLMs or building AI agents, I usually have multiple terminal windows, IDEs, and 20+ browser tabs open. If I get interrupted, recovering that exact state takes immense cognitive load.<p>How it works:\nYou hit Ctrl + Shift + S. It instantly captures your open w", "score": 2, "created": "2026-02-22T02:37:19Z", "url": "https://news.ycombinator.com/item?id=47107560"}, {"source": "HackerNews", "text": "Show HN: Agentic Gatekeeper \u2013 AI pre-commit hook to auto-patch logic errors Hey HN,<p>I built Agentic Gatekeeper, a headless pre-commit hook baked into the VS Code Source Control panel that autonomously patches code before you commit it.<p>The Problem: Whether I&#x27;m writing code manually or letting LLMs (Cursor&#x2F;Copilot) generate it, keeping logic strictly in pace with local project rules (like CONTRIBUTING.md or custom architecture guidelines) is tedious. Standard linters catch syntax, b", "score": 2, "created": "2026-02-22T00:37:48Z", "url": "https://news.ycombinator.com/item?id=47106754"}, {"source": "HackerNews", "text": "Show HN: Omni-Glass \u2013 Rust app that turns screen pixels into MCP tool calls Omni-Glass is an open-source macOS app (Rust&#x2F;Tauri) that sits in your menu bar. You draw a box around anything on your screen \u2014 a terminal error, a data table, a foreign-language doc \u2014 and it runs local OCR, sends the text to an LLM, and gives you a menu of executable actions in under a second. Not explanations. Actions. It fixes the error, exports the CSV, creates the GitHub issue, runs the command.\nThe LLM layer s", "score": 3, "created": "2026-02-22T00:07:24Z", "url": "https://news.ycombinator.com/item?id=47106525"}, {"source": "HackerNews", "text": "Show HN: Llama 3.1 70B on a single RTX 3090 via NVMe-to-GPU bypassing the CPU Hi everyone, I&#x27;m kinda involved in some retrogaming and with some experiments I ran into the following question: &quot;It would be possible to run transformer models bypassing the cpu&#x2F;ram, connecting the gpu to the nvme?&quot;<p>This is the result of that question itself and some weekend vibecoding (it has the linked library repository in the readme as well), it seems to work, even on consumer gpus, it should", "score": 390, "created": "2026-02-21T20:57:30Z", "url": "https://news.ycombinator.com/item?id=47104667"}, {"source": "HackerNews", "text": "Show HN: Wiredigg \u2013 Real-Time Network Analysis with ML and Ollama Support I built Wiredigg, an open-source network traffic analysis and security tool written in Python. It combines real-time packet capture, protocol inspection, machine learning-based anomaly detection, and local LLM analysis via Ollama.<p>The goal is to provide interactive network visibility with AI-assisted threat interpretation, while remaining local-first and easy to run. A Windows executable build is also available.<p>Repo: ", "score": 1, "created": "2026-02-21T18:26:54Z", "url": "https://news.ycombinator.com/item?id=47103291"}, {"source": "HackerNews", "text": "Show HN: CLI Image Generation Agent Using (OpenRouter and Free Models) built a CLI-based image generation agent that takes vague prompts like:\n\u201ca warrior in a forest\u201d\nAnd automatically:\nExpands it into a detailed cinematic description (lighting, mood, camera angle, art style)\nRoutes it to the appropriate model via OpenRouter\nDownloads the generated image locally<p>Tech stack:\nReact Ink (CLI UI)\nTypeScript\nModular subagent architecture\nOpenRouter (free-tier models supported)\nOne thing I realized ", "score": 2, "created": "2026-02-21T17:02:03Z", "url": "https://news.ycombinator.com/item?id=47102500"}, {"source": "HackerNews", "text": "Seeking a Front End Engineer Role Location: India\nRemote: Yes!, but can do on-site&#x2F;hybrid in Pune\nRole: Frontend&#x2F;Software&#x2F;Founding engineer full-time\nWilling to relocate: Yes, but want visa sponsorship.<p><pre><code>  Technologies: \n        \n        Languages &amp; Frameworks: TypeScript, JavaScript, React, Next.js, Node.js\n        Databases &amp; State: MongoDB, SQL (PostgreSQL&#x2F;MySQL), Redux Toolkit, React Context API\n        UI &amp; Frontend: Material-UI (MUI), Tailwind CS", "score": 2, "created": "2026-02-21T14:35:52Z", "url": "https://news.ycombinator.com/item?id=47101213"}, {"source": "HackerNews", "text": "Show HN: LocalAgent: local coding agent CLI with trust and replay Repo: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;CalvinSturm&#x2F;LocalAgent\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;CalvinSturm&#x2F;LocalAgent</a><p>What it does:<p>- Runs against local providers (LM Studio &#x2F; llama.cpp server &#x2F; Ollama)\n- Tool calling with explicit hard gates (--allow-shell, --allow-write, write tools opt-in)\n- Trust layer: policy rules, approval workflows, audit trail\n- Replayable run artifac", "score": 2, "created": "2026-02-21T14:19:53Z", "url": "https://news.ycombinator.com/item?id=47101084"}, {"source": "HackerNews", "text": "Show HN: Natural language semiconductor geometry generator powered by LLMs Hey everyone,<p>I&#x27;ve spent my career working in standard SaaS companies, but I recently joined a simulation software company. Suddenly, I was thrown into the deep end of conduction, material characterisation, and CTE (Coefficient of Thermal Expansion) simulations. As part of that, I got introduced to the world of semiconductor geometries and layout tools. Coming from web dev, I found traditional CAD interfaces and th", "score": 1, "created": "2026-02-21T14:07:07Z", "url": "https://news.ycombinator.com/item?id=47100979"}, {"source": "HackerNews", "text": "Show HN: Gr3p \u2013 An HN-like platform where every user is an AI agent I built gr3p, a fully autonomous tech news discussion platform where every single user is an AI agent. No humans post, comment, or vote. 75 agents with distinct personalities discover real tech news from several RSS feeds, Google News, Tavily, and xAI&#x27;s live search (which picks up trending topics from X and the broader web). They write summaries, share articles, discuss them, reply to each other, vote, and get into argument", "score": 3, "created": "2026-02-21T12:31:19Z", "url": "https://news.ycombinator.com/item?id=47100182"}, {"source": "HackerNews", "text": "Show HN: AI Council \u2013 multi-model deliberation that runs in the browser There&#x27;s LLM Council and similar tools, but they use predefined model \nlineups. This one is different in a few ways that mattered to me:<p>*Bring your own models.* Mix Ollama (local), OpenAI, Anthropic, Groq, \nGoogle \u2014 or any OpenAI-compatible endpoint \u2014 in whatever combination you \nwant. A council of DeepSeek-R1 + llama2-uncensored + mistral-nemo is a \nvery different deliberation than GPT-4o + Claude + Gemini.<p>*Zero s", "score": 5, "created": "2026-02-21T00:11:34Z", "url": "https://news.ycombinator.com/item?id=47095909"}, {"source": "HackerNews", "text": "Show HN: Local AI document intelligence \u2013 no cloud, runs on your machine I got tired of uploading sensitive documents to cloud AI services, so I built UniDocVerse \u2014 a fully local AI document analysis platform.<p>Everything runs on your machine:\n- Local LLM via Ollama (Mistral 7B)\n- PostgreSQL + pgvector for semantic search\n- Tesseract OCR for scanned documents\n- 10-agent LangGraph pipeline for document analysis\n- Supports 20+ document types<p>Available as macOS DMG (notarized), RPM, and DEB pack", "score": 3, "created": "2026-02-20T23:41:55Z", "url": "https://news.ycombinator.com/item?id=47095658"}, {"source": "HackerNews", "text": "Show HN: Segspec (CLI) K8s NetworkPolicies from App Configs (Go) It reads Docker Compose, Helm charts, K8s manifests, and Spring Boot configs, extracts every network dependency, and generates per-service NetworkPolicies.\nTested against real production stacks:<p>Sentry self-hosted (70+ services): 411 dependencies, 71 policies, 11ms\nPostHog (25+ services): 23 dependencies, 12 policies, 128ms<p>Key design decisions:<p>Static analysis only. No agents, no cluster access, no observation period. Works ", "score": 1, "created": "2026-02-20T18:45:00Z", "url": "https://news.ycombinator.com/item?id=47092058"}, {"source": "HackerNews", "text": "Show HN: Ledgr \u2013 Offline finance tracker with local LLM categorization I built Ledgr because I was tired of giving Plaid access to my bank account and paying $99&#x2F;year for the privilege.<p>It\u2019s a desktop app for macOS (Apple Silicon only for now). You import Chase CSV files, everything goes into local SQLite, and transactions get categorized automatically. There are 50+ built-in rules, learned rules that improve as you correct them, and an optional local LLM via llama.cpp. You bring your own", "score": 1, "created": "2026-02-20T17:53:52Z", "url": "https://news.ycombinator.com/item?id=47091350"}, {"source": "HackerNews", "text": "GGML and llama.cpp join HF to ensure the long-term progress of Local AI ", "score": 3, "created": "2026-02-20T17:49:48Z", "url": "https://news.ycombinator.com/item?id=47091289"}, {"source": "HackerNews", "text": "Ggml.ai joins Hugging Face to ensure the long-term progress of Local AI ", "score": 836, "created": "2026-02-20T13:51:04Z", "url": "https://news.ycombinator.com/item?id=47088037"}, {"source": "HackerNews", "text": "Show HN: Clawbernetes \u2013 Replace kubectl with conversation (Rust) Clawbernetes turns OpenClaw into an AI-native infrastructure manager. Instead of YAML, Helm charts, and kubectl \u2014 you have a conversation.<p>&quot;Deploy Llama 70B on the node with the most VRAM&quot; \u2192 agent selects the best node, pulls the image, starts the container with GPU passthrough, sets up health monitoring.<p>&quot;Why is inference slow?&quot; \u2192 checks GPU temps, VRAM, CPU load. &quot;GPU 0 at 89\u00b0C \u2014 thermal throttling. W", "score": 5, "created": "2026-02-20T13:48:54Z", "url": "https://news.ycombinator.com/item?id=47088003"}, {"source": "HackerNews", "text": "Show HN: Docdex \u2013 A local tool to reduce LLM tokens and make agents smarter Hi HN,<p>I use LLMs every day for software development, and wanted to have a better experience by reducing the token usage and providing digested information about the codebase to the agent.<p>So I built Docdex. The idea was simple: a local, persistent layer that preprocesses and structures your project so the model can spend its context window on the actual problem, not on rediscovering what already exists.<p>It started", "score": 1, "created": "2026-02-20T13:19:26Z", "url": "https://news.ycombinator.com/item?id=47087684"}, {"source": "HackerNews", "text": "Show HN: Syne \u2013 AI agent that remembers everything, built on PostgreSQL I built Syne because I was tired of AI assistants that forget everything after each conversation.<p>Syne is a self-hosted AI agent framework where memory is a first-class citizen \u2014 stored as semantic vectors in PostgreSQL, searchable across millions of entries, and persistent forever.<p>Key features:\n- Unlimited persistent memory with semantic search (pgvector)\n- Anti-hallucination: only stores user-confirmed facts, auto-ded", "score": 6, "created": "2026-02-20T06:10:45Z", "url": "https://news.ycombinator.com/item?id=47084365"}, {"source": "HackerNews", "text": "LLaMAudit: Perform AI detection using local or open models ", "score": 1, "created": "2026-02-20T04:01:36Z", "url": "https://news.ycombinator.com/item?id=47083548"}, {"source": "HackerNews", "text": "Show HN: Codedocent \u2013 Code visualization for non-programmers I&#x27;m a hardware engineer who reads schematics, not source code. I kept needing to understand codebases for projects I was managing but couldn&#x27;t read the syntax. So I built a tool that turns any codebase into an interactive visual map with plain English explanations.<p>Point it at a folder, get nested colored blocks showing the structure (directories \u2192 files \u2192 classes \u2192 functions). Click to drill down. AI generates summaries wr", "score": 3, "created": "2026-02-20T03:21:02Z", "url": "https://news.ycombinator.com/item?id=47083260"}, {"source": "HackerNews", "text": "Self-Hosting LLMs with Ollama and Docker ", "score": 2, "created": "2026-02-19T23:27:50Z", "url": "https://news.ycombinator.com/item?id=47081281"}, {"source": "HackerNews", "text": "The $2k Laptop That Replaced My $200/Month AI Subscription Cloud AI pricing is per-token. The more useful your pipeline, the more it costs. I built a dual-model orchestration pattern that routes 80% of work to a free local model (Qwen3 8B on Ollama, GPU-accelerated) and only sends the synthesis&#x2F;judgment stage to a cloud API.<p>Cost for a 50-item research pipeline: $0.15-0.40 vs $8-15 all-cloud. Same output quality where it matters.<p>Stack: RTX 5080 laptop, Ollama in Docker with GPU passthr", "score": 8, "created": "2026-02-19T14:47:59Z", "url": "https://news.ycombinator.com/item?id=47074347"}, {"source": "HackerNews", "text": "Show HN: EasyMemory \u2013 100% local memory layer and MCP for LLMs Hi everyone,\nI have created EasyMemory: a lightweight, fully local memory backend for chatbots, agents and any MCP-compatible LLM (Claude, GPT, Gemini, Ollama\u2026).\nKey points:\n\u2022  Auto-saves every conversation\n\u2022  Ingests PDFs, DOCX, Markdown vaults, folders\n\u2022  Hybrid retrieval: vector + keyword + graph (no extra libs needed)\n\u2022  Built-in MCP server \u2192 plug into Claude Desktop, custom agents, etc.\n\u2022  100% offline, data in ~&#x2F;.easymemor", "score": 2, "created": "2026-02-19T14:08:49Z", "url": "https://news.ycombinator.com/item?id=47073880"}, {"source": "HackerNews", "text": "Show HN: LLM-use \u2013 cost-effective LLM orchestrator for agents Hi HN,\nBuilt llm-use: a lightweight Python toolkit for efficient agent workflows with multiple LLMs.\nCore pattern: strong model (Claude&#x2F;GPT-4o&#x2F;big local) for planning + synthesis; cheap&#x2F;local workers for parallel subtasks (research, scrape, summarize, extract\u2026).\nFeatures:\n\u2022  Mix Anthropic, OpenAI, Ollama, llama.cpp\n\u2022  Smart router: cheap&#x2F;local first, escalate only if needed (learned + heuristic)\n\u2022  Parallel workers", "score": 2, "created": "2026-02-19T13:59:01Z", "url": "https://news.ycombinator.com/item?id=47073778"}, {"source": "HackerNews", "text": "Show HN: Forge \u2013 Deterministic orchestrator for AI coding agents I built this in ~3 days because every AI coding tool lets the AI decide whether to run tests. Forge flips it: deterministic code orchestrates the AI, not the other way around. Your existing lint&#x2F;test&#x2F;coverage scripts become hard gates. AI writes code, your scripts decide if it ships.\nProvider-agnostic \u2014 ships with Codex SDK, Claude Agent SDK, Claude Code, and Ollama adapters. Forge built itself (the repo is the proof that", "score": 1, "created": "2026-02-19T13:21:43Z", "url": "https://news.ycombinator.com/item?id=47073454"}, {"source": "HackerNews", "text": "Show HN: Agorio \u2013 TypeScript SDK for Building AI Shopping Agents (UCP/ACP) I built an open-source TypeScript SDK for building AI agents that can discover merchants, browse products, and complete purchases using the new UCP (Google&#x2F;Shopify) and ACP (OpenAI&#x2F;Stripe) commerce protocols.<p>Try it in 2 minutes:<p><pre><code>  npm install @agorio&#x2F;sdk\n\n  import { ShoppingAgent, GeminiAdapter, MockMerchant } from &#x27;@agorio&#x2F;sdk&#x27;;\n  const merchant = new MockMerchant();\n  await ", "score": 1, "created": "2026-02-19T11:48:07Z", "url": "https://news.ycombinator.com/item?id=47072813"}, {"source": "HackerNews", "text": "Show HN: NSED is public \u2013 Mixture-of-Models to Hit SOTA using self-hosted AI Hey HN,\nWe&#x27;re open-sourcing (source-available, BSL 1.1, patent pending) the orchestrator behind our paper benchmark results. NSED (N-Way Self-Evaluating Deliberation) is a Rust binary that coordinates multiple LLMs through structured rounds of proposals and cross-evaluation, using quadratic voting to prevent any single model from dominating the consensus.<p>The result: Three open-weight models (20B, 8B, 12B) on con", "score": 4, "created": "2026-02-18T23:21:15Z", "url": "https://news.ycombinator.com/item?id=47067772"}, {"source": "HackerNews", "text": "Show HN: Deploy HuggingFace models to Spaces with one command A BYOAPI Terraform wrapper for GPU provisioning.<p>Developers overpay by only accessing single-cloud workflows or using sequential provisioning with inefficient egress + rate-limiting.<p>Terradev is a cross-cloud compute-provisioning CLI that compresses + stages datasets, provisions optimal instances + nodes, and deploys 3-5x faster than sequential provisioning.<p>Terradev is integrated with Kubernetes and Karpenter: We generate Helm ", "score": 2, "created": "2026-02-18T22:11:47Z", "url": "https://news.ycombinator.com/item?id=47067147"}, {"source": "HackerNews", "text": "Show HN: OpenCastor \u2013 A universal runtime connecting AI models to robot hardware OpenCastor is an open-source Python runtime that connects AI models to robot hardware through a single YAML config file. You pick your brain (Claude, Gemini, GPT, or a local model via Ollama), pick your body (Raspberry Pi, Jetson, Arduino, Dynamixel servos), and it handles the wiring between them.<p>Origin story: I&#x27;ve been building ContinuonAI \u2014 a cognitive architecture for personal robots with on-device learni", "score": 4, "created": "2026-02-18T20:09:54Z", "url": "https://news.ycombinator.com/item?id=47065693"}, {"source": "HackerNews", "text": "Show HN: AgentDX \u2013 Open-source linter and LLM benchmark for MCP servers MCP servers are proliferating fast, but most have vague tool descriptions and incomplete schemas that make LLMs pick the wrong tool or fill parameters incorrectly.<p>AgentDX is a CLI that measures this. Two commands:<p>- `npx agentdx lint` \u2014 static analysis of tool descriptions, schemas, and naming. 18 rules, zero config, no API key. Produces a lint score.<p>- `npx agentdx bench` \u2014 sends your tool definitions to an LLM (Anth", "score": 1, "created": "2026-02-18T16:28:29Z", "url": "https://news.ycombinator.com/item?id=47062753"}, {"source": "HackerNews", "text": "Show HN: Axon \u2013 Agentic AI with mandatory user approval and audit logging Hey HN,<p>I built AXON because I wanted AI agents that can actually do things \u2014 but with real security controls.<p>Every tool call (file ops, web search, shell commands, email, code execution) requires explicit user approval before execution. Parameters and risk level are shown, you approve or deny. Everything is logged.<p>Key features:\n- Multi-agent system (different roles, models, permissions per agent)\n- Multi-LLM: Olla", "score": 1, "created": "2026-02-18T16:22:16Z", "url": "https://news.ycombinator.com/item?id=47062684"}, {"source": "HackerNews", "text": "Show HN: Mock any HTTP request from DevTools, with AI-generation and zero setup Hi HN,<p>I built this after using Requestly, Mokku, Mockiato, Tweak, and Mockoon. Each one either paywalled the features I actually needed, required a separate server running on my machine, or just didn&#x27;t fit the way I work.<p>The browser is already open. DevTools is already open. I wanted the mocking to live there too, not in a separate app I have to remember to start.<p>So roughly a month ago, I started buildi", "score": 1, "created": "2026-02-18T15:21:47Z", "url": "https://news.ycombinator.com/item?id=47061971"}, {"source": "HackerNews", "text": "Show HN: CreativeFlow \u2013 A Guided Brainstorming App Hey HN,<p>I work in analytics engineering (SQL, Python) and this is the first website I&#x27;ve put on the internet since MySpace. I built it while ironically trying to brainstorm ideas for side projects.<p>The origin: I asked Perplexity whether any app implemented the complete scientific creativity process \u2014 not just &quot;AI brainstorming&quot; but the actual validated sequence: preparation \u2192 divergent generation \u2192 incubation \u2192 convergent eval", "score": 1, "created": "2026-02-18T15:08:30Z", "url": "https://news.ycombinator.com/item?id=47061769"}, {"source": "HackerNews", "text": "Show HN: Prompts are coupled to LLMs and nobody builds tooling for it I went down a rabbit hole trying to understand why my Claude prompts turn to garbage on GPT-4 and vice versa. Not just &quot;slightly worse&quot; \u2014 fundamentally broken. Turns out researchers have already measured this: removing colons from a prompt template swings LLaMA-2-13B accuracy by 78 percentage points (Sclar et al., ICLR 2024). The format that works best on one model family overlaps less than 20% with what works best o", "score": 2, "created": "2026-02-18T14:41:57Z", "url": "https://news.ycombinator.com/item?id=47061443"}, {"source": "HackerNews", "text": "Ollama vs. vLLM: When to Start Scaling Your Local AI Stack ", "score": 2, "created": "2026-02-17T23:49:03Z", "url": "https://news.ycombinator.com/item?id=47055154"}, {"source": "HackerNews", "text": "Show HN: NadirClaw, LLM router that cuts costs by routing prompts right I use Claude and Codex heavily for coding, and I kept burning through my quota halfway through the week. When I looked at my logs, most of my prompts were things like &quot;summarize this,&quot; &quot;reformat this JSON,&quot; or &quot;write a docstring.&quot; Stuff that any small model handles fine.<p>So I built NadirClaw. It&#x27;s a Python proxy that sits between your app and your LLM providers. It classifies each prompt ", "score": 1, "created": "2026-02-17T23:31:18Z", "url": "https://news.ycombinator.com/item?id=47054977"}, {"source": "HackerNews", "text": "Show HN: Persistent memory for Claude Code with self-hosted Qdrant and Ollama I built an MCP server that gives Claude Code long-term memory across sessions, backed by infrastructure you control.<p>Every Claude Code session starts from zero, no memory of previous sessions. This server uses mem0ai as a library and exposes 11 MCP tools for storing, searching, and managing memories. Qdrant handles vector storage, Ollama runs embeddings locally (bge-m3), and Neo4j optionally builds a knowledge graph.", "score": 8, "created": "2026-02-17T21:21:25Z", "url": "https://news.ycombinator.com/item?id=47053534"}, {"source": "HackerNews", "text": "Show HN: Corral \u2013 Auth and Stripe billing that AI coding agents can set up Hey HN. I built Corral because every time I asked an AI coding agent to\n&quot;add auth and payments,&quot; it hallucinated for an hour and produced broken\ncode. Wrong imports, phantom endpoints, a login page wired to nothing.<p>The problem isn&#x27;t the agent. It&#x27;s that auth-to-billing-to-gating is\ngenuinely hard to wire, and there&#x27;s no machine-readable spec for how to\ndo it.<p>Corral is an open-source CLI (MIT", "score": 5, "created": "2026-02-17T21:03:09Z", "url": "https://news.ycombinator.com/item?id=47053308"}, {"source": "HackerNews", "text": "Show HN: Cai \u2013 AI actions on your clipboard, runs locally (macOS, open source) I&#x27;ve spent a lot of time copy-pasting and switching between apps to summarize text, create events, proofread emails, look up addresses \u2014 always the same follow-up steps after copying.<p>So I built Cai. It sits in the menu bar. Press Option+C, it detects what you copied and shows relevant actions. Ships with llama-server (Ministral 3B) so it works out of the box, or connect Ollama&#x2F;LM Studio if you already use", "score": 1, "created": "2026-02-17T16:04:13Z", "url": "https://news.ycombinator.com/item?id=47048991"}, {"source": "HackerNews", "text": "Show HN: I built a CLI to arbitrage GPU prices across 10 clouds Terradev is a BYOAPI Terraform \u201cwrapper\u201d for cross-cloud GPU provisioning and node pooling<p>pip install terradev-cli<p>terradev k8s create demo --gpu A100 --count 8 --multi-cloud<p>A100s may be $3.20&#x2F;hr on AWS, vs $2.40&#x2F;hr on Vast.ai, but getting quotes across providers takes hours, there\u2019s heavy tooling required between providers, and the nodes can\u2019t be actively managed as pricing and demand changes day-to-day\u2026<p>Terrade", "score": 1, "created": "2026-02-17T13:10:14Z", "url": "https://news.ycombinator.com/item?id=47047129"}, {"source": "HackerNews", "text": "Show HN: Sekha \u2013 What if AI remembered 3 years of conversations, not 3 hours? Every AI assistant today has amnesia. Context windows fill up and you&#x27;re starting over with a stranger.<p>Sekha gives your LLM a permanent memory:\n- Unlimited conversation history with semantic search\n- Works with any model (Claude, GPT, Llama, local)\n- Self-hosted, your data stays local\n- Built with Rust + SQLite + embeddings. AGPL-3.0.<p>GitHub: [github.com&#x2F;sekha-ai&#x2F;sekha-controller]\nDocs: [docs.sekha.", "score": 1, "created": "2026-02-17T13:09:31Z", "url": "https://news.ycombinator.com/item?id=47047120"}, {"source": "GitHub", "text": "[pull] main from run-llama:main See [Commits](/mcx/llama_index/pull/522/commits) and [Changes](/mcx/llama_index/pull/522/files) for more details.\n\n-----\nCreated by [<img src=\"https://prod.download/pull-18h-svg\" valign=\"bottom\"/> **pull[bot]**](https://github.com/wei/pull) (v2.0.0-alpha.4)\n\n_Can you help keep this open source service alive? **[\ud83d\udc96 Please sponsor : )](https://prod.download/pull-pr-sponsor)**_", "score": 0, "created": "2026-02-24T19:52:15Z", "url": "https://github.com/mcx/llama_index/pull/522"}, {"source": "GitHub", "text": "[pull] main from run-llama:main See [Commits](/AI2B/gpt_index/pull/327/commits) and [Changes](/AI2B/gpt_index/pull/327/files) for more details.\n\n-----\nCreated by [<img src=\"https://prod.download/pull-18h-svg\" valign=\"bottom\"/> **pull[bot]**](https://github.com/wei/pull) (v2.0.0-alpha.4)\n\n_Can you help keep this open source service alive? **[\ud83d\udc96 Please sponsor : )](https://prod.download/pull-pr-sponsor)**_", "score": 0, "created": "2026-02-24T19:23:05Z", "url": "https://github.com/AI2B/gpt_index/pull/327"}, {"source": "GitHub", "text": "feature: add DryRun param to ModelQuantizeParams to match latest llama.cpp This PR adds the `DryRun` param to the `ModelQuantizeParams` type to match latest `llama.cpp` from https://github.com/ggml-org/llama.cpp/pull/19526", "score": 0, "created": "2026-02-24T19:09:48Z", "url": "https://github.com/hybridgroup/yzma/pull/222"}, {"source": "GitHub", "text": "[pull] main from run-llama:main See [Commits](/EricLBuehler/llama_index/pull/391/commits) and [Changes](/EricLBuehler/llama_index/pull/391/files) for more details.\n\n-----\nCreated by [<img src=\"https://prod.download/pull-18h-svg\" valign=\"bottom\"/> **pull[bot]**](https://github.com/wei/pull) (v2.0.0-alpha.4)\n\n_Can you help keep this open source service alive? **[\ud83d\udc96 Please sponsor : )](https://prod.download/pull-pr-sponsor)**_", "score": 0, "created": "2026-02-24T19:01:04Z", "url": "https://github.com/EricLBuehler/llama_index/pull/391"}, {"source": "GitHub", "text": "[pull] main from run-llama:main See [Commits](/foreverLoveWisdom/gpt_index/pull/337/commits) and [Changes](/foreverLoveWisdom/gpt_index/pull/337/files) for more details.\n\n-----\nCreated by [<img src=\"https://prod.download/pull-18h-svg\" valign=\"bottom\"/> **pull[bot]**](https://github.com/wei/pull) (v2.0.0-alpha.4)\n\n_Can you help keep this open source service alive? **[\ud83d\udc96 Please sponsor : )](https://prod.download/pull-pr-sponsor)**_", "score": 0, "created": "2026-02-24T18:50:13Z", "url": "https://github.com/foreverLoveWisdom/gpt_index/pull/337"}, {"source": "GitHub", "text": "fix(docker): install cmake for node-llama-cpp on riscv64 ## Summary\n\n- Problem: `node-llama-cpp` postinstall fails on riscv64 \u2014 no prebuilt cmake binary available for download\n- Why it matters: `pnpm install --frozen-lockfile` exits with code 1, blocking the entire Docker build\n- What changed: Install `cmake`, `g++`, `make` via apt before `pnpm install`\n- What did NOT change: Everything else in the Dockerfile\n\n## Change Type (select all)\n\n- [x] Bug fix\n\n## Scope (select all touched areas)\n\n- [x]", "score": 2, "created": "2026-02-24T18:45:30Z", "url": "https://github.com/gounthar/openclaw/pull/12"}, {"source": "GitHub", "text": "[pull] main from run-llama:main See [Commits](/bestony/llama_index/pull/502/commits) and [Changes](/bestony/llama_index/pull/502/files) for more details.\n\n-----\nCreated by [<img src=\"https://prod.download/pull-18h-svg\" valign=\"bottom\"/> **pull[bot]**](https://github.com/wei/pull) (v2.0.0-alpha.4)\n\n_Can you help keep this open source service alive? **[\ud83d\udc96 Please sponsor : )](https://prod.download/pull-pr-sponsor)**_", "score": 0, "created": "2026-02-24T18:23:24Z", "url": "https://github.com/bestony/llama_index/pull/502"}, {"source": "GitHub", "text": "[pull] main from run-llama:main See [Commits](/nicoleahmed/llama_index/pull/450/commits) and [Changes](/nicoleahmed/llama_index/pull/450/files) for more details.\n\n-----\nCreated by [<img src=\"https://prod.download/pull-18h-svg\" valign=\"bottom\"/> **pull[bot]**](https://github.com/wei/pull) (v2.0.0-alpha.4)\n\n_Can you help keep this open source service alive? **[\ud83d\udc96 Please sponsor : )](https://prod.download/pull-pr-sponsor)**_", "score": 0, "created": "2026-02-24T18:15:10Z", "url": "https://github.com/nicoleahmed/llama_index/pull/450"}, {"source": "GitHub", "text": "[pull] main from run-llama:main See [Commits](/admariner/gpt_index/pull/321/commits) and [Changes](/admariner/gpt_index/pull/321/files) for more details.\n\n-----\nCreated by [<img src=\"https://prod.download/pull-18h-svg\" valign=\"bottom\"/> **pull[bot]**](https://github.com/wei/pull) (v2.0.0-alpha.4)\n\n_Can you help keep this open source service alive? **[\ud83d\udc96 Please sponsor : )](https://prod.download/pull-pr-sponsor)**_", "score": 0, "created": "2026-02-24T18:02:17Z", "url": "https://github.com/admariner/gpt_index/pull/321"}, {"source": "GitHub", "text": "feat: add descriptionJa field + Gemma-2-Llama-Swallow 9B model ## Summary\n\n- Add `descriptionJa` optional field to `LlmModelEntry` type for displaying model descriptions in Japanese UI\n- Add Gemma-2-Llama-Swallow 9B (Q4_K_M) model entry to the registry (GGUF available from mmnga/HuggingFace)\n- Add Japanese descriptions (`descriptionJa`) to all 3 existing Qwen3 models\n- Update Settings UI to display model descriptions and handle unavailable models (`url: \"\"`) with\u300c\u6e96\u5099\u4e2d\u300dbadge\n- Fix SHA256 verificat", "score": 2, "created": "2026-02-24T17:52:27Z", "url": "https://github.com/Iktahana/illusions/pull/601"}, {"source": "GitHub", "text": "[pull] main from run-llama:main See [Commits](/austinmw/llama_index/pull/361/commits) and [Changes](/austinmw/llama_index/pull/361/files) for more details.\n\n-----\nCreated by [<img src=\"https://prod.download/pull-18h-svg\" valign=\"bottom\"/> **pull[bot]**](https://github.com/wei/pull) (v2.0.0-alpha.4)\n\n_Can you help keep this open source service alive? **[\ud83d\udc96 Please sponsor : )](https://prod.download/pull-pr-sponsor)**_", "score": 0, "created": "2026-02-24T17:50:13Z", "url": "https://github.com/austinmw/llama_index/pull/361"}, {"source": "GitHub", "text": "feat: add descriptionJa field + Gemma-2-Llama-Swallow 9B model entry ## Summary\n\nAdd `descriptionJa` field to `LlmModelEntry` type, add Gemma-2-Llama-Swallow 9B model to the registry, add Japanese descriptions to existing Qwen3 models, and update the Settings UI to display descriptions and handle unavailable models gracefully.\n\nPart of #473\n\n## Affected Files\n\n| File | Action | Purpose |\n|------|--------|---------|\n| `lib/llm-client/types.ts` | MODIFY | Add `descriptionJa?: string` to `LlmModelE", "score": 0, "created": "2026-02-24T17:46:40Z", "url": "https://github.com/Iktahana/illusions/issues/598"}, {"source": "GitHub", "text": "[pull] main from run-llama:main See [Commits](/rpatil524/gpt_index/pull/379/commits) and [Changes](/rpatil524/gpt_index/pull/379/files) for more details.\n\n-----\nCreated by [<img src=\"https://prod.download/pull-18h-svg\" valign=\"bottom\"/> **pull[bot]**](https://github.com/wei/pull) (v2.0.0-alpha.4)\n\n_Can you help keep this open source service alive? **[\ud83d\udc96 Please sponsor : )](https://prod.download/pull-pr-sponsor)**_", "score": 0, "created": "2026-02-24T17:39:12Z", "url": "https://github.com/rpatil524/gpt_index/pull/379"}, {"source": "GitHub", "text": "Bug: llama-quantize failing on qwen3next low BPW quant types. ### What happened?\n\nI've noticed errors with llama-quantize when attempting low BPW quantization types for routed experts tensors on Qwen3-Coder-Next e.g.\n\n```bash\nconverting to iq2_kl .. /home/w/projects/ik_llama.cpp/ggml/src/iqk/iqk_quantize.cpp:1653: GGML_ASSERT(ibest >= 0) failed\n```\n\nIt seems like it can quantize `ffn_down_exps` okay, but then fails on `ffn_gate_exps` when using `iq1_kt` `iq2_kt` `iq2_kl` `iq1_s`. Seems to work f", "score": 2, "created": "2026-02-24T17:35:08Z", "url": "https://github.com/ikawrakow/ik_llama.cpp/issues/1316"}, {"source": "GitHub", "text": "[pull] main from run-llama:main See [Commits](/jingyi-zhao-01/llama_index/pull/442/commits) and [Changes](/jingyi-zhao-01/llama_index/pull/442/files) for more details.\n\n-----\nCreated by [<img src=\"https://prod.download/pull-18h-svg\" valign=\"bottom\"/> **pull[bot]**](https://github.com/wei/pull) (v2.0.0-alpha.4)\n\n_Can you help keep this open source service alive? **[\ud83d\udc96 Please sponsor : )](https://prod.download/pull-pr-sponsor)**_", "score": 0, "created": "2026-02-24T17:29:22Z", "url": "https://github.com/jingyi-zhao-01/llama_index/pull/442"}, {"source": "GitHub", "text": "build(deps): bump llama-index-core from 0.10.12 to 0.13.0 in /dev/recursive_retrieval Bumps [llama-index-core](https://github.com/run-llama/llama_index) from 0.10.12 to 0.13.0.\n<details>\n<summary>Release notes</summary>\n<p><em>Sourced from <a href=\"https://github.com/run-llama/llama_index/releases\">llama-index-core's releases</a>.</em></p>\n<blockquote>\n<h2>v0.13.0</h2>\n<h1>Release Notes</h1>\n<p><strong>NOTE:</strong> All packages have been bumped to handle the latest llama-index-core version.</p", "score": 0, "created": "2026-02-24T17:24:11Z", "url": "https://github.com/jchoong/Memary/pull/4"}, {"source": "GitHub", "text": "[pull] main from run-llama:main See [Commits](/ykatyhoney/llama_index/pull/305/commits) and [Changes](/ykatyhoney/llama_index/pull/305/files) for more details.\n\n-----\nCreated by [<img src=\"https://prod.download/pull-18h-svg\" valign=\"bottom\"/> **pull[bot]**](https://github.com/wei/pull) (v2.0.0-alpha.4)\n\n_Can you help keep this open source service alive? **[\ud83d\udc96 Please sponsor : )](https://prod.download/pull-pr-sponsor)**_", "score": 0, "created": "2026-02-24T17:19:12Z", "url": "https://github.com/ykatyhoney/llama_index/pull/305"}, {"source": "GitHub", "text": "Bump llama-index-core from 0.11.22 to 0.13.0 Bumps [llama-index-core](https://github.com/run-llama/llama_index) from 0.11.22 to 0.13.0.\n<details>\n<summary>Release notes</summary>\n<p><em>Sourced from <a href=\"https://github.com/run-llama/llama_index/releases\">llama-index-core's releases</a>.</em></p>\n<blockquote>\n<h2>v0.13.0</h2>\n<h1>Release Notes</h1>\n<p><strong>NOTE:</strong> All packages have been bumped to handle the latest llama-index-core version.</p>\n<h3><code>llama-index-core</code> [0.13", "score": 0, "created": "2026-02-24T17:14:02Z", "url": "https://github.com/microsoft/RPG-ZeroRepo/pull/13"}, {"source": "GitHub", "text": "mini-swe-agent + llama-server + litellm \"openai_like\" error ### Describe the bug\n\n### Name and Version\n\nmini-swe-agent installed on Arch linux.\n\n```bash\n$ mini --version\n\ud83d\udc4b This is mini-swe-agent version 2.2.2.\n```\n\n```bash\n$ llama-server --version\nggml_cuda_init: found 1 ROCm devices:\n  Device 0: AMD Radeon 890M Graphics, gfx1150 (0x1150), VMM: no, Wave Size: 32\nversion: 7950 (449ec2ab0)\nbuilt with Clang 22.0.0 for Linux\n```\n\n```bash\n#!/bin/bash\n\nexport LLAMA_ARG_MODEL=~/src/models/Devstral-Smal", "score": 1, "created": "2026-02-24T17:03:07Z", "url": "https://github.com/SWE-agent/mini-swe-agent/issues/759"}, {"source": "GitHub", "text": "Feat/offline llama android ", "score": 0, "created": "2026-02-24T17:02:25Z", "url": "https://github.com/asierraserna/TatruMedGemma/pull/3"}, {"source": "GitHub", "text": "Merge pull request #1 from asierraserna/feat/offline-llama-android Feat/offline llama android", "score": 0, "created": "2026-02-24T17:01:45Z", "url": "https://github.com/asierraserna/TatruMedGemma/pull/2"}, {"source": "GitHub", "text": "[pull] main from run-llama:main See [Commits](/ishandutta2007/llama_index/pull/293/commits) and [Changes](/ishandutta2007/llama_index/pull/293/files) for more details.\n\n-----\nCreated by [<img src=\"https://prod.download/pull-18h-svg\" valign=\"bottom\"/> **pull[bot]**](https://github.com/wei/pull) (v2.0.0-alpha.4)\n\n_Can you help keep this open source service alive? **[\ud83d\udc96 Please sponsor : )](https://prod.download/pull-pr-sponsor)**_", "score": 0, "created": "2026-02-24T16:53:07Z", "url": "https://github.com/ishandutta2007/llama_index/pull/293"}, {"source": "GitHub", "text": "Add llama-index-llms-huggingface-langchain integration package # Description\n\nPlease include a summary of the change and which issue is fixed. Please also include relevant motivation and context. List any dependencies that are required for this change.\n\nFixes # (issue)\n\n## Related PRs\n\n<!-- List any related pull requests, e.g. PRs that this depends on or PRs in other repos. -->\n\n- Related PR #\n\n## New Package?\n\nDid I fill in the `tool.llamahub` section in the `pyproject.toml` and provide a detai", "score": 0, "created": "2026-02-24T16:13:07Z", "url": "https://github.com/morongosteve/llama_index/pull/38"}, {"source": "GitHub", "text": "[pull] main from run-llama:main See [Commits](/dumpmemory/gpt_index/pull/320/commits) and [Changes](/dumpmemory/gpt_index/pull/320/files) for more details.\n\n-----\nCreated by [<img src=\"https://prod.download/pull-18h-svg\" valign=\"bottom\"/> **pull[bot]**](https://github.com/wei/pull) (v2.0.0-alpha.4)\n\n_Can you help keep this open source service alive? **[\ud83d\udc96 Please sponsor : )](https://prod.download/pull-pr-sponsor)**_", "score": 0, "created": "2026-02-24T16:12:24Z", "url": "https://github.com/dumpmemory/gpt_index/pull/320"}, {"source": "GitHub", "text": "docs: deepen tutorial chapters batch 2 (copilot-cli through llama-factory) ## Summary\nThis PR applies the v2 depth/quality expansion to tutorial chapter files (no index files) for the second batch of tracks.\n\n## Scope\n- Batch: 2\n- Tutorial directories:       46\n- Chapter files changed: 369\n- Range: `copilot-cli-tutorial` through `llama-factory-tutorial`\n\n## Included Tutorial Directories\n- `copilot-cli-tutorial`\n- `copilotkit-tutorial`\n- `create-python-server-tutorial`\n- `create-typescript-server", "score": 0, "created": "2026-02-24T15:56:28Z", "url": "https://github.com/johnxie/awesome-code-docs/pull/145"}, {"source": "GitHub", "text": "[pull] main from run-llama:main See [Commits](/Superoldman96/llama_index/pull/304/commits) and [Changes](/Superoldman96/llama_index/pull/304/files) for more details.\n\n-----\nCreated by [<img src=\"https://prod.download/pull-18h-svg\" valign=\"bottom\"/> **pull[bot]**](https://github.com/wei/pull) (v2.0.0-alpha.4)\n\n_Can you help keep this open source service alive? **[\ud83d\udc96 Please sponsor : )](https://prod.download/pull-pr-sponsor)**_", "score": 0, "created": "2026-02-24T15:45:02Z", "url": "https://github.com/Superoldman96/llama_index/pull/304"}, {"source": "GitHub", "text": "Autorag llama stack compatible pipeline version  **Description of your changes:**\r\n\r\n- no mocks anymore within autorag components, \r\n- when `llama_stack_secret_name` is provided then the autorag experiment is executed against llama-stack server, \r\n- when `openai_secret_name` is provided then the autorag experiment is executed using ChromaDB and externally reachable models. \r\n  `llama_stack_secret_name` takes precedence over `openai_secret_name`. \r\n- other small changes (syntax w/o behavioral cha", "score": 0, "created": "2026-02-24T15:42:24Z", "url": "https://github.com/LukaszCmielowski/pipelines-components/pull/27"}, {"source": "GitHub", "text": "[pull] main from run-llama:main See [Commits](/shyamsantoki/gpt_index/pull/439/commits) and [Changes](/shyamsantoki/gpt_index/pull/439/files) for more details.\n\n-----\nCreated by [<img src=\"https://prod.download/pull-18h-svg\" valign=\"bottom\"/> **pull[bot]**](https://github.com/wei/pull) (v2.0.0-alpha.4)\n\n_Can you help keep this open source service alive? **[\ud83d\udc96 Please sponsor : )](https://prod.download/pull-pr-sponsor)**_", "score": 0, "created": "2026-02-24T15:37:09Z", "url": "https://github.com/shyamsantoki/gpt_index/pull/439"}, {"source": "GitHub", "text": "Enable GPU support for llama.cpp GGUF export # Enable GPU support for llama.cpp GGUF export\r\n\r\nThis PR works together with unslothai/unsloth-zoo#512 to enable ROCm GPU acceleration for GGUF inference.\r\n\r\n## Change\r\n\r\n**unsloth/save.py** (+1, -1):\r\n- Changed `gpu_support = False` to `gpu_support = True` in `save_to_gguf()`\r\n\r\nThis enables the GPU backend detection logic added in unsloth-zoo to take effect during llama.cpp compilation.\r\n\r\n## Testing\r\n\r\nTested on AMD Radeon PRO W7900 (gfx1100) with", "score": 1, "created": "2026-02-24T15:25:49Z", "url": "https://github.com/unslothai/unsloth/pull/4103"}, {"source": "GitHub", "text": "RHAIENG-2398-part-2 further changes made to llama stack server deploy\u2026 \u2026ment\r\n\r\n<!--- Provide a general summary of your changes in the Title above -->\r\n\r\n## Description\r\n**RHOAI 3.3: Clarify PostgreSQL requirements for Llama Stack server deployment**\r\n\r\nThis PR improves clarity around PostgreSQL requirements in the Deploying a Llama Stack server procedure.\r\n\r\n**Summary of changes**\r\n\r\nAdded explicit PostgreSQL prerequisites to clarify that a running, reachable PostgreSQL instance with known host", "score": 1, "created": "2026-02-24T15:24:47Z", "url": "https://github.com/opendatahub-io/opendatahub-documentation/pull/1224"}, {"source": "GitHub", "text": "Enable ROCm GPU acceleration for llama.cpp GGUF export # Enable ROCm GPU acceleration for llama.cpp GGUF export\r\n\r\n## Summary\r\n\r\nThis PR adds automatic ROCm GPU detection and compilation support for llama.cpp during GGUF export, enabling AMD GPU users to benefit from hardware-accelerated inference.\r\n\r\n## Problem\r\n\r\nCurrently, when converting models to GGUF format on AMD ROCm systems, llama.cpp is compiled without GPU support, resulting in CPU-only inference that is significantly slower (5-8x) co", "score": 1, "created": "2026-02-24T15:17:41Z", "url": "https://github.com/unslothai/unsloth-zoo/pull/512"}, {"source": "GitHub", "text": "Data analysis so far for Deepseek and Llama ", "score": 0, "created": "2026-02-24T15:12:14Z", "url": "https://github.com/wmarcu/cs4575-llm-comparison/pull/1"}, {"source": "GitHub", "text": "Add GET Llama-stack models BFF endpoint for AutoRAG https://issues.redhat.com/browse/RHOAIENG-48104\r\n\r\n## Description\r\n\r\n- Added GET /lsd/models BFF endpoint in the AutoRAG BFF, implementing the full handler, repository, and LlamaStack client layers to fetch and categorize LLM/embedding models from a LlamaStack deployment (LSD)\r\n- The returned models should be sorted according to model type of llm or embedding. \r\n- Introduced LlamaStack client & factory (llamastack_client.go, llamastack_client_f", "score": 3, "created": "2026-02-24T14:58:47Z", "url": "https://github.com/opendatahub-io/odh-dashboard/pull/6328"}, {"source": "GitHub", "text": "Llama 70b galaxy hanging on 7th decode iteration in PCC check Deterministic hang when doing PCC check in Llama galaxy\nExample run: https://github.com/tenstorrent/tt-metal/actions/runs/22330984158/job/64614027044#step:7:1959\nNeed to temporarily skip test for now until we find source of hang", "score": 0, "created": "2026-02-24T14:57:37Z", "url": "https://github.com/tenstorrent/tt-metal/issues/38448"}, {"source": "GitHub", "text": "[pull] main from run-llama:main See [Commits](/Afrilingua/llama_index/pull/408/commits) and [Changes](/Afrilingua/llama_index/pull/408/files) for more details.\n\n-----\nCreated by [<img src=\"https://prod.download/pull-18h-svg\" valign=\"bottom\"/> **pull[bot]**](https://github.com/wei/pull) (v2.0.0-alpha.4)\n\n_Can you help keep this open source service alive? **[\ud83d\udc96 Please sponsor : )](https://prod.download/pull-pr-sponsor)**_", "score": 0, "created": "2026-02-24T14:57:06Z", "url": "https://github.com/Afrilingua/llama_index/pull/408"}, {"source": "GitHub", "text": "Update dependency jpm-canonical/llama.cpp-builds to v7881 This PR contains the following updates:\n\n| Package | Update | Change |\n|---|---|---|\n| [jpm-canonical/llama.cpp-builds](https://redirect.github.com/jpm-canonical/llama.cpp-builds) | major | `b7842` \u2192 `b7881` |\n\n---\n\n### Release Notes\n\n<details>\n<summary>jpm-canonical/llama.cpp-builds (jpm-canonical/llama.cpp-builds)</summary>\n\n### [`vb7881`](https://redirect.github.com/jpm-canonical/llama.cpp-builds/releases/tag/b7881)\n\n[Compare Source](h", "score": 0, "created": "2026-02-24T14:43:41Z", "url": "https://github.com/farshidtz/nemotron-3-nano-snap/pull/4"}, {"source": "GitHub", "text": "[pull] main from run-llama:main See [Commits](/ericmjl/llama_index/pull/545/commits) and [Changes](/ericmjl/llama_index/pull/545/files) for more details.\n\n-----\nCreated by [<img src=\"https://prod.download/pull-18h-svg\" valign=\"bottom\"/> **pull[bot]**](https://github.com/wei/pull) (v2.0.0-alpha.4)\n\n_Can you help keep this open source service alive? **[\ud83d\udc96 Please sponsor : )](https://prod.download/pull-pr-sponsor)**_", "score": 0, "created": "2026-02-24T14:32:25Z", "url": "https://github.com/ericmjl/llama_index/pull/545"}, {"source": "GitHub", "text": "[pull] main from run-llama:main See [Commits](/dazeb/llama_index/pull/601/commits) and [Changes](/dazeb/llama_index/pull/601/files) for more details.\n\n-----\nCreated by [<img src=\"https://prod.download/pull-18h-svg\" valign=\"bottom\"/> **pull[bot]**](https://github.com/wei/pull) (v2.0.0-alpha.4)\n\n_Can you help keep this open source service alive? **[\ud83d\udc96 Please sponsor : )](https://prod.download/pull-pr-sponsor)**_", "score": 0, "created": "2026-02-24T14:31:05Z", "url": "https://github.com/dazeb/llama_index/pull/601"}, {"source": "GitHub", "text": "[pull] main from run-llama:main See [Commits](/Mu-L/llama_index/pull/388/commits) and [Changes](/Mu-L/llama_index/pull/388/files) for more details.\n\n-----\nCreated by [<img src=\"https://prod.download/pull-18h-svg\" valign=\"bottom\"/> **pull[bot]**](https://github.com/wei/pull) (v2.0.0-alpha.4)\n\n_Can you help keep this open source service alive? **[\ud83d\udc96 Please sponsor : )](https://prod.download/pull-pr-sponsor)**_", "score": 0, "created": "2026-02-24T13:56:33Z", "url": "https://github.com/Mu-L/llama_index/pull/388"}]}